{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a basic training loop - not using attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Changing directories '''\n",
    "import os \n",
    "if 'BERT' not in os.getcwd():\n",
    "    os.chdir('BERT')\n",
    "print(\"Current working dir is {}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaml\n",
    "import onmt\n",
    "import torch\n",
    "from dataset import TextDataset\n",
    "from encoder import Encoder \n",
    "from decoder import Decoder\n",
    "from discriminator import Discriminator\n",
    "from lib.huggingface.transformers import RobertaTokenizer, CamembertTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.path.dirname(os.getcwd()), \"config\", \"config.yml\"), \"r\") as fd:\n",
    "    config = pyaml.yaml.load(fd, Loader=pyaml.yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset_train = TextDataset(\"data/data-30k-default/\", is_train=True)\n",
    "text_dataset_val = TextDataset(\"data/data-30k-default/\", is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_fr = CamembertTokenizer.from_pretrained('camembert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(data): \n",
    "    '''Collating function to be passed into the dataloader '''\n",
    "    input_sentences, output_sentences = zip(*data)\n",
    "    input_lengths = [len(sentence)+2 for sentence in input_sentences]\n",
    "    output_lengths = [len(sentence)+2 for sentence in output_sentences]\n",
    "    \n",
    "    batch_size = len(input_sentences)\n",
    "    \n",
    "    max_input_lengths = max(input_lengths)\n",
    "    max_output_lengths = max(output_lengths)\n",
    "    \n",
    "    max_length = max(max_input_lengths, max_output_lengths)\n",
    "    \n",
    "    input_idx_tensor = torch.zeros((batch_size, max_length), dtype=torch.long)\n",
    "    output_idx_tensor = torch.zeros((batch_size, max_length), dtype=torch.long)\n",
    "    \n",
    "    for idx, (sentence_len, input_sentence) in enumerate(zip(input_lengths, input_sentences)): \n",
    "        input_idx_tensor[idx, :] = torch.tensor(tokenizer_en.encode(input_sentence) + [1]*(max_length-sentence_len))\n",
    "\n",
    "    \n",
    "    for idx, (sentence_len, output_sentence) in enumerate(zip(output_lengths, output_sentences)): \n",
    "        output_idx_tensor[idx, :] = torch.tensor(tokenizer_fr.encode(output_sentence) + [1]*(max_length-sentence_len))\n",
    "\n",
    "    return ((input_idx_tensor, torch.tensor(input_lengths)), (output_idx_tensor, torch.tensor(output_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(text_dataset_train, **config[\"data_loader\"], collate_fn=collate)\n",
    "val_dataloader = DataLoader(text_dataset_val, **config[\"data_loader\"], collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the encoding and decoding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"Using CUDA!\")\n",
    "else:\n",
    "    print(\"Using CPU - Played yourself!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del encoder_en\n",
    "    del encoder_fr\n",
    "except:\n",
    "    pass \n",
    "encoder_en = Encoder(\"english\").to(device=device)\n",
    "encoder_fr = Encoder(\"french\").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same\n",
    "word_padding_idx_en = encoder_en._modules['model'].embeddings.padding_idx\n",
    "word_padding_idx_fr = encoder_fr._modules['model'].embeddings.padding_idx\n",
    "\n",
    "# en > fr\n",
    "word_vocab_size_en = encoder_en._modules['model'].embeddings.word_embeddings.num_embeddings\n",
    "word_vocab_size_fr = encoder_fr._modules['model'].embeddings.word_embeddings.num_embeddings\n",
    "\n",
    "# same\n",
    "word_vec_size_en = encoder_en._modules['model'].embeddings.word_embeddings.embedding_dim\n",
    "word_vec_size_fr = encoder_fr._modules['model'].embeddings.word_embeddings.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_en = onmt.modules.embeddings.Embeddings(\n",
    "    word_vec_size_en, \n",
    "    word_vocab_size_en, \n",
    "    word_padding_idx_en, \n",
    "    position_encoding=True\n",
    ").to(device=device)\n",
    "\n",
    "embeddings_fr = onmt.modules.embeddings.Embeddings(\n",
    "    word_vec_size_fr, \n",
    "    word_vocab_size_fr, \n",
    "    word_padding_idx_fr, \n",
    "    position_encoding=True\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_en = Decoder(**config[\"small_transformer\"], embeddings=embeddings_en).to(device=device)\n",
    "decoder_fr = Decoder(**config[\"small_transformer\"], embeddings=embeddings_fr).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection: 768 -> 1\n",
    "discriminator = Discriminator(config[\"small_transformer\"]['d_model'], 1).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict):\n",
    "    '''Standard machine translation cross entropy loss'''\n",
    "    ce_loss = torch.nn.CrossEntropyLoss()\n",
    "    loss_english_to_french = ce_loss(english_predict.transpose(1,2), english_gt)\n",
    "    loss_french_to_english = ce_loss(french_predict.transpose(1,2), french_gt)\n",
    "    return loss_english_to_french + loss_french_to_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_hidden_regularization(english_gt, french_gt, english_predict, french_predict,\n",
    "                                discriminator_gt, discriminator_predict):\n",
    "    '''Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs'''\n",
    "    ce_term = loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict)\n",
    "    \n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    regularizing_term = bce_loss(discriminator_predict, discriminator_gt)\n",
    "    \n",
    "    return ce_term + regularizing_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data_iter, val_data_iter, regularize=\"hidden_state\"): \n",
    "    ''' \n",
    "    Train the encoding and decoding models. User needs to pass in a valid iterator over the data,\n",
    "    and also specify a type of adversarial regularization. regularize = [\"hidden_state\", \"attention\"]\n",
    "    '''\n",
    "    \n",
    "    params = list(encoder_en.parameters()) + list(encoder_fr.parameters()) +\\\n",
    "             list(decoder_fr.parameters()) + list(decoder_en.parameters())\n",
    "    \n",
    "    if (regularize == \"hidden_state\"):\n",
    "        params += list(discriminator.parameters())\n",
    "        \n",
    "    optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9)\n",
    "    \n",
    "    for batch_num, batch in enumerate(train_data_iter):\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Reading in input and moving to device\n",
    "        (english_sentences, english_sentences_lengths), (french_sentences, french_sentences_lengths) = batch\n",
    "        english_sentences = english_sentences.to(device=device)\n",
    "        english_sentences_lengths = english_sentences_lengths.to(device=device)\n",
    "        french_sentences = french_sentences.to(device=device)\n",
    "        french_sentences_lengths = french_sentences_lengths.to(device=device)\n",
    "    \n",
    "        # Encoding - Decoding for English -> French\n",
    "        encoder_outputs_en = encoder_en(english_sentences)\n",
    "        decoder_fr.init_state(english_sentences.unsqueeze(2).transpose(0,1), None, None)\n",
    "        dec_outs_fr, _ = decoder_fr(french_sentences.unsqueeze(2).transpose(0,1)[:-1], encoder_outputs_en[0].transpose(0,1), memory_lengths=english_sentences_lengths)\n",
    "        \n",
    "        # Encoding - Decoding for French -> English\n",
    "        encoder_outputs_fr = encoder_fr(french_sentences)\n",
    "        decoder_en.init_state(french_sentences.unsqueeze(2).transpose(0,1), None, None) \n",
    "        dec_outs_en, _ = decoder_en(english_sentences.unsqueeze(2).transpose(0,1)[:-1], encoder_outputs_fr[0].transpose(0,1), memory_lengths=french_sentences_lengths)\n",
    "        \n",
    "        if (regularize == \"hidden_state\"):\n",
    "            # using the pooled outputs of the encoders for regularizing \n",
    "            discriminator_outputs_en = discriminator(encoder_outputs_en[1])\n",
    "            discriminator_outputs_fr = discriminator(encoder_outputs_fr[1])\n",
    "            discriminator_outputs_cat = torch.cat((discriminator_outputs_en, discriminator_outputs_fr))\n",
    "            discriminator_labels = torch.tensor([1.0]*discriminator_outputs_en.shape[0] + [0.0]*discriminator_outputs_fr.shape[0])\n",
    "            discriminator_labels = discriminator_labels.unsqueeze(1).to(device=device)\n",
    "            \n",
    "            loss = loss_fn_hidden_regularization(english_sentences[:, 1:],\n",
    "                                               french_sentences[:, 1:],\n",
    "                                               dec_outs_en,\n",
    "                                               dec_outs_fr,\n",
    "                                               discriminator_labels,\n",
    "                                               discriminator_outputs_cat,\n",
    "                                              )\n",
    "        else:\n",
    "            loss = loss_fn_no_regularization(english_sentences,\n",
    "                                           french_sentences,\n",
    "                                           dec_outs_en,\n",
    "                                           dec_outs_fr,\n",
    "                                          )\n",
    "        # must be put here to avoid name claim by val loop\n",
    "        print(\"Batch num {}: Loss {}\".format(batch_num, loss.item()))\n",
    "        \n",
    "        if (batch_num % 100 == 0):\n",
    "            with torch.no_grad():\n",
    "                for batch_num, batch in enumerate(val_data_iter):\n",
    "                    # Reading in input and moving to device\n",
    "                    (english_sentences_val, english_sentences_lengths_val), (french_sentences_val, french_sentences_lengths_val) = batch\n",
    "                    english_sentences_val = english_sentences_val.to(device=device)\n",
    "                    english_sentences_lengths_val = english_sentences_lengths_val.to(device=device)\n",
    "                    french_sentences_val = french_sentences_val.to(device=device)\n",
    "                    french_sentences_lengths_val = french_sentences_lengths_val.to(device=device)\n",
    "\n",
    "                    # Encoding - Decoding for English -> French\n",
    "                    encoder_outputs_en = encoder_en(english_sentences_val)\n",
    "                    decoder_fr.init_state(english_sentences_val.unsqueeze(2).transpose(0,1), None, None) \n",
    "                    dec_outs_fr, _ = decoder_fr(french_sentences_val.unsqueeze(2).transpose(0,1)[:-1], encoder_outputs_en[0].transpose(0,1), memory_lengths=english_sentences_lengths_val)\n",
    "                    \n",
    "                    # Calculate BLUE Scores and EM\n",
    "                    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: \n",
    "* train without any regularizing terms - train with regularizing terms on the hidden layers, train with regularizing terms on the attention, train with regularizing terms on both \n",
    "* Write up validation metrics and print out validation at intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
