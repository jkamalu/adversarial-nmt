{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a basic training loop - not using attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir is /juice/scr/scr110/scr/nlp/mtl_bert/unidirectional-NMT/BERT\n"
     ]
    }
   ],
   "source": [
    "''' Changing directories '''\n",
    "import os \n",
    "if 'BERT' not in os.getcwd():\n",
    "    os.chdir('BERT')\n",
    "print(\"Current working dir is {}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "import pyaml\n",
    "import onmt\n",
    "import torch\n",
    "from dataset import TextDataset\n",
    "from encoder import Encoder \n",
    "from decoder import Decoder\n",
    "from discriminator import Discriminator\n",
    "from lib.huggingface.transformers import RobertaTokenizer, CamembertTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.path.dirname(os.getcwd()), \"config\", \"config.yml\"), \"r\") as fd:\n",
    "    config = pyaml.yaml.load(fd, Loader=pyaml.yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: data.train.2.pt\n",
      "Loading data from file: data.train.1.pt\n",
      "Loading data from file: data.train.0.pt\n",
      "removed 11332 examples - not long enough\n",
      "Loading data from file: data.valid.0.pt\n",
      "removed 27 examples - not long enough\n"
     ]
    }
   ],
   "source": [
    "text_dataset_train = TextDataset(\"data/data-30k-default/\", is_train=True)\n",
    "text_dataset_val = TextDataset(\"data/data-30k-default/\", is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_fr = CamembertTokenizer.from_pretrained('camembert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(data): \n",
    "    '''Collating function to be passed into the dataloader '''\n",
    "    input_sentences, output_sentences = zip(*data)\n",
    "    input_lengths = [len(sentence)+2 for sentence in input_sentences]\n",
    "    output_lengths = [len(sentence)+2 for sentence in output_sentences]\n",
    "    \n",
    "    batch_size = len(input_sentences)\n",
    "    \n",
    "    max_input_lengths = max(input_lengths)\n",
    "    max_output_lengths = max(output_lengths)\n",
    "    \n",
    "    max_length = max(max_input_lengths, max_output_lengths)\n",
    "    \n",
    "    input_idx_tensor = torch.zeros((batch_size, max_length), dtype=torch.long)\n",
    "    output_idx_tensor = torch.zeros((batch_size, max_length), dtype=torch.long)\n",
    "    \n",
    "    for idx, (sentence_len, input_sentence) in enumerate(zip(input_lengths, input_sentences)): \n",
    "        input_idx_tensor[idx, :] = torch.tensor(tokenizer_en.encode(input_sentence) + [1]*(max_length-sentence_len))\n",
    "\n",
    "    \n",
    "    for idx, (sentence_len, output_sentence) in enumerate(zip(output_lengths, output_sentences)): \n",
    "        output_idx_tensor[idx, :] = torch.tensor(tokenizer_fr.encode(output_sentence) + [1]*(max_length-sentence_len))\n",
    "\n",
    "    return ((input_idx_tensor, torch.tensor(input_lengths)), (output_idx_tensor, torch.tensor(output_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(text_dataset_train, **config[\"data_loader\"], collate_fn=collate)\n",
    "val_dataloader = DataLoader(text_dataset_val, **config[\"data_loader\"], collate_fn=collate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the encoding and decoding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA!\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"Using CUDA!\")\n",
    "else:\n",
    "    print(\"Using CPU - Played yourself!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del encoder_en\n",
    "    del encoder_fr\n",
    "except:\n",
    "    pass \n",
    "encoder_en = Encoder(\"english\").to(device=device)\n",
    "encoder_fr = Encoder(\"french\").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_padding_idx_en = encoder_en._modules['model'].embeddings.padding_idx\n",
    "word_padding_idx_fr = encoder_fr._modules['model'].embeddings.padding_idx\n",
    "\n",
    "word_vocab_size_en = encoder_en._modules['model'].embeddings.word_embeddings.num_embeddings\n",
    "word_vocab_size_fr = encoder_fr._modules['model'].embeddings.word_embeddings.num_embeddings\n",
    "\n",
    "word_vec_size_en = encoder_en._modules['model'].embeddings.word_embeddings.embedding_dim\n",
    "word_vec_size_fr = encoder_fr._modules['model'].embeddings.word_embeddings.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_en = onmt.modules.embeddings.Embeddings(\n",
    "    word_vec_size_en, \n",
    "    word_vocab_size_en, \n",
    "    word_padding_idx_en, \n",
    "    position_encoding=True\n",
    ").to(device=device)\n",
    "\n",
    "embeddings_fr = onmt.modules.embeddings.Embeddings(\n",
    "    word_vec_size_fr, \n",
    "    word_vocab_size_fr, \n",
    "    word_padding_idx_fr, \n",
    "    position_encoding=True\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_en = Decoder(**config[\"small_transformer\"], embeddings=embeddings_en).to(device=device)\n",
    "decoder_fr = Decoder(**config[\"small_transformer\"], embeddings=embeddings_fr).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(config[\"small_transformer\"]['d_model'], 1).to(device=device) #768->1 projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict):\n",
    "    '''Standard machine translation cross entropy loss'''\n",
    "    ce_loss = torch.nn.CrossEntropyLoss()\n",
    "    loss_english_to_french = ce_loss(english_predict.transpose(1,2), english_gt)\n",
    "    loss_french_to_english = ce_loss(french_predict.transpose(1,2), french_gt)\n",
    "    return loss_english_to_french + loss_french_to_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_hidden_regularization(english_gt, french_gt, english_predict, french_predict,\n",
    "                                discriminator_gt, discriminator_predict):\n",
    "    '''Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs'''\n",
    "    ce_term = loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict)\n",
    "    \n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    regularizing_term = bce_loss(discriminator_predict, discriminator_gt)\n",
    "    \n",
    "    return ce_term + regularizing_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data_iter, val_data_iter, regularize=\"hidden_state\"): \n",
    "    ''' \n",
    "    Train the encoding and decoding models. User needs to pass in a valid iterator over the data,\n",
    "    and also specify a type of adversarial regularization. regularize = [\"hidden_state\", \"attention\"]\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    params = list(encoder_en.parameters()) + list(encoder_fr.parameters()) +\\\n",
    "             list(decoder_fr.parameters()) + list(decoder_en.parameters())\n",
    "    \n",
    "    if (regularize == \"hidden_state\"):\n",
    "        params += list(discriminator.parameters())\n",
    "        \n",
    "    optimizer = optim.SGD(params, lr=0.01, momentum=0.9)\n",
    "    \n",
    "    for batch_num, batch in enumerate(train_data_iter):\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Reading in input and moving to device\n",
    "        (english_sentences, english_sentences_lengths), (french_sentences, french_sentences_lengths) = batch\n",
    "        english_sentences = english_sentences.to(device=device)\n",
    "        english_sentences_lengths = english_sentences_lengths.to(device=device)\n",
    "        french_sentences = french_sentences.to(device=device)\n",
    "        french_sentences_lengths = french_sentences_lengths.to(device=device)\n",
    "    \n",
    "        # Encoding - Decoding for English -> French\n",
    "        encoder_outputs_en = encoder_en(english_sentences)\n",
    "        decoder_fr.init_state(english_sentences.unsqueeze(2).transpose(0,1), None, None) \n",
    "        dec_outs_fr, _ = decoder_fr(french_sentences.unsqueeze(2).transpose(0,1), encoder_outputs_en[0].transpose(0,1), memory_lengths=english_sentences_lengths)\n",
    "        \n",
    "        # Encoding - Decoding for French -> English\n",
    "        encoder_outputs_fr = encoder_fr(french_sentences)\n",
    "        decoder_en.init_state(french_sentences.unsqueeze(2).transpose(0,1), None, None) \n",
    "        dec_outs_en, _ = decoder_en(english_sentences.unsqueeze(2).transpose(0,1), encoder_outputs_fr[0].transpose(0,1), memory_lengths=french_sentences_lengths)\n",
    "        \n",
    "        if (regularize == \"hidden_state\"):\n",
    "            # using the pooled outputs of the encoders for regularizing \n",
    "            discriminator_outputs_en = discriminator(encoder_outputs_en[1])\n",
    "            discriminator_outputs_fr = discriminator(encoder_outputs_fr[1])\n",
    "            discriminator_outputs_cat = torch.cat((discriminator_outputs_en, discriminator_outputs_fr))\n",
    "            discriminator_labels = torch.tensor([1.0]*discriminator_outputs_en.shape[0] + [0.0]*discriminator_outputs_fr.shape[0])\n",
    "            discriminator_labels = discriminator_labels.unsqueeze(1).to(device=device)\n",
    "            \n",
    "            loss = loss_fn_hidden_regularization(english_sentences,\n",
    "                                               french_sentences,\n",
    "                                               dec_outs_en,\n",
    "                                               dec_outs_fr,\n",
    "                                               discriminator_labels,\n",
    "                                               discriminator_outputs_cat,\n",
    "                                              )\n",
    "        else:\n",
    "            loss = loss_fn_no_regularization(english_sentences,\n",
    "                                           french_sentences,\n",
    "                                           dec_outs_en,\n",
    "                                           dec_outs_fr,\n",
    "                                          )\n",
    "        \n",
    "        if (batch_num % 100 == 0):\n",
    "            with torch.no_grad():\n",
    "                for batch_num, batch in enumerate(val_data_iter):\n",
    "                    # Reading in input and moving to device\n",
    "                    (english_sentences_val, english_sentences_lengths_val), (french_sentences_val, french_sentences_lengths_val) = batch\n",
    "                    english_sentences_val = english_sentences_val.to(device=device)\n",
    "                    english_sentences_lengths_val = english_sentences_lengths_val.to(device=device)\n",
    "                    french_sentences_val = french_sentences_val.to(device=device)\n",
    "                    french_sentences_lengths_val = french_sentences_lengths_val.to(device=device)\n",
    "\n",
    "                    # Encoding - Decoding for English -> French\n",
    "                    encoder_outputs_en = encoder_en(english_sentences_val)\n",
    "                    decoder_fr.init_state(english_sentences_val.unsqueeze(2).transpose(0,1), None, None) \n",
    "                    dec_outs_fr, _ = decoder_fr(french_sentences_val.unsqueeze(2).transpose(0,1), encoder_outputs_en[0].transpose(0,1), memory_lengths=english_sentences_lengths_val)\n",
    "                    \n",
    "                    # Calculate BLUE Scores and EM \n",
    "                    return \n",
    "                    \n",
    "\n",
    "        print(\"Batch num {}: Loss {}\".format(batch_num, loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 0: Loss 0.023226561024785042\n",
      "Batch num 1: Loss 0.022844761610031128\n",
      "Batch num 2: Loss 0.022143220528960228\n",
      "Batch num 3: Loss 0.021185748279094696\n",
      "Batch num 4: Loss 0.02003978192806244\n",
      "Batch num 5: Loss 0.018770121037960052\n",
      "Batch num 6: Loss 0.017434770241379738\n",
      "Batch num 7: Loss 0.01608271710574627\n",
      "Batch num 8: Loss 0.014753120020031929\n",
      "Batch num 9: Loss 0.013474980369210243\n",
      "Batch num 10: Loss 0.012268390506505966\n",
      "Batch num 11: Loss 0.011144986376166344\n",
      "Batch num 12: Loss 0.010111508890986443\n",
      "Batch num 13: Loss 0.009169865399599075\n",
      "Batch num 14: Loss 0.008317830972373486\n",
      "Batch num 15: Loss 0.0075512295588850975\n",
      "Batch num 16: Loss 0.006864524446427822\n",
      "Batch num 17: Loss 0.006251282058656216\n",
      "Batch num 18: Loss 0.0057050003670156\n",
      "Batch num 19: Loss 0.005218992009758949\n",
      "Batch num 20: Loss 0.004786944016814232\n",
      "Batch num 21: Loss 0.004402622580528259\n",
      "Batch num 22: Loss 0.00406078714877367\n",
      "Batch num 23: Loss 0.0037563624791800976\n",
      "Batch num 24: Loss 0.003485016757622361\n",
      "Batch num 25: Loss 0.0032429236453026533\n",
      "Batch num 26: Loss 0.0030264875385910273\n",
      "Batch num 27: Loss 0.0028325081802904606\n",
      "Batch num 28: Loss 0.002658405341207981\n",
      "Batch num 29: Loss 0.00250181183218956\n",
      "Batch num 30: Loss 0.0023606319446116686\n",
      "Batch num 31: Loss 0.002233095932751894\n",
      "Batch num 32: Loss 0.002117639873176813\n",
      "Batch num 33: Loss 0.002012788550928235\n",
      "Batch num 34: Loss 0.0019174821209162474\n",
      "Batch num 35: Loss 0.0018305701669305563\n",
      "Batch num 36: Loss 0.001751140458509326\n",
      "Batch num 37: Loss 0.0016784134786576033\n",
      "Batch num 38: Loss 0.0016116618644446135\n",
      "Batch num 39: Loss 0.0015501875896006823\n",
      "Batch num 40: Loss 0.0014935897197574377\n",
      "Batch num 41: Loss 0.0014412740711122751\n",
      "Batch num 42: Loss 0.001392824575304985\n",
      "Batch num 43: Loss 0.0013478846522048116\n",
      "Batch num 44: Loss 0.0013060977216809988\n",
      "Batch num 45: Loss 0.001267166342586279\n",
      "Batch num 46: Loss 0.001230733934789896\n",
      "Batch num 47: Loss 0.0011967930477112532\n",
      "Batch num 48: Loss 0.0011648901272565126\n",
      "Batch num 49: Loss 0.0011349807027727365\n",
      "Batch num 50: Loss 0.0011068414896726608\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-786b1a7126fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-188264f132be>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_iter, regularize)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch num {}: Loss {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/mtl_bert_environment/lib/python3.5/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: \n",
    "* train without any regularizing terms - train with regularizing terms on the hidden layers, train with regularizing terms on the attention, train with regularizing terms on both \n",
    "* Write up validation metrics and print out validation at intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
