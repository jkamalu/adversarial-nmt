{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL EXPERIMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "import torch\n",
    "from torch.cuda import is_available\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import transformers\n",
    "from modules.data import TextDataset, Collator\n",
    "from modules.model import Encoder, Decoder, Discriminator, Hook \n",
    "from modules import utils\n",
    "\n",
    "experiment = \"\"\n",
    "config = utils.load_config(\"config/{}.yml\".format(experiment))\n",
    "\n",
    "if is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"WARNING: CUDA IS NOT AVAILABLE\".format(device))\n",
    "    \n",
    "ckpt_dir = \"experiments/{}/checkpoints\".format(experiment)\n",
    "runs_dir = \"experiments/{}/tensorboard\".format(experiment)\n",
    "\n",
    "try: \n",
    "    os.makedirs(ckpt_dir)\n",
    "    os.makedirs(runs_dir)\n",
    "except FileExistsError: \n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build tokenizer for English and French\n",
    "tokenizer_en = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_fr = transformers.CamembertTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "# Build TextDataset for train and valid\n",
    "data_path = utils.data_path(\"europarl-v7\")\n",
    "dataset_train = TextDataset(\n",
    "    data_path, \n",
    "    tokenizer_en, \n",
    "    tokenizer_fr, \n",
    "    training=True, \n",
    "    minlen=config[\"minlen\"],\n",
    "    maxlen=config[\"maxlen\"]\n",
    ")\n",
    "dataset_valid = TextDataset(\n",
    "    data_path, \n",
    "    tokenizer_en, \n",
    "    tokenizer_fr, \n",
    "    training=False, \n",
    "    minlen=config[\"minlen\"],\n",
    "    maxlen=config[\"maxlen\"]\n",
    ")\n",
    "\n",
    "# Build DataLoader for train and valid\n",
    "collator = Collator(maxlen=config[\"maxlen\"])\n",
    "dataloader_train = DataLoader(dataset_train, **config[\"data_loader\"], collate_fn=collator)\n",
    "dataloader_valid = DataLoader(dataset_valid, **config[\"data_loader\"], collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Init BERT or vanilla Transformer encoder w/pretrained embeddings from BERT encoder\n",
    "enc = config[\"encoder\"]\n",
    "\n",
    "bert_en = Encoder.init_from_config(enc, config[\"encoder_kwargs\"][enc], language=\"english\").to(device=device)\n",
    "bert_fr = Encoder.init_from_config(enc, config[\"encoder_kwargs\"][enc], language=\"french\").to(device=device)\n",
    "\n",
    "embeddings_en = bert_en.model.get_input_embeddings().to(device=device)\n",
    "embeddings_fr = bert_fr.model.get_input_embeddings().to(device=device)\n",
    "\n",
    "if config[\"encoder\"] == \"bert\":\n",
    "    encoder_en = bert_en\n",
    "    encoder_fr = bert_fr\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Hooks for attention extration from encoder layers\n",
    "hooks_en = [Hook(layer[1]) for layer in list(encoder_en.named_modules())]\n",
    "hooks_fr = [Hook(layer[1]) for layer in list(encoder_fr.named_modules())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init vanilla Transformer decoder w/pretrained embeddings from BERT encoder\n",
    "dec = config[\"decoder\"]\n",
    "decoder_en = Decoder.init_from_config(dec, config[\"decoder_kwargs\"][dec], embeddings_en).to(device=device)\n",
    "decoder_fr = Decoder.init_from_config(dec, config[\"decoder_kwargs\"][dec], embeddings_fr).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Discriminator(s)\n",
    "discriminators = {}\n",
    "for regularization in config[\"regularization\"][\"type\"]:\n",
    "    discriminators[regularization] = Discriminator(regularization, **config[\"discriminator\"]).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define loss and evaluation metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def loss_fn(real_en, real_fr, pred_en, pred_fr, real_pred_ys={}, ignore_index=1):\n",
    "    '''\n",
    "    Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs\n",
    "    '''\n",
    "    cce_loss = torch.nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "    \n",
    "    loss_en2fr = cce_loss(pred_fr.transpose(1,2), real_fr)\n",
    "    loss_fr2en = cce_loss(pred_en.transpose(1,2), real_en)\n",
    "    \n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    reg_losses = defaultdict(lambda: torch.tensor(0.0))\n",
    "    for regularization in real_pred_ys:\n",
    "        real_y, pred_y = real_pred_ys[regularization]\n",
    "        reg_losses[regularization] = bce_loss(pred_y, real_y)\n",
    "\n",
    "    return loss_en2fr + loss_fr2en + torch.sum(torch.tensor(list(reg_losses.values()))), loss_en2fr, loss_fr2en, reg_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def exact_match(pred, real, ignore_index=1):\n",
    "    '''\n",
    "    Evaluate percent exact match between predictions and ground truth\n",
    "    '''\n",
    "    mask = real != ignore_index\n",
    "    return torch.sum((pred == real) * mask).item() / torch.sum(mask).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining training logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     10,
     22,
     27
    ]
   },
   "outputs": [],
   "source": [
    "def get_optimizer(encoder_en, encoder_fr, decoder_en, decoder_fr, discriminators, **kwargs):\n",
    "    params = (\n",
    "        list(encoder_en.parameters()) + list(encoder_fr.parameters()) +\n",
    "        list(decoder_fr.parameters()) + list(decoder_en.parameters())\n",
    "    )\n",
    "    for regularization in discriminators:\n",
    "        if discriminators[regularization] is not None and regularization != \"none\":\n",
    "            params += list(discriminators[regularization].parameters())\n",
    "    return Adam(params, **kwargs)\n",
    "\n",
    "def switch_trainable(model, step):\n",
    "    switch = step % 2 == 0\n",
    "    if len(model[\"discriminators\"]) > 0:\n",
    "        for module in model:\n",
    "            if module == \"discriminators\":\n",
    "                for regularization in model[module]:\n",
    "                    for param in model[module][regularization].parameters():\n",
    "                        param.requires_grad = not switch\n",
    "            elif module in (\"encoder_en, encoder_fr\"):\n",
    "                for param in model[module].parameters():\n",
    "                    param.requires_grad = switch\n",
    "\n",
    "def save_weights(model, step):\n",
    "    for name, module in model.items(): \n",
    "        if module:\n",
    "            torch.save(module.state_dict(), os.path.join(ckpt_dir, \"{}.{}.pt\".format(step, name)))\n",
    "        \n",
    "def forward(model, batch):\n",
    "        # Unpack batch and move to device\n",
    "        batch_en, batch_fr = batch\n",
    "        sents_en, sents_no_eos_en, lengths_en = map(lambda t: t.to(device=device), batch_en)\n",
    "        sents_fr, sents_no_eos_fr, lengths_fr = map(lambda t: t.to(device=device), batch_fr)\n",
    "\n",
    "        # Encode English to French\n",
    "        enc_out_en = encoder_en(sents_en, lengths=lengths_en)\n",
    "        # Decoder English to French\n",
    "        dec_out_fr = decoder_fr(sents_no_eos_fr, enc_out_en, lengths_en)\n",
    "\n",
    "        # Encoder French to English\n",
    "        enc_out_fr = encoder_fr(sents_fr, lengths=lengths_fr)\n",
    "        # Decoder French to English\n",
    "        dec_out_en = decoder_en(sents_no_eos_en, enc_out_fr, lengths_fr)\n",
    "        \n",
    "        return sents_en, sents_fr, enc_out_en, enc_out_fr, dec_out_en, dec_out_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train(): \n",
    "    ''' \n",
    "    Train the encoding and decoding models. User needs to pass in a valid iterator over the data,\n",
    "    and also specify a type of adversarial regularization. regularize = [\"hidden\", \"attention\", \"both\"]\n",
    "    '''\n",
    "    model = {\n",
    "        \"encoder_en\": encoder_en,\n",
    "        \"encoder_fr\": encoder_fr,\n",
    "        \"decoder_en\": decoder_en,\n",
    "        \"decoder_fr\": decoder_fr,\n",
    "        \"discriminators\": discriminators,\n",
    "    }\n",
    "    \n",
    "    optimizer = get_optimizer(**model, **config[\"adam\"])\n",
    "    \n",
    "    writer = SummaryWriter(runs_dir)\n",
    "                                                   \n",
    "    for batch_i, batch in enumerate(dataloader_train):\n",
    "                        \n",
    "        # Clear optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Alternate trainable for encoder/decoder \n",
    "        # and discriminator parameters\n",
    "        switch_trainable(model, batch_i)\n",
    "\n",
    "        # Save weights and continue training\n",
    "        if batch_i > 0 and batch_i % config[\"checkpoint_frequency\"] == 0:\n",
    "            save_weights(model, batch_i)\n",
    "        \n",
    "        # Save weights and terminate training\n",
    "        if batch_i >= config[\"max_step_num\"]:\n",
    "            save_weights(model, batch_i)\n",
    "            break\n",
    "        \n",
    "        # Unpack the batch, run the encoders, run the decoders\n",
    "        sents_en, sents_fr, enc_out_en, enc_out_fr, dec_out_en, dec_out_fr = forward(model, batch)\n",
    "        \n",
    "        # Initial default values for regularization \n",
    "        real_pred_ys = {}\n",
    "        switch = batch_i % 2 == 0\n",
    "        y_real = [float(switch)] * config[\"batch_size\"] + [float(not switch)] * config[\"batch_size\"]\n",
    "        y_real = torch.tensor(y_real).unsqueeze(-1).to(device=device)\n",
    "        \n",
    "        # Gather attention discriminator labels/predictions\n",
    "        if \"attention\" in model[\"discriminators\"]:\n",
    "            \n",
    "            if (config[\"regularization\"][\"n_affine\"] == 12):\n",
    "                attention_en = torch.cat(list(utils.extract_attention_scores(hooks_en).values()), dim=1)\n",
    "                attention_fr = torch.cat(list(utils.extract_attention_scores(hooks_fr).values()), dim=1)\n",
    "                attention_en = attention_en.view(config[\"batch_size\"], config[\"regularization\"][\"n_affine\"], -1)\n",
    "                attention_fr = attention_fr.view(config[\"batch_size\"], config[\"regularization\"][\"n_affine\"], -1)\n",
    "            else: \n",
    "                # NOTE: this has to be changed individually if different attention needs want to be used \n",
    "                # we only investiage the 7th attention layer \n",
    "                attention_en = utils.extract_attention_scores(hooks_en)[6].view(config[\"batch_size\"], 1, -1)\n",
    "                attention_fr = utils.extract_attention_scores(hooks_fr)[6].view(config[\"batch_size\"], 1, -1)\n",
    "            \n",
    "            y_attn_pred_en = model[\"discriminators\"][\"attention\"](attention_en)\n",
    "            y_attn_pred_fr = model[\"discriminators\"][\"attention\"](attention_fr)\n",
    "            y_attn_pred = torch.cat([y_attn_pred_en, y_attn_pred_fr])\n",
    "            real_pred_ys[\"attention\"] = y_real, y_attn_pred\n",
    "            \n",
    "        # Gather hidden discriminator labels/predictions\n",
    "        if \"hidden\" in model[\"discriminators\"]:\n",
    "            # Use the pooled outputs of the encoders for regularization\n",
    "            y_hddn_pred_en = model[\"discriminators\"][\"hidden\"](enc_out_en)\n",
    "            y_hddn_pred_fr = model[\"discriminators\"][\"hidden\"](enc_out_fr)\n",
    "            y_hddn_pred = torch.cat([y_hddn_pred_en, y_hddn_pred_fr])\n",
    "            real_pred_ys[\"hidden\"] = y_real, y_hddn_pred\n",
    "\n",
    "        loss, loss_en2fr, loss_fr2en, reg_losses = loss_fn(\n",
    "            sents_en[:, 1:], sents_fr[:, 1:],\n",
    "            dec_out_en, dec_out_fr, \n",
    "            real_pred_ys,\n",
    "            ignore_index=1\n",
    "        )\n",
    "        \n",
    "        # Optimize trainable parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Write training losses/metrics to stdout and tensorboard\n",
    "        if batch_i % config[\"log_frequency\"] == 0:\n",
    "            print(\"Train batch {}: loss {}, en-fr {}, fr-en {}, hddn {}\".format(\n",
    "                batch_i, loss.item(), loss_en2fr.item(), loss_fr2en.item(), reg_losses[\"hidden\"].item()))\n",
    "            \n",
    "            cce_metrics = {\"en-fr\": loss_en2fr.item(), \"fr-en\": loss_fr2en.item()}\n",
    "            utils.write_to_tensorboard(\"CCE\", cce_metrics, training=True, step=batch_i, writer=writer)\n",
    "            bce_metrics = {\"attn\": reg_losses[\"attention\"].item(), \"hddn\": reg_losses[\"hidden\"].item()}\n",
    "            utils.write_to_tensorboard(\"BCE\", bce_metrics, training=True, step=batch_i, writer=writer)\n",
    "\n",
    "        # Running validation script  \n",
    "        if batch_i > 0 and batch_i % config[\"val_frequency\"] == 0:\n",
    "            _loss_en2fr_val = []\n",
    "            _loss_fr2en_val = []                \n",
    "\n",
    "            _bleu_en2fr = []\n",
    "            _bleu_fr2en = []\n",
    "\n",
    "            _em_en2fr = []\n",
    "            _em_fr2en = []\n",
    "            \n",
    "            with torch.no_grad():    \n",
    "\n",
    "                for batch_j, batch in enumerate(dataloader_valid):\n",
    "                    \n",
    "                    if (batch_j == config[\"n_valid\"]):\n",
    "                        break\n",
    "                    \n",
    "                    # Unpack the batch, run the encoders, run the decoders\n",
    "                    sents_en, sents_fr, enc_out_en, enc_out_fr, dec_out_en, dec_out_fr = forward(model, batch)\n",
    "                    \n",
    "                    # Calculate BLUE Scores, EM and Perplexity\n",
    "                    preds_fr = torch.argmax(dec_out_fr, dim=-1)\n",
    "                    preds_en = torch.argmax(dec_out_en, dim=-1)\n",
    "                    \n",
    "                    _, loss_en2fr_val, loss_fr2en_val, _ = loss_fn(\n",
    "                        sents_en[:, 1:], sents_fr[:, 1:],\n",
    "                        dec_out_en, dec_out_fr,\n",
    "                        ignore_index=1\n",
    "                    )\n",
    "                    _loss_en2fr_val.append(loss_en2fr_val.item())\n",
    "                    _loss_fr2en_val.append(loss_fr2en_val.item())\n",
    "                    \n",
    "                    for idx in range(config[\"batch_size\"]):\n",
    "                        text_real_fr = tokenizer_fr.decode(sents_fr[idx, 1:-1].tolist())\n",
    "                        text_pred_fr = tokenizer_fr.decode(preds_fr[idx, 0:-1].tolist())\n",
    "                        _bleu_en2fr.append(sentence_bleu([text_real_fr], text_pred_fr))\n",
    "                        \n",
    "                        text_real_en = tokenizer_en.decode(sents_en[idx, 1:-1].tolist())\n",
    "                        text_pred_en = tokenizer_en.decode(preds_en[idx, 0:-1].tolist())\n",
    "                        _bleu_fr2en.append(sentence_bleu([text_real_en], text_pred_en))\n",
    "                        \n",
    "                        # print(\"FR-REAL:\", text_real_fr, \"\\n\")\n",
    "                        # print(\"FR-PRED:\", text_pred_fr, \"\\n\")\n",
    "                        # print(\"EN-REAL:\", text_real_en, \"\\n\")\n",
    "                        # print(\"EN-PRED:\", text_pred_en, \"\\n\")\n",
    "\n",
    "                    _em_en2fr.append(exact_match(preds_fr[:, 0:-1], sents_fr[:, 1:-1]))\n",
    "                    _em_fr2en.append(exact_match(preds_en[:, 0:-1], sents_en[:, 1:-1]))\n",
    "\n",
    "            avg_em_en2fr = sum(_em_en2fr) / len(_em_en2fr)\n",
    "            avg_em_fr2en = sum(_em_fr2en) / len(_em_fr2en)\n",
    "            avg_bleu_en2fr = sum(_bleu_en2fr) / len(_bleu_en2fr)\n",
    "            avg_bleu_fr2en = sum(_bleu_fr2en) / len(_bleu_fr2en)\n",
    "            avg_loss_en2fr_val = sum(_loss_en2fr_val) / len(_loss_en2fr_val)\n",
    "            avg_loss_fr2en_val = sum(_loss_fr2en_val) / len(_loss_fr2en_val)\n",
    "\n",
    "            bleu_metrics = {\"en-fr\": avg_bleu_en2fr, \"fr-en\": avg_bleu_fr2en}\n",
    "            utils.write_to_tensorboard(\"BLEU\", bleu_metrics, training=False, step=batch_i, writer=writer)\n",
    "\n",
    "            em_metrics = {\"en-fr\": avg_em_en2fr, \"fr-en\": avg_em_fr2en}\n",
    "            utils.write_to_tensorboard(\"EM\", em_metrics, training=False, step=batch_i, writer=writer)\n",
    "\n",
    "            loss_val_metrics = {\"en-fr\": avg_loss_en2fr_val, \"fr-en\": avg_loss_fr2en_val}\n",
    "            utils.write_to_tensorboard(\"CCE\", loss_val_metrics, training=False, step=batch_i, writer=writer)\n",
    "            \n",
    "            print(\"Val bleu batch {}: en-fr {}, fr-en {}\".format(\n",
    "                batch_i, avg_bleu_en2fr, avg_bleu_fr2en))\n",
    "            \n",
    "            print(\"Val em batch {}: en-fr {}, fr-en {}\".format(\n",
    "                batch_i, avg_em_en2fr, avg_em_fr2en))\n",
    "            \n",
    "            print(\"Val loss batch {}: en-fr {}, fr-en {}\".format(\n",
    "                batch_i, avg_loss_en2fr_val, avg_loss_fr2en_val))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
