{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMT with Adversarial Regularization\n",
    "\n",
    "#### 1. Load data\n",
    "\n",
    "Load 2 Mil. Europarl v7 fr-en sentence pairs\n",
    "\n",
    "#### 2. Build model\n",
    "\n",
    "Initialize the encoder, decoder, and discriminator architectures\n",
    "\n",
    "    experiment parameters:\n",
    "        - encoder = {Transformer, RoBERTa, CamemBERT}\n",
    "        - decoder = {Transformer}\n",
    "\n",
    "#### 3. Define loss/metric functions\n",
    "\n",
    "Define the sequence cross entropy and adversarial loss functions\n",
    "\n",
    "    experiment parameters:\n",
    "        - regularization = {encoder attention, latent variable, both}\n",
    "\n",
    "#### 4. Define training logic\n",
    "\n",
    "Define the optimizer and training loop for an arbitrary configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch.cuda import is_available\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from modules.lib.huggingface import transformers\n",
    "from modules.data import TextDataset, Collator\n",
    "from modules.model import Embeddings, Encoder, Decoder, Discriminator, Hook \n",
    "from modules import utils\n",
    "\n",
    "# TODO replace: \n",
    "experiment = \"transformer_none\"\n",
    "config = utils.load_config(\"config/{}.yml\".format(experiment))\n",
    "\n",
    "if is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"WARNING: CUDA IS NOT AVAILABLE\".format(device))\n",
    "    \n",
    "ckpt_dir = \"experiments/{}/checkpoints\".format(experiment)\n",
    "runs_dir = \"experiments/{}/tensorboard\".format(experiment)\n",
    "os.makedirs(ckpt_dir)\n",
    "os.makedirs(runs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tokenizer for English and French\n",
    "tokenizer_en = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_fr = transformers.CamembertTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "# Build TextDataset for train and valid\n",
    "data_path = utils.data_path(\"europarl-v7\")\n",
    "dataset_train = TextDataset(\n",
    "    data_path, \n",
    "    tokenizer_en, \n",
    "    tokenizer_fr, \n",
    "    training=True, \n",
    "    minlen=config[\"minlen\"],\n",
    "    maxlen=config[\"maxlen\"],\n",
    "    size=config[\"n_train\"]\n",
    ")\n",
    "dataset_valid = TextDataset(\n",
    "    data_path, \n",
    "    tokenizer_en, \n",
    "    tokenizer_fr, \n",
    "    training=False, \n",
    "    minlen=config[\"minlen\"],\n",
    "    maxlen=config[\"maxlen\"],\n",
    "    size=config[\"n_valid\"]\n",
    ")\n",
    "\n",
    "# Build DataLoader for train and valid\n",
    "collator = Collator(maxlen=config[\"maxlen\"])\n",
    "dataloader_train = DataLoader(dataset_train, **config[\"data_loader\"], collate_fn=collator)\n",
    "dataloader_valid = DataLoader(dataset_valid, **config[\"data_loader\"], collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Model\n",
    "\n",
    "1. Using the Output Embedding to Improve Language Models - http://arxiv.org/abs/1608.05859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init BERT encoder w/pretrained weights\n",
    "bert_en = Encoder.init_bert(\"english\").to(device=device)\n",
    "bert_fr = Encoder.init_bert(\"french\").to(device=device)\n",
    "\n",
    "# Init embeddings w/pretrained weights from BERT encoder\n",
    "embeddings_en = Embeddings.from_pretrained(bert_en.model.get_input_embeddings()).to(device=device)\n",
    "embeddings_fr = Embeddings.from_pretrained(bert_fr.model.get_input_embeddings()).to(device=device)\n",
    "\n",
    "if config[\"use_bert\"]:\n",
    "    # Use BERT encoder\n",
    "    encoder_en = bert_en\n",
    "    encoder_fr = bert_fr\n",
    "else:\n",
    "    # Init vanilla Transformer encoder w/pretrained embeddings from BERT encoder\n",
    "    del bert_en\n",
    "    del bert_fr\n",
    "    encoder_en = Encoder.init_vanilla(**config[\"vanilla_encoder\"], embeddings=embeddings_en).to(device=device)\n",
    "    encoder_fr = Encoder.init_vanilla(**config[\"vanilla_encoder\"], embeddings=embeddings_fr).to(device=device)\n",
    "\n",
    "# Init vanilla Transformer decoder w/pretrained embeddings from BERT encoder\n",
    "decoder_en = Decoder(**config[\"vanilla_decoder\"], embeddings=embeddings_en).to(device=device)\n",
    "decoder_fr = Decoder(**config[\"vanilla_decoder\"], embeddings=embeddings_fr).to(device=device)\n",
    "\n",
    "# Init Discriminator\n",
    "attn_discriminator = None\n",
    "hidden_discriminator = None\n",
    "if (config[\"regularization\"][\"type\"] == \"attention\"):\n",
    "    attn_discriminator = Discriminator(config[\"maxlen\"] ** 2, 1, config[\"regularization\"][\"n_affine\"]).to(device=device)\n",
    "if (config[\"regularization\"][\"type\"] == \"hidden\"): \n",
    "    hidden_discriminator = Discriminator(config[\"d_model\"], 1).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define loss/metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_no_regularization(real_en, real_fr, pred_en, pred_fr, ignore_index=1):\n",
    "    '''\n",
    "    Standard machine translation cross entropy loss\n",
    "    '''\n",
    "    cce_loss = torch.nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "    loss_en2fr = cce_loss(pred_fr.transpose(1,2), real_fr)\n",
    "    loss_fr2en = cce_loss(pred_en.transpose(1,2), real_en)\n",
    "    return loss_en2fr + loss_fr2en, loss_en2fr, loss_fr2en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_single_regularization(real_en, real_fr, pred_en, pred_fr, real_y, pred_y):\n",
    "    '''\n",
    "    Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs\n",
    "    '''\n",
    "    crossentropy_term, loss_en2fr, loss_fr2en = loss_fn_no_regularization(real_en, real_gt, pred_en, pred_fr)\n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    regularizing_term = bce_loss(pred_y, real_y)\n",
    "    return crossentropy_term + regularizing_term, loss_en2fr, loss_fr2en, regularizing_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_multi_regularization(real_en, real_fr, pred_en, pred_fr, real_ys, pred_ys):\n",
    "    '''\n",
    "    Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs\n",
    "    '''\n",
    "    crossentropy_term, loss_en2fr, loss_fr2en = loss_fn_no_regularization(real_en, real_gt, pred_en, pred_fr)    \n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    regularizing_terms = []\n",
    "    for real_y, pred_y in zip(real_ys, pred_ys):\n",
    "        regularizing_terms.append(bce_loss(pred_y, real_y))\n",
    "    return crossentropy_term + sum(regularizing_terms), loss_en2fr, loss_fr2en, regularizing_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(pred, real, ignore_index=1):\n",
    "    '''\n",
    "    Evaluate percent exact match between predictions and ground truth\n",
    "    '''\n",
    "    mask = real != ignore_index\n",
    "    return torch.sum((pred == real) * mask).item() / torch.sum(mask).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining training logic\n",
    "\n",
    "1. What Does BERT Look At? An Analysis of BERT's Attention - https://arxiv.org/abs/1906.04341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(encoder_en, encoder_fr, decoder_en, decoder_fr, attn_discriminator=None, hidden_discriminator=None, **kwargs):\n",
    "    params = (\n",
    "        list(encoder_en.parameters()) + list(encoder_fr.parameters()) +\n",
    "        list(decoder_fr.parameters()) + list(decoder_en.parameters())\n",
    "    )\n",
    "    if hidden_discriminator is not None:\n",
    "        params += list(hidden_discriminator.parameters())\n",
    "    if attn_discriminator is not None: \n",
    "        params += list(attn_discriminator.parameters())\n",
    "    return Adam(params, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(\n",
    "    encoder_en, encoder_fr, \n",
    "    decoder_en, decoder_fr,\n",
    "    attn_discriminator = attn_discriminator,\n",
    "    hidden_discriminator = hidden_discriminator,\n",
    "    **config[\"adam\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hooks_en = [Hook(layer[1]) for layer in list(encoder_en.named_modules())]\n",
    "_hooks_fr = [Hook(layer[1]) for layer in list(encoder_fr.named_modules())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"encoder_en\": encoder_en,\n",
    "    \"encoder_fr\": encoder_fr,\n",
    "    \"decoder_en\": decoder_en,\n",
    "    \"decoder_fr\": decoder_fr,\n",
    "    \"attn_discriminator\": attn_discriminator,\n",
    "    \"hidden_discriminator\": hidden_discriminator,\n",
    "}\n",
    "\n",
    "def train(model, dataloader_train, dataloader_valid, optimizer, tokenizer_en, tokenizer_fr, regularization=None): \n",
    "    ''' \n",
    "    Train the encoding and decoding models. User needs to pass in a valid iterator over the data,\n",
    "    and also specify a type of adversarial regularization. regularize = [\"hidden\", \"attention\"]\n",
    "    '''\n",
    "    writer = SummaryWriter(\"runs/{}\".format(regularization))\n",
    "                                                   \n",
    "    for batch_i, batch in enumerate(dataloader_train):\n",
    "        \n",
    "        if (batch_i == config[\"checkpoint_frequency\"]):   \n",
    "            for _module_name, _model in model.items(): \n",
    "                _model.save_state_dict(os.path.join(ckpt_dir, \"{}.{}.pt\".format(batch_i,_module_name)))\n",
    "        \n",
    "        if (batch_i >= config[\"max_step_num\"]):\n",
    "            for _module_name, _model in model.items(): \n",
    "                _model.save_state_dict(os.path.join(ckpt_dir, \"{}.{}.pt\".format(batch_i,_module_name)))\n",
    "                break \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        switch = batch_i % 2 == 0\n",
    "        \n",
    "        if regularization is not None:\n",
    "            for module in model:\n",
    "                if \"discriminator\" in module:\n",
    "                    for param in model[module].parameters():\n",
    "                        param.requires_grad = not switch\n",
    "                elif \"encoder\" in module:\n",
    "                    for param in model[module].parameters():\n",
    "                        param.requires_grad = switch\n",
    "\n",
    "        # Read in input and move to device\n",
    "        batch_en, batch_fr = batch\n",
    "        sents_en, sents_no_eos_en, lengths_en = batch_en\n",
    "        sents_fr, sents_no_eos_fr, lengths_fr = batch_fr\n",
    "\n",
    "        sents_en = sents_en.to(device=device)\n",
    "        sents_no_eos_en = sents_no_eos_en.to(device=device)\n",
    "        lengths_en = lengths_en.to(device=device)\n",
    "        \n",
    "        sents_fr = sents_fr.to(device=device)\n",
    "        sents_no_eos_fr = sents_no_eos_fr.to(device=device)\n",
    "        lengths_fr = lengths_fr.to(device=device)\n",
    "\n",
    "        # Encoding/Decoding for en -> fr\n",
    "        enc_out_en = encoder_en(sents_en)\n",
    "        decoder_fr.init_state(sents_en.unsqueeze(2).transpose(0,1), None, None)\n",
    "        dec_out_fr, _ = decoder_fr(\n",
    "            sents_no_eos_fr.unsqueeze(2).transpose(0,1), \n",
    "            enc_out_en[0].transpose(0,1), \n",
    "            memory_lengths=lengths_en\n",
    "        )\n",
    "\n",
    "        # Encoding/Decoding for fr -> en\n",
    "        enc_out_fr = encoder_fr(sents_fr)\n",
    "        decoder_en.init_state(sents_fr.unsqueeze(2).transpose(0,1), None, None) \n",
    "        dec_out_en, _ = decoder_en(\n",
    "            sents_no_eos_en.unsqueeze(2).transpose(0,1), \n",
    "            enc_out_fr[0].transpose(0,1), \n",
    "            memory_lengths=lengths_fr\n",
    "        )\n",
    "        \n",
    "        # Initial default values for regularization \n",
    "        attention_regularization = torch.tensor(0.0)\n",
    "        hidden_regularization = torch.tensor(0.0)\n",
    "\n",
    "        if regularization == \"attention\":\n",
    "            \n",
    "            batch_size = len(sents_en)\n",
    "            \n",
    "            attention_en = utils.extract_attention_scores(_hooks_en)[6]\n",
    "            attention_fr = utils.extract_attention_scores(_hooks_fr)[6]\n",
    "            attention_en = attention_en.view(batch_size, -1)\n",
    "            attention_fr = attention_fr.view(batch_size, -1)\n",
    "            \n",
    "            discriminator_output_en = attn_discriminator(attention_en)\n",
    "            discriminator_output_fr = attn_discriminator(attention_fr)\n",
    "            discriminator_output = torch.cat([discriminator_output_en, discriminator_output_fr])\n",
    "            \n",
    "            switch = batch_i % 2 == 0\n",
    "            discriminator_labels = torch.tensor([float(switch)] * batch_size + [float(not switch)] * batch_size)\n",
    "            discriminator_labels = discriminator_labels.unsqueeze(1).to(device=device)\n",
    "\n",
    "            losses = loss_fn_single_regularization(\n",
    "                sents_en[:, 1:],\n",
    "                sents_fr[:, 1:],\n",
    "                dec_outs_en,\n",
    "                dec_outs_fr,\n",
    "                discriminator_labels,\n",
    "                discriminator_output\n",
    "            )\n",
    "            \n",
    "            loss, loss_en2fr, loss_fr2en, attention_regularization = losses\n",
    "            \n",
    "        elif regularization == \"hidden\":\n",
    "            # Use the pooled outputs of the encoders for regularization\n",
    "            discriminator_output_en = hidden_discriminator(enc_out_en[1])\n",
    "            discriminator_output_fr = hidden_discriminator(enc_out_fr[1])\n",
    "            discriminator_output = torch.cat((discriminator_output_en, discriminator_output_fr))\n",
    "            \n",
    "            switch = batch_i % 2 == 0\n",
    "            discriminator_labels = torch.tensor([float(switch)] * batch_size + [float(not switch)] * batch_size)\n",
    "            discriminator_labels = discriminator_labels.unsqueeze(1).to(device=device)\n",
    "\n",
    "            losses = loss_fn_single_regularization(\n",
    "                sents_en[:, 1:],\n",
    "                sents_fr[:, 1:],\n",
    "                dec_outs_en,\n",
    "                dec_outs_fr,\n",
    "                discriminator_labels,\n",
    "                discriminator_output\n",
    "            )\n",
    "            \n",
    "            loss, loss_en2fr, loss_fr2en, hidden_regularization = losses\n",
    "        elif regularization == \"both\":\n",
    "            \n",
    "            batch_size = len(sents_en)\n",
    "            \n",
    "            # Applying attention regularization  \n",
    "            attention_en = utils.extract_attention_scores(_hooks_en)[6]\n",
    "            attention_fr = utils.extract_attention_scores(_hooks_fr)[6]\n",
    "            attention_en = attention_en.view(batch_size, -1)\n",
    "            attention_fr = attention_fr.view(batch_size, -1)\n",
    "            \n",
    "            attn_discriminator_output_en = attn_discriminator(attention_en)\n",
    "            attn_discriminator_output_fr = attn_discriminator(attention_fr)\n",
    "            attn_discriminator_output = torch.cat([attn_discriminator_output_en, attn_discriminator_output_fr])\n",
    "            \n",
    "            # Applying hidden regularization\n",
    "            hidden_discriminator_output_en = hidden_discriminator(enc_out_en[1])\n",
    "            hidden_discriminator_output_fr = hidden_discriminator(enc_out_fr[1])\n",
    "            hidden_discriminator_output = torch.cat((hidden_discriminator_output_en, hidden_discriminator_output_fr))\n",
    "            \n",
    "            # Creating labels\n",
    "            switch = batch_i % 2 == 0\n",
    "            discriminator_labels = torch.tensor([float(switch)] * batch_size + [float(not switch)] * batch_size)\n",
    "            discriminator_labels = [discriminator_labels.unsqueeze(1).to(device=device),\\\n",
    "                                    discriminator_labels.unsqueeze(1).to(device=device)]\n",
    "            \n",
    "            discriminator_output = [attn_discriminator_output, hidden_discriminator_output]\n",
    "\n",
    "            losses = loss_fn_multi_regularization(\n",
    "                sents_en[:, 1:],\n",
    "                sents_fr[:, 1:],\n",
    "                dec_outs_en,\n",
    "                dec_outs_fr,\n",
    "                discriminator_labels,\n",
    "                discriminator_output\n",
    "            )\n",
    "            loss = losses[0]\n",
    "            attention_regularization, hidden_regularization = losses[-1]\n",
    "\n",
    "        else:\n",
    "            losses = loss_fn_no_regularization(\n",
    "                sents_en[:, 1:],\n",
    "                sents_fr[:, 1:],\n",
    "                dec_outs_en,\n",
    "                dec_outs_fr\n",
    "            )\n",
    "            \n",
    "            loss, loss_en2fr, loss_fr2en = losses\n",
    "            \n",
    "        if (batch_i % 50 == 0):\n",
    "            print(\"Batch {}: Loss {}\".format(batch_i, loss.item()))\n",
    "\n",
    "        # Optimizer weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Write training losses/metrics to tensorboard\n",
    "        cce_metrics = {\"en-fr\": loss_en2fr.item(), \"fr-en\": loss_fr2en.item()}\n",
    "        utils.write_to_tensorboard(\"CCE\", cce_metrics, training=True, step=batch_i, writer=writer)\n",
    "        if regularization is not None:\n",
    "            bce_metrics = {\"attention_regularization\": attention_regularization.item(),\n",
    "                          \"hidden_regularization\": hidden_regularization.item()}\n",
    "            utils.write_to_tensorboard(\"BCE\", bce_metrics, training=True, step=batch_i, writer=writer)\n",
    "\n",
    "        # Running validation script  \n",
    "        if (batch_i > 0 and batch_i % 500 == 0):\n",
    "            with torch.no_grad():\n",
    "                _blue_scores_en2fr = []\n",
    "                _exact_matches_en2fr = []\n",
    "                _blue_scores_fr2en = []\n",
    "                _exact_matches_fr2en = []\n",
    "                _val_loss_en2fr = []\n",
    "                _val_loss_en2fr = []\n",
    "                for batch_j, batch in enumerate(dataloader_valid):\n",
    "                    \n",
    "                    if (batch_j == 50):\n",
    "                        break\n",
    "                    \n",
    "                    # Read in input and move to device\n",
    "                    batch_en, batch_fr = batch\n",
    "                    sents_en, sents_no_eos_en, lengths_en = batch_en\n",
    "                    sents_fr, sents_no_eos_fr, lengths_fr = batch_fr\n",
    "\n",
    "                    sents_en = sents_en.to(device=device)\n",
    "                    sents_no_eos_en = sents_no_eos_en.to(device=device)\n",
    "                    lengths_en = lengths_en.to(device=device)\n",
    "\n",
    "                    sents_fr = sents_fr.to(device=device)\n",
    "                    sents_no_eos_fr = sents_no_eos_fr.to(device=device)\n",
    "                    lengths_fr = lengths_fr.to(device=device)\n",
    "\n",
    "                    # Encoding/Decoding for en -> fr\n",
    "                    enc_out_en = encoder_en(sents_en)\n",
    "                    decoder_fr.init_state(sents_en.unsqueeze(2).transpose(0,1), None, None)\n",
    "                    dec_out_fr, _ = decoder_fr(\n",
    "                        sents_no_eos_fr.unsqueeze(2).transpose(0,1), \n",
    "                        enc_out_en[0].transpose(0,1), \n",
    "                        memory_lengths=lengths_en\n",
    "                    )\n",
    "\n",
    "                    # Encoding/Decoding for fr -> en\n",
    "                    enc_out_fr = encoder_fr(sents_fr)\n",
    "                    decoder_en.init_state(sents_fr.unsqueeze(2).transpose(0,1), None, None) \n",
    "                    dec_out_en, _ = decoder_en(\n",
    "                        sents_no_eos_en.unsqueeze(2).transpose(0,1), \n",
    "                        enc_out_fr[0].transpose(0,1), \n",
    "                        memory_lengths=lengths_fr\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate BLUE Scores, EM and Perplexity\n",
    "                    preds_fr = torch.argmax(dec_out_fr, dim=2)\n",
    "                    preds_en = torch.argmax(dec_out_en, dim=2)\n",
    "                    \n",
    "                    _, val_loss_en2fr, val_loss_fr2en = loss_fn_no_regularization(sents_en[:, 1:], sents_fr[:, 1:], dec_outs_en, dec_outs_fr)\n",
    "                    _val_loss_en2fr.append(val_loss_en2fr.item())\n",
    "                    _val_loss_fr2en.append(val_loss_fr2en.item())\n",
    "                    \n",
    "                    for idx in range(batch_size):\n",
    "                        detokenized_real_fr = tokenizer_fr.convert_tokens_to_string(sents_fr[idx, 1:].tolist())\n",
    "                        detokenized_pred_fr = tokenizer_fr.convert_tokens_to_string(preds_fr[idx].tolist())\n",
    "                        _blue_scores_en2fr.append(sentence_bleu(detokenized_real_fr, detokenized_pred_fr))\n",
    "                        \n",
    "                        detokenized_real_en = tokenizer_en.convert_tokens_to_string(sents_en[idx, 1:].tolist())\n",
    "                        detokenized_pred_en = tokenizer_en.convert_tokens_to_string(preds_en[idx].tolist())\n",
    "                        _blue_scores_fr2en.append(sentence_bleu(detokenized_real_en, detokenized_pred_en))\n",
    "                        \n",
    "                    _exact_matches_en2fr.append(exact_match(preds_fr, sents_fr[:, 1:]))\n",
    "                    _exact_matches_fr2en.append(exact_match(preds_en, sents_en[:, 1:]))\n",
    "                    \n",
    "                                                   \n",
    "                avg_bleu_en2fr = sum(_blue_scores_en2fr) / len(_blue_scores_en2fr)\n",
    "                avg_bleu_fr2en = sum(_blue_scores_fr2en) / len(_blue_scores_fr2en)\n",
    "                avg_em_en2fr = sum(_exact_matches_en2fr) / len(_exact_matches_en2fr)\n",
    "                avg_em_fr2en = sum(_exact_matches_fr2en) / len(_exact_matches_fr2en)\n",
    "                avg_loss_en2fr = sum(_val_loss_en2fr) / len(_val_loss_en2fr)\n",
    "                avg_loss_fr2en = sum(_val_loss_fr2en) / len(_val_loss_fr2en)\n",
    "                \n",
    "                bleu_metrics = {\"en-fr\": avg_bleu_en2fr, \"fr-en\":avg_bleu_fr2en}\n",
    "                write_to_tensorboard(\"BLEU\", bleu_metrics, training=False, step=batch_i, writer=writer)\n",
    "                \n",
    "                exact_match_metrics = {\"en-fr\": avg_em_en2fr, \"fr-en\":avg_em_fr2en}\n",
    "                write_to_tensorboard(\"EM\", exact_match_metrics, training=False, step=batch_i, writer=writer)\n",
    "                \n",
    "                val_loss_metrics = {\"en-fr\": avg_loss_en2fr, \"fr-en\":avg_loss_fr2en}\n",
    "                write_to_tensorboard(\"CE_LOSS\", val_loss_metrics, training=False, step=batch_i, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(model, dataloader_train, dataloader_valid, optimizer, regularize=config[\"regularization\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do for final paper: \n",
    "* Add Tensorboard stuff \n",
    "* Print out accuracy for the encoder\n",
    "* Create Perplexity evaluation metric\n",
    "* Run example with discriminator over both the attention and hidden\n",
    "* Factorize Code into util and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
