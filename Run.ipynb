{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMT with Adversarial Regularization\n",
    "\n",
    "#### 1. Load data\n",
    "\n",
    "Load 2 Mil. Europarl v7 fr-en sentence pairs\n",
    "\n",
    "#### 2. Build model\n",
    "\n",
    "Initialize the encoder, decoder, and discriminator architectures\n",
    "\n",
    "    experiment parameters:\n",
    "        - encoder = {Transformer, RoBERTa, CamemBERT}\n",
    "        - decoder = {Transformer}\n",
    "\n",
    "#### 3. Define loss/metric functions\n",
    "\n",
    "Define the sequence cross entropy and adversarial loss functions\n",
    "\n",
    "    experiment parameters:\n",
    "        - regularization = {encoder attention, latent variable, both}\n",
    "\n",
    "#### 4. Define training logic\n",
    "\n",
    "Define the optimizer and training loop for an arbitrary configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from torch.cuda import is_available\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from modules.lib.huggingface import transformers\n",
    "from modules.data import TextDataset, Collator\n",
    "from modules.model import Embeddings, Encoder, Decoder, Discriminator, Hook \n",
    "from modules import utils\n",
    "\n",
    "# TODO replace: \n",
    "experiment = \"transformer_none\"\n",
    "config = utils.load_config(\"config/{}.yml\".format(experiment))\n",
    "\n",
    "if is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"WARNING: CUDA IS NOT AVAILABLE\".format(device))\n",
    "    \n",
    "ckpt_dir = \"experiments/{}/checkpoints\".format(experiment)\n",
    "runs_dir = \"experiments/{}/tensorboard\".format(experiment)\n",
    "\n",
    "try: \n",
    "    os.makedirs(ckpt_dir)\n",
    "    os.makedirs(runs_dir)\n",
    "except FileExistsError: \n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Loading in pre-saved file: /home/john_kamalu/unidirectional-NMT/data/europarl-v7/data.train.pkl\n",
      "**Loading in pre-saved file: /home/john_kamalu/unidirectional-NMT/data/europarl-v7/data.val.pkl\n"
     ]
    }
   ],
   "source": [
    "# Build tokenizer for English and French\n",
    "tokenizer_en = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_fr = transformers.CamembertTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "# Build TextDataset for train and valid\n",
    "data_path = utils.data_path(\"europarl-v7\")\n",
    "dataset_train = TextDataset(\n",
    "    data_path, \n",
    "    tokenizer_en, \n",
    "    tokenizer_fr, \n",
    "    training=True, \n",
    "    minlen=config[\"minlen\"],\n",
    "    maxlen=config[\"maxlen\"],\n",
    "    size=config[\"n_train\"]\n",
    ")\n",
    "dataset_valid = TextDataset(\n",
    "    data_path, \n",
    "    tokenizer_en, \n",
    "    tokenizer_fr, \n",
    "    training=False, \n",
    "    minlen=config[\"minlen\"],\n",
    "    maxlen=config[\"maxlen\"],\n",
    "    size=config[\"n_valid\"]\n",
    ")\n",
    "\n",
    "# Build DataLoader for train and valid\n",
    "collator = Collator(maxlen=config[\"maxlen\"])\n",
    "dataloader_train = DataLoader(dataset_train, **config[\"data_loader\"], collate_fn=collator)\n",
    "dataloader_valid = DataLoader(dataset_valid, **config[\"data_loader\"], collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Model\n",
    "\n",
    "1. Using the Output Embedding to Improve Language Models - http://arxiv.org/abs/1608.05859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init BERT encoder w/pretrained weights\n",
    "bert_en = Encoder.init_bert(\"english\").to(device=device)\n",
    "bert_fr = Encoder.init_bert(\"french\").to(device=device)\n",
    "\n",
    "# Init embeddings w/pretrained weights from BERT encoder\n",
    "embeddings_en = Embeddings.from_pretrained(bert_en.model.get_input_embeddings()).to(device=device)\n",
    "embeddings_fr = Embeddings.from_pretrained(bert_fr.model.get_input_embeddings()).to(device=device)\n",
    "\n",
    "if config[\"use_bert\"]:\n",
    "    # Use BERT encoder\n",
    "    encoder_en = bert_en\n",
    "    encoder_fr = bert_fr\n",
    "else:\n",
    "    # Init vanilla Transformer encoder w/pretrained embeddings from BERT encoder\n",
    "    del bert_en\n",
    "    del bert_fr\n",
    "    encoder_en = Encoder.init_vanilla(**config[\"vanilla_encoder\"], embeddings=embeddings_en).to(device=device)\n",
    "    encoder_fr = Encoder.init_vanilla(**config[\"vanilla_encoder\"], embeddings=embeddings_fr).to(device=device)\n",
    "\n",
    "# Init vanilla Transformer decoder w/pretrained embeddings from BERT encoder\n",
    "decoder_en = Decoder(**config[\"vanilla_decoder\"], embeddings=embeddings_en).to(device=device)\n",
    "decoder_fr = Decoder(**config[\"vanilla_decoder\"], embeddings=embeddings_fr).to(device=device)\n",
    "\n",
    "# Init Discriminator\n",
    "attn_discriminator = None\n",
    "hidden_discriminator = None\n",
    "if (config[\"regularization\"][\"type\"] == \"attention\"):\n",
    "    attn_discriminator = Discriminator(config[\"maxlen\"] ** 2, 1, config[\"regularization\"][\"n_affine\"]).to(device=device)\n",
    "if (config[\"regularization\"][\"type\"] == \"hidden\"): \n",
    "    hidden_discriminator = Discriminator(config[\"d_model\"], 1).to(device=device)\n",
    "if (config[\"regularization\"][\"type\"] == \"both\"): \n",
    "    attn_discriminator = Discriminator(config[\"maxlen\"] ** 2, 1, config[\"regularization\"][\"n_affine\"]).to(device=device)\n",
    "    hidden_discriminator = Discriminator(config[\"d_model\"], 1).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define loss/metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_no_regularization(real_en, real_fr, pred_en, pred_fr, ignore_index=1):\n",
    "    '''\n",
    "    Standard machine translation cross entropy loss\n",
    "    '''\n",
    "    cce_loss = torch.nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "    loss_en2fr = cce_loss(pred_fr.transpose(1,2), real_fr)\n",
    "    loss_fr2en = cce_loss(pred_en.transpose(1,2), real_en)\n",
    "    return loss_en2fr + loss_fr2en, loss_en2fr, loss_fr2en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_single_regularization(real_en, real_fr, pred_en, pred_fr, real_y, pred_y):\n",
    "    '''\n",
    "    Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs\n",
    "    '''\n",
    "    crossentropy_term, loss_en2fr, loss_fr2en = loss_fn_no_regularization(real_en, real_gt, pred_en, pred_fr)\n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    regularizing_term = bce_loss(pred_y, real_y)\n",
    "    return crossentropy_term + regularizing_term, loss_en2fr, loss_fr2en, regularizing_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_multi_regularization(real_en, real_fr, pred_en, pred_fr, real_ys, pred_ys):\n",
    "    '''\n",
    "    Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs\n",
    "    '''\n",
    "    crossentropy_term, loss_en2fr, loss_fr2en = loss_fn_no_regularization(real_en, real_gt, pred_en, pred_fr)    \n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    regularizing_terms = []\n",
    "    for real_y, pred_y in zip(real_ys, pred_ys):\n",
    "        regularizing_terms.append(bce_loss(pred_y, real_y))\n",
    "    return crossentropy_term + sum(regularizing_terms), loss_en2fr, loss_fr2en, regularizing_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(pred, real, ignore_index=1):\n",
    "    '''\n",
    "    Evaluate percent exact match between predictions and ground truth\n",
    "    '''\n",
    "    mask = real != ignore_index\n",
    "    return torch.sum((pred == real) * mask).item() / torch.sum(mask).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining training logic\n",
    "\n",
    "1. What Does BERT Look At? An Analysis of BERT's Attention - https://arxiv.org/abs/1906.04341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(encoder_en, encoder_fr, decoder_en, decoder_fr, attn_discriminator=None, hidden_discriminator=None, **kwargs):\n",
    "    params = (\n",
    "        list(encoder_en.parameters()) + list(encoder_fr.parameters()) +\n",
    "        list(decoder_fr.parameters()) + list(decoder_en.parameters())\n",
    "    )\n",
    "    if hidden_discriminator is not None:\n",
    "        params += list(hidden_discriminator.parameters())\n",
    "    if attn_discriminator is not None: \n",
    "        params += list(attn_discriminator.parameters())\n",
    "    return Adam(params, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(\n",
    "    encoder_en, encoder_fr, \n",
    "    decoder_en, decoder_fr,\n",
    "    attn_discriminator = attn_discriminator,\n",
    "    hidden_discriminator = hidden_discriminator,\n",
    "    **config[\"adam\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hooks_en = [Hook(layer[1]) for layer in list(encoder_en.named_modules())]\n",
    "_hooks_fr = [Hook(layer[1]) for layer in list(encoder_fr.named_modules())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"encoder_en\": encoder_en,\n",
    "    \"encoder_fr\": encoder_fr,\n",
    "    \"decoder_en\": decoder_en,\n",
    "    \"decoder_fr\": decoder_fr,\n",
    "    \"attn_discriminator\": attn_discriminator,\n",
    "    \"hidden_discriminator\": hidden_discriminator,\n",
    "}\n",
    "\n",
    "def train(model, dataloader_train, dataloader_valid, optimizer, regularization): \n",
    "    ''' \n",
    "    Train the encoding and decoding models. User needs to pass in a valid iterator over the data,\n",
    "    and also specify a type of adversarial regularization. regularize = [\"hidden\", \"attention\", \"both\"]\n",
    "    '''\n",
    "    writer = SummaryWriter(runs_dir)\n",
    "                                                   \n",
    "    for batch_i, batch in enumerate(dataloader_train):\n",
    "        \n",
    "        if (batch_i == config[\"checkpoint_frequency\"]):   \n",
    "            for _module_name, _model in model.items(): \n",
    "                _model.save_state_dict(os.path.join(ckpt_dir, \"{}.{}.pt\".format(batch_i,_module_name)))\n",
    "        \n",
    "        if (batch_i >= config[\"max_step_num\"]):\n",
    "            for _module_name, _model in model.items(): \n",
    "                _model.save_state_dict(os.path.join(ckpt_dir, \"{}.{}.pt\".format(batch_i,_module_name)))\n",
    "            break \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        switch = batch_i % 2 == 0\n",
    "        \n",
    "        if regularization[\"type\"] != \"none\":\n",
    "            print(regularization[\"type\"], type(regularization[\"type\"]))\n",
    "            for module in model:\n",
    "                if \"discriminator\" in module:\n",
    "                    for param in model[module].parameters():\n",
    "                        param.requires_grad = not switch\n",
    "                elif \"encoder\" in module:\n",
    "                    for param in model[module].parameters():\n",
    "                        param.requires_grad = switch\n",
    "\n",
    "        # Read in input and move to device\n",
    "        batch_en, batch_fr = batch\n",
    "        sents_en, sents_no_eos_en, lengths_en = batch_en\n",
    "        sents_fr, sents_no_eos_fr, lengths_fr = batch_fr\n",
    "        \n",
    "        if not config[\"use_bert\"]:\n",
    "            sents_en = sents_en.unsqueeze(-1)\n",
    "            sents_fr = sents_fr.unsqueeze(-1)\n",
    "\n",
    "        sents_en = sents_en.to(device=device)\n",
    "        sents_no_eos_en = sents_no_eos_en.to(device=device)\n",
    "        lengths_en = lengths_en.to(device=device)\n",
    "        \n",
    "        sents_fr = sents_fr.to(device=device)\n",
    "        sents_no_eos_fr = sents_no_eos_fr.to(device=device)\n",
    "        lengths_fr = lengths_fr.to(device=device)\n",
    "\n",
    "        # Encoding/Decoding for en -> fr\n",
    "        if not config[\"use_bert\"]:\n",
    "            enc_out_en = encoder_en(sents_en.transpose(0,1), lengths=lengths_en)\n",
    "        else:\n",
    "            enc_out_en = encoder_en(sents_en)\n",
    "        if not config[\"use_bert\"]:\n",
    "            enc_out_en[1] = enc_out_en[1].transpose(0,1)\n",
    "        decoder_fr.init_state(sents_en.unsqueeze(2).transpose(0,1), None, None)\n",
    "        dec_out_fr, _ = decoder_fr(\n",
    "            sents_no_eos_fr.unsqueeze(2).transpose(0,1), \n",
    "            enc_out_en[1].transpose(0,1), \n",
    "            memory_lengths=lengths_en\n",
    "        )\n",
    "\n",
    "        # Encoding/Decoding for fr -> en\n",
    "        if not config[\"use_bert\"]:\n",
    "            enc_out_fr = encoder_fr(sents_fr.transpose(0,1), lengths=lengths_fr)\n",
    "        else:\n",
    "            enc_out_fr = encoder_fr(sents_fr)\n",
    "        if not config[\"use_bert\"]:\n",
    "            enc_out_fr[1] = enc_out_fr[1].transpose(0,1)\n",
    "        decoder_en.init_state(sents_fr.unsqueeze(2).transpose(0,1), None, None) \n",
    "        dec_out_en, _ = decoder_en(\n",
    "            sents_no_eos_en.unsqueeze(2).transpose(0,1), \n",
    "            enc_out_fr[1].transpose(0,1), \n",
    "            memory_lengths=lengths_fr\n",
    "        )\n",
    "        \n",
    "        # Initial default values for regularization \n",
    "        attention_regularization = torch.tensor(0.0)\n",
    "        hidden_regularization = torch.tensor(0.0)\n",
    "\n",
    "        if regularization[\"type\"] == \"attention\":\n",
    "            \n",
    "            batch_size = len(sents_en)\n",
    "            \n",
    "            attention_en = utils.extract_attention_scores(_hooks_en)[6]\n",
    "            attention_fr = utils.extract_attention_scores(_hooks_fr)[6]\n",
    "            attention_en = attention_en.view(batch_size, -1)\n",
    "            attention_fr = attention_fr.view(batch_size, -1)\n",
    "            \n",
    "            discriminator_output_en = attn_discriminator(attention_en)\n",
    "            discriminator_output_fr = attn_discriminator(attention_fr)\n",
    "            discriminator_output = torch.cat([discriminator_output_en, discriminator_output_fr])\n",
    "            \n",
    "            switch = batch_i % 2 == 0\n",
    "            discriminator_labels = torch.tensor([float(switch)] * batch_size + [float(not switch)] * batch_size)\n",
    "            discriminator_labels = discriminator_labels.unsqueeze(1).to(device=device)\n",
    "\n",
    "            losses = loss_fn_single_regularization(\n",
    "                sents_en[:, 1:],\n",
    "                sents_fr[:, 1:],\n",
    "                dec_outs_en,\n",
    "                dec_outs_fr,\n",
    "                discriminator_labels,\n",
    "                discriminator_output\n",
    "            )\n",
    "            \n",
    "            loss, loss_en2fr, loss_fr2en, attention_regularization = losses\n",
    "            \n",
    "        elif regularization[\"type\"] == \"hidden\":\n",
    "            # Use the pooled outputs of the encoders for regularization\n",
    "            discriminator_output_en = hidden_discriminator(enc_out_en[1])\n",
    "            discriminator_output_fr = hidden_discriminator(enc_out_fr[1])\n",
    "            discriminator_output = torch.cat((discriminator_output_en, discriminator_output_fr))\n",
    "            \n",
    "            switch = batch_i % 2 == 0\n",
    "            discriminator_labels = torch.tensor([float(switch)] * batch_size + [float(not switch)] * batch_size)\n",
    "            discriminator_labels = discriminator_labels.unsqueeze(1).to(device=device)\n",
    "\n",
    "            losses = loss_fn_single_regularization(\n",
    "                sents_en[:, 1:],\n",
    "                sents_fr[:, 1:],\n",
    "                dec_outs_en,\n",
    "                dec_outs_fr,\n",
    "                discriminator_labels,\n",
    "                discriminator_output\n",
    "            )\n",
    "            \n",
    "            loss, loss_en2fr, loss_fr2en, hidden_regularization = losses\n",
    "        elif regularization[\"type\"] == \"both\":\n",
    "            \n",
    "            batch_size = len(sents_en)\n",
    "            \n",
    "            # Applying attention regularization  \n",
    "            attention_en = utils.extract_attention_scores(_hooks_en)[6]\n",
    "            attention_fr = utils.extract_attention_scores(_hooks_fr)[6]\n",
    "            attention_en = attention_en.view(batch_size, -1)\n",
    "            attention_fr = attention_fr.view(batch_size, -1)\n",
    "            \n",
    "            attn_discriminator_output_en = attn_discriminator(attention_en)\n",
    "            attn_discriminator_output_fr = attn_discriminator(attention_fr)\n",
    "            attn_discriminator_output = torch.cat([attn_discriminator_output_en, attn_discriminator_output_fr])\n",
    "            \n",
    "            # Applying hidden regularization\n",
    "            hidden_discriminator_output_en = hidden_discriminator(enc_out_en[1])\n",
    "            hidden_discriminator_output_fr = hidden_discriminator(enc_out_fr[1])\n",
    "            hidden_discriminator_output = torch.cat((hidden_discriminator_output_en, hidden_discriminator_output_fr))\n",
    "            \n",
    "            # Creating labels\n",
    "            switch = batch_i % 2 == 0\n",
    "            discriminator_labels = torch.tensor([float(switch)] * batch_size + [float(not switch)] * batch_size)\n",
    "            discriminator_labels = [discriminator_labels.unsqueeze(1).to(device=device),\\\n",
    "                                    discriminator_labels.unsqueeze(1).to(device=device)]\n",
    "            \n",
    "            discriminator_output = [attn_discriminator_output, hidden_discriminator_output]\n",
    "\n",
    "            losses = loss_fn_multi_regularization(\n",
    "                sents_en[:, 1:],\n",
    "                sents_fr[:, 1:],\n",
    "                dec_outs_en,\n",
    "                dec_outs_fr,\n",
    "                discriminator_labels,\n",
    "                discriminator_output\n",
    "            )\n",
    "            loss = losses[0]\n",
    "            attention_regularization, hidden_regularization = losses[-1]\n",
    "\n",
    "        else:\n",
    "            losses = loss_fn_no_regularization(\n",
    "                sents_en[:, 1:],\n",
    "                sents_fr[:, 1:],\n",
    "                dec_outs_en,\n",
    "                dec_outs_fr\n",
    "            )\n",
    "            \n",
    "            loss, loss_en2fr, loss_fr2en = losses\n",
    "            \n",
    "        if (batch_i % 50 == 0):\n",
    "            print(\"Batch {}: Loss {}\".format(batch_i, loss.item()))\n",
    "\n",
    "        # Optimizer weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Write training losses/metrics to tensorboard\n",
    "        cce_metrics = {\"en-fr\": loss_en2fr.item(), \"fr-en\": loss_fr2en.item()}\n",
    "        utils.write_to_tensorboard(\"CCE\", cce_metrics, training=True, step=batch_i, writer=writer)\n",
    "        if regularization is not None:\n",
    "            bce_metrics = {\"attention_regularization\": attention_regularization.item(),\n",
    "                          \"hidden_regularization\": hidden_regularization.item()}\n",
    "            utils.write_to_tensorboard(\"BCE\", bce_metrics, training=True, step=batch_i, writer=writer)\n",
    "\n",
    "        # Running validation script  \n",
    "        if (batch_i > 0 and batch_i % 500 == 0):\n",
    "            with torch.no_grad():\n",
    "                _blue_scores_en2fr = []\n",
    "                _exact_matches_en2fr = []\n",
    "                _blue_scores_fr2en = []\n",
    "                _exact_matches_fr2en = []\n",
    "                _val_loss_en2fr = []\n",
    "                _val_loss_en2fr = []\n",
    "                for batch_j, batch in enumerate(dataloader_valid):\n",
    "                    \n",
    "                    if (batch_j == 50):\n",
    "                        break\n",
    "                    \n",
    "                    # Read in input and move to device\n",
    "                    batch_en, batch_fr = batch\n",
    "                    sents_en, sents_no_eos_en, lengths_en = batch_en\n",
    "                    sents_fr, sents_no_eos_fr, lengths_fr = batch_fr\n",
    "\n",
    "                    sents_en = sents_en.to(device=device)\n",
    "                    sents_no_eos_en = sents_no_eos_en.to(device=device)\n",
    "                    lengths_en = lengths_en.to(device=device)\n",
    "\n",
    "                    sents_fr = sents_fr.to(device=device)\n",
    "                    sents_no_eos_fr = sents_no_eos_fr.to(device=device)\n",
    "                    lengths_fr = lengths_fr.to(device=device)\n",
    "\n",
    "                    # Encoding/Decoding for en -> fr\n",
    "                    enc_out_en = encoder_en(sents_en)\n",
    "                    decoder_fr.init_state(sents_en.unsqueeze(2).transpose(0,1), None, None)\n",
    "                    dec_out_fr, _ = decoder_fr(\n",
    "                        sents_no_eos_fr.unsqueeze(2).transpose(0,1), \n",
    "                        enc_out_en[0].transpose(0,1), \n",
    "                        memory_lengths=lengths_en\n",
    "                    )\n",
    "\n",
    "                    # Encoding/Decoding for fr -> en\n",
    "                    enc_out_fr = encoder_fr(sents_fr)\n",
    "                    decoder_en.init_state(sents_fr.unsqueeze(2).transpose(0,1), None, None) \n",
    "                    dec_out_en, _ = decoder_en(\n",
    "                        sents_no_eos_en.unsqueeze(2).transpose(0,1), \n",
    "                        enc_out_fr[0].transpose(0,1), \n",
    "                        memory_lengths=lengths_fr\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate BLUE Scores, EM and Perplexity\n",
    "                    preds_fr = torch.argmax(dec_out_fr, dim=2)\n",
    "                    preds_en = torch.argmax(dec_out_en, dim=2)\n",
    "                    \n",
    "                    _, val_loss_en2fr, val_loss_fr2en = loss_fn_no_regularization(sents_en[:, 1:], sents_fr[:, 1:], dec_outs_en, dec_outs_fr)\n",
    "                    _val_loss_en2fr.append(val_loss_en2fr.item())\n",
    "                    _val_loss_fr2en.append(val_loss_fr2en.item())\n",
    "                    \n",
    "                    for idx in range(batch_size):\n",
    "                        detokenized_real_fr = tokenizer_fr.convert_tokens_to_string(sents_fr[idx, 1:].tolist())\n",
    "                        detokenized_pred_fr = tokenizer_fr.convert_tokens_to_string(preds_fr[idx].tolist())\n",
    "                        _blue_scores_en2fr.append(sentence_bleu(detokenized_real_fr, detokenized_pred_fr))\n",
    "                        \n",
    "                        detokenized_real_en = tokenizer_en.convert_tokens_to_string(sents_en[idx, 1:].tolist())\n",
    "                        detokenized_pred_en = tokenizer_en.convert_tokens_to_string(preds_en[idx].tolist())\n",
    "                        _blue_scores_fr2en.append(sentence_bleu(detokenized_real_en, detokenized_pred_en))\n",
    "                        \n",
    "                    _exact_matches_en2fr.append(exact_match(preds_fr, sents_fr[:, 1:]))\n",
    "                    _exact_matches_fr2en.append(exact_match(preds_en, sents_en[:, 1:]))\n",
    "                    \n",
    "                                                   \n",
    "                avg_bleu_en2fr = sum(_blue_scores_en2fr) / len(_blue_scores_en2fr)\n",
    "                avg_bleu_fr2en = sum(_blue_scores_fr2en) / len(_blue_scores_fr2en)\n",
    "                avg_em_en2fr = sum(_exact_matches_en2fr) / len(_exact_matches_en2fr)\n",
    "                avg_em_fr2en = sum(_exact_matches_fr2en) / len(_exact_matches_fr2en)\n",
    "                avg_loss_en2fr = sum(_val_loss_en2fr) / len(_val_loss_en2fr)\n",
    "                avg_loss_fr2en = sum(_val_loss_fr2en) / len(_val_loss_fr2en)\n",
    "                \n",
    "                bleu_metrics = {\"en-fr\": avg_bleu_en2fr, \"fr-en\":avg_bleu_fr2en}\n",
    "                write_to_tensorboard(\"BLEU\", bleu_metrics, training=False, step=batch_i, writer=writer)\n",
    "                \n",
    "                exact_match_metrics = {\"en-fr\": avg_em_en2fr, \"fr-en\":avg_em_fr2en}\n",
    "                write_to_tensorboard(\"EM\", exact_match_metrics, training=False, step=batch_i, writer=writer)\n",
    "                \n",
    "                val_loss_metrics = {\"en-fr\": avg_loss_en2fr, \"fr-en\":avg_loss_fr2en}\n",
    "                write_to_tensorboard(\"CE_LOSS\", val_loss_metrics, training=False, step=batch_i, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0],\n",
      "        [ 9226],\n",
      "        [11459],\n",
      "        [   45],\n",
      "        [  129],\n",
      "        [    7],\n",
      "        [    5],\n",
      "        [ 7401],\n",
      "        [    8],\n",
      "        [13135],\n",
      "        [    9],\n",
      "        [15569],\n",
      "        [   53],\n",
      "        [   67],\n",
      "        [    7],\n",
      "        [   49],\n",
      "        [ 1318],\n",
      "        [    4],\n",
      "        [    2],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1]], device='cuda:0') tensor(19, device='cuda:0')\n",
      "sents_en.shape = torch.Size([16, 50, 1])\n",
      "sents_en.transpose(0,1).shape = torch.Size([50, 16, 1])\n",
      "sents_no_eos_fr.shape = torch.Size([16, 49])\n",
      "lengths_en.shape = torch.Size([16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (48) must match the size of tensor b (50) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1226116900ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"regularization\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-b24ffe1d0fb7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader_train, dataloader_valid, optimizer, regularization)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_bert\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0menc_out_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_en\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlengths_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0menc_out_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_en\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/unidirectional-NMT/modules/model/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, lengths)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mencoder_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Output of the encoder needs to have pooled layer in second dimension\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.7/site-packages/onmt/encoders/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, lengths)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# Run the forward pass of every layer of the tranformer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.7/site-packages/onmt/encoders/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0minput_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         context, _ = self.self_attn(input_norm, input_norm, input_norm,\n\u001b[0;32m---> 50\u001b[0;31m                                     mask=mask, attn_type=\"self\")\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.7/site-packages/onmt/modules/multi_headed_attn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, key, value, query, mask, layer_cache, attn_type)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, 1, 1, T_values]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# 3) Apply attention dropout and compute context vectors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (48) must match the size of tensor b (50) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "train(model, dataloader_train, dataloader_valid, optimizer, config[\"regularization\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
