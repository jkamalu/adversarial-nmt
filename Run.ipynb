{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMT with Adversarial Regularization\n",
    "\n",
    "#### 1. Load data\n",
    "\n",
    "Load 2 Mil. Europarl v7 fr-en sentence pairs\n",
    "\n",
    "#### 2. Build model\n",
    "\n",
    "Initialize the encoder, decoder, and discriminator architectures\n",
    "\n",
    "    experiment parameters:\n",
    "        - encoder = {Transformer, RoBERTa, CamemBERT}\n",
    "        - decoder = {Transformer}\n",
    "\n",
    "#### 3. Define loss/metric functions\n",
    "\n",
    "Define the sequence cross entropy and adversarial loss functions\n",
    "\n",
    "    experiment parameters:\n",
    "        - regularization = {encoder attention, latent variable, both}\n",
    "\n",
    "#### 4. Define training logic\n",
    "\n",
    "Define the optimizer and training loop for an arbitrary configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.cuda import is_available\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from modules.lib.huggingface import transformers\n",
    "from modules.data import TextDataset, Collator\n",
    "from modules.model import Embeddings, Encoder, Decoder, Discriminator, Hook \n",
    "from modules import utils\n",
    "\n",
    "# TODO replace: \n",
    "experiment = \"transformer_none\"\n",
    "config = utils.load_config(\"config/config.yml\".format(experiment))\n",
    "\n",
    "if is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"WARNING: CUDA IS NOT AVAILABLE\".format(device))\n",
    "    \n",
    "ckpt_dir = \"experiments/{}/checkpoints\".format(experiment)\n",
    "runs_dir = \"experiments/{}/tensorboard\".format(experiment)\n",
    "\n",
    "try: \n",
    "    os.makedirs(ckpt_dir)\n",
    "    os.makedirs(runs_dir)\n",
    "except FileExistsError: \n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build tokenizer for English and French\n",
    "tokenizer_en = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_fr = transformers.CamembertTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "# Build TextDataset for train and valid\n",
    "data_path = utils.data_path(\"europarl-v7\")\n",
    "dataset_train = TextDataset(\n",
    "    data_path, \n",
    "    tokenizer_en, \n",
    "    tokenizer_fr, \n",
    "    training=True, \n",
    "    minlen=config[\"minlen\"],\n",
    "    maxlen=config[\"maxlen\"]\n",
    ")\n",
    "dataset_valid = TextDataset(\n",
    "    data_path, \n",
    "    tokenizer_en, \n",
    "    tokenizer_fr, \n",
    "    training=False, \n",
    "    minlen=config[\"minlen\"],\n",
    "    maxlen=config[\"maxlen\"]\n",
    ")\n",
    "\n",
    "# Build DataLoader for train and valid\n",
    "collator = Collator(maxlen=config[\"maxlen\"])\n",
    "dataloader_train = DataLoader(dataset_train, **config[\"data_loader\"], collate_fn=collator)\n",
    "dataloader_valid = DataLoader(dataset_valid, **config[\"data_loader\"], collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Model\n",
    "\n",
    "1. Using the Output Embedding to Improve Language Models - http://arxiv.org/abs/1608.05859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init BERT encoder w/pretrained weights\n",
    "bert_en = Encoder.init_bert(\"english\").to(device=device)\n",
    "bert_fr = Encoder.init_bert(\"french\").to(device=device)\n",
    "\n",
    "# Init embeddings w/pretrained weights from BERT encoder\n",
    "embeddings_en = Embeddings.from_pretrained(bert_en.model.get_input_embeddings()).to(device=device)\n",
    "embeddings_fr = Embeddings.from_pretrained(bert_fr.model.get_input_embeddings()).to(device=device)\n",
    "\n",
    "if config[\"use_bert\"]:\n",
    "    # Use BERT encoder\n",
    "    encoder_en = bert_en\n",
    "    encoder_fr = bert_fr\n",
    "else:\n",
    "    # Init vanilla Transformer encoder w/pretrained embeddings from BERT encoder\n",
    "    del bert_en\n",
    "    del bert_fr\n",
    "    encoder_en = Encoder.init_vanilla(**config[\"vanilla_encoder\"], embeddings=embeddings_en).to(device=device)\n",
    "    encoder_fr = Encoder.init_vanilla(**config[\"vanilla_encoder\"], embeddings=embeddings_fr).to(device=device)\n",
    "\n",
    "# Init Hooks for encoder layers\n",
    "hooks_en = [Hook(layer[1]) for layer in list(encoder_en.named_modules())]\n",
    "hooks_fr = [Hook(layer[1]) for layer in list(encoder_fr.named_modules())]\n",
    "    \n",
    "# Init vanilla Transformer decoder w/pretrained embeddings from BERT encoder\n",
    "decoder_en = Decoder(**config[\"vanilla_decoder\"], embeddings=embeddings_en).to(device=device)\n",
    "decoder_fr = Decoder(**config[\"vanilla_decoder\"], embeddings=embeddings_fr).to(device=device)\n",
    "\n",
    "# Init Discriminator(s)\n",
    "discriminators = {}\n",
    "for regularization in config[\"regularization\"][\"type\"]:\n",
    "    discriminators[regularization] = Discriminator(regularization, **config[\"discriminator\"]).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define loss/metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(real_en, real_fr, pred_en, pred_fr, real_pred_ys={}, ignore_index=1):\n",
    "    '''\n",
    "    Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs\n",
    "    '''\n",
    "    cce_loss = torch.nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "    loss_en2fr = cce_loss(pred_fr.transpose(1,2), real_fr)\n",
    "    loss_fr2en = cce_loss(pred_en.transpose(1,2), real_en)\n",
    "    \n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    reg_losses = defaultdict(lambda: tensor(0.0))\n",
    "    for regularization in real_pred_ys:\n",
    "        real_y, pred_y = real_pred_ys[regularization]\n",
    "        reg_losses[regularization] = bce_loss(pred_y, real_y)\n",
    "    return cce_loss + sum(reg_losses.values()), loss_en2fr, loss_fr2en, reg_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(pred, real, ignore_index=1):\n",
    "    '''\n",
    "    Evaluate percent exact match between predictions and ground truth\n",
    "    '''\n",
    "    mask = real != ignore_index\n",
    "    return torch.sum((pred == real) * mask).item() / torch.sum(mask).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining training logic\n",
    "\n",
    "1. What Does BERT Look At? An Analysis of BERT's Attention - https://arxiv.org/abs/1906.04341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     10,
     22
    ]
   },
   "outputs": [],
   "source": [
    "def get_optimizer(encoder_en, encoder_fr, decoder_en, decoder_fr, discriminators, **kwargs):\n",
    "    params = (\n",
    "        list(encoder_en.parameters()) + list(encoder_fr.parameters()) +\n",
    "        list(decoder_fr.parameters()) + list(decoder_en.parameters())\n",
    "    )\n",
    "    for regularization in discriminators:\n",
    "        if discriminators[regularization] is not None and regularization != \"none\":\n",
    "            params += list(discriminators[regularization].parameters())\n",
    "    return Adam(params, **kwargs)\n",
    "\n",
    "def switch_trainable(model, step):\n",
    "    switch = step % 2 == 0\n",
    "    if len(model[\"discriminators\"]) > 0:\n",
    "        for module in model:\n",
    "            if module == \"discriminators\":\n",
    "                for regularization in model[module]:\n",
    "                    for param in model[module][regularization].parameters():\n",
    "                        param.requires_grad = not switch\n",
    "            elif module in (\"encoder_en, encoder_fr\"):\n",
    "                for param in model[module].parameters():\n",
    "                    param.requires_grad = switch\n",
    "\n",
    "def save_weights(model, step):\n",
    "    for name, module in model.items(): \n",
    "        module.save_state_dict(os.path.join(ckpt_dir, \"{}.{}.pt\".format(step, name)))\n",
    "        \n",
    "def forward(model, batch):\n",
    "        # Unpack batch and move to device\n",
    "        batch_en, batch_fr = batch\n",
    "        sents_en, sents_no_eos_en, lengths_en = map(lambda t: t.to(device=device), batch_en)\n",
    "        sents_fr, sents_no_eos_fr, lengths_fr = map(lambda t: t.to(device=device), batch_fr)\n",
    "\n",
    "        # Encode English to French\n",
    "        enc_out_en = encoder_en(sents_en, lengths=lengths_en)\n",
    "        # Decoder English to French\n",
    "        decoder_fr.init_state(sents_en)\n",
    "        dec_out_fr = decoder_fr(sents_no_eos_fr, enc_out_en, memory_lengths=lengths_en)\n",
    "\n",
    "        # Encoder French to English\n",
    "        enc_out_fr = encoder_fr(sents_fr, lengths=lengths_fr)\n",
    "        # Decoder French to English\n",
    "        decoder_en.init_state(sents_fr)\n",
    "        dec_out_en = decoder_en(sents_no_eos_en, enc_out_fr, memory_lengths=lengths_fr)\n",
    "        \n",
    "        return sents_en, sents_fr, enc_out_en, enc_out_fr, dec_out_en, dec_out_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train(): \n",
    "    ''' \n",
    "    Train the encoding and decoding models. User needs to pass in a valid iterator over the data,\n",
    "    and also specify a type of adversarial regularization. regularize = [\"hidden\", \"attention\", \"both\"]\n",
    "    '''\n",
    "    model = {\n",
    "        \"encoder_en\": encoder_en,\n",
    "        \"encoder_fr\": encoder_fr,\n",
    "        \"decoder_en\": decoder_en,\n",
    "        \"decoder_fr\": decoder_fr,\n",
    "        \"discriminators\": discriminators,\n",
    "    }\n",
    "    \n",
    "    optimizer = get_optimizer(**model, **config[\"adam\"])\n",
    "    \n",
    "    writer = SummaryWriter(runs_dir)\n",
    "                                                   \n",
    "    for batch_i, batch in enumerate(dataloader_train):\n",
    "        \n",
    "        # Clear optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Alternate trainable for encoder/decoder \n",
    "        # and discriminator parameters\n",
    "        switch_trainable(model, batch_i)\n",
    "\n",
    "        # Save weights and continue training\n",
    "        if batch_i > 0 and batch_i % config[\"checkpoint_frequency\"] == 0:\n",
    "            save_weights(model, batch_i)\n",
    "        \n",
    "        # Save weights and terminate training\n",
    "        if batch_i >= config[\"max_step_num\"]:\n",
    "            save_weights(model, batch_i)\n",
    "            break\n",
    "        \n",
    "        # Unpack the batch, run the encoders, run the decoders\n",
    "        sents_en, sents_fr, enc_out_en, enc_out_fr, dec_out_en, dec_out_fr = forward(model, batch)\n",
    "        \n",
    "        # Initial default values for regularization \n",
    "        real_pred_ys = {}\n",
    "        switch = step % 2 == 0\n",
    "        y_real = [float(switch)] * config[\"batch_size\"] + [float(not switch)] * config[\"batch_size\"]\n",
    "        y_real = torch.tensor(y_real).unsqueeze(-1).to(device=device)\n",
    "        \n",
    "        # Gather attention discriminator labels/predictions\n",
    "        if \"attention\" in model[\"discriminators\"]:\n",
    "            # Use the attention scores of the 6th layer\n",
    "            attention_en = utils.extract_attention_scores(hooks_en)[6].view(config[\"batch_size\"], -1)\n",
    "            attention_fr = utils.extract_attention_scores(hooks_fr)[6].view(config[\"batch_size\"], -1)\n",
    "            y_attn_pred_en = model[\"discriminators\"][\"attention\"](attention_en)\n",
    "            y_attn_pred_fr = model[\"discriminators\"][\"attention\"](attention_fr)\n",
    "            y_attn_pred = torch.cat([y_attn_pred_en, y_attn_pred_fr])\n",
    "            real_pred_ys[\"attention\"] = y_real, y_attn_pred\n",
    "            \n",
    "        # Gather hidden discriminator labels/predictions\n",
    "        if \"attention\" in model[\"discriminators\"]:\n",
    "            # Use the pooled outputs of the encoders for regularization\n",
    "            y_hddn_pred_en = model[\"discriminators\"][\"hidden\"](enc_out_en)\n",
    "            y_hddn_pred_fr = model[\"discriminators\"][\"hidden\"](enc_out_fr)\n",
    "            y_hddn_pred = torch.cat([y_hddn_pred_en, y_hddn_pred_fr])\n",
    "            real_pred_ys[\"hidden\"] = y_real, y_hddn_pred\n",
    "\n",
    "        loss, loss_en2fr, loss_fr2en, reg_losses = loss_fn(\n",
    "            sents_en[:, 1:], sents_fr[:, 1:],\n",
    "            dec_out_en, dec_out_fr, \n",
    "            real_pred_ys,\n",
    "            ignore_index=1\n",
    "        )\n",
    "        \n",
    "        # Optimize trainable parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Write training losses/metrics to stdout and tensorboard\n",
    "        if batch_i % config[\"logging_frequency\"] == 0:\n",
    "            print(\"Batch {}: Loss {}\".format(batch_i, loss.item()))\n",
    "            cce_metrics = {\"en-fr\": loss_en2fr.item(), \"fr-en\": loss_fr2en.item()}\n",
    "            utils.write_to_tensorboard(\"CCE\", cce_metrics, training=True, step=batch_i, writer=writer)\n",
    "            bce_metrics = {\"attn\": reg_losses[\"attention\"].item(), \"hddn\": reg_losses[\"hidden\"].item()}\n",
    "            utils.write_to_tensorboard(\"BCE\", bce_metrics, training=True, step=batch_i, writer=writer)\n",
    "\n",
    "        # Running validation script  \n",
    "        if batch_i > 0 and batch_i % config[\"val_frequency\"] == 0:\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                _loss_en2fr_val = []\n",
    "                _loss_fr2en_val = []                \n",
    "\n",
    "                _bleu_en2fr = []\n",
    "                _bleu_fr2en = []\n",
    "\n",
    "                _em_en2fr = []\n",
    "                _em_fr2en = []\n",
    "                for batch_j, batch in enumerate(dataloader_valid):\n",
    "                    \n",
    "                    if (batch_j == config[\"n_valid\"]):\n",
    "                        break\n",
    "                    \n",
    "                    # Unpack the batch, run the encoders, run the decoders\n",
    "                    sents_en, sents_fr, enc_out_en, enc_out_fr, dec_out_en, dec_out_fr = forward(model, batch)\n",
    "                    \n",
    "                    # Calculate BLUE Scores, EM and Perplexity\n",
    "                    pred_fr = torch.argmax(dec_out_fr, dim=-1)\n",
    "                    pred_en = torch.argmax(dec_out_en, dim=-1)\n",
    "                    \n",
    "                    _, loss_en2fr_val, loss_fr2en_val, _ = loss_fn(\n",
    "                        sents_en[:, 1:], sents_fr[:, 1:],\n",
    "                        dec_out_en, dec_out_fr,\n",
    "                        ignore_index=1\n",
    "                    )\n",
    "                    _loss_en2fr_val.append(loss_en2fr_val.item())\n",
    "                    _loss_fr2en_val.append(loss_fr2en_val.item())\n",
    "                    \n",
    "                    for idx in range(batch_size):\n",
    "                        text_real_fr = tokenizer_fr.convert_tokens_to_string(sents_fr[idx, 1:-1].tolist())\n",
    "                        text_pred_fr = tokenizer_fr.convert_tokens_to_string(preds_fr[idx, 0:-1].tolist())\n",
    "                        _bleu_en2fr.append(sentence_bleu(text_real_fr, text_pred_fr))\n",
    "                        \n",
    "                        text_real_fr = tokenizer_fr.convert_tokens_to_string(sents_fr[idx, 1:-1].tolist())\n",
    "                        text_pred_fr = tokenizer_fr.convert_tokens_to_string(preds_fr[idx, 0:-1].tolist())\n",
    "                        _bleu_en2fr.append(sentence_bleu(text_real_fr, text_pred_fr))\n",
    "                        \n",
    "                    _em_en2fr.append(exact_match(preds_en[:, 0:-1], sents_fr[:, 1:-1]))\n",
    "                    _em_fr2en.append(exact_match(preds_en[:, 0:-1], sents_en[:, 1:-1]))\n",
    "\n",
    "                avg_em_en2fr = sum(_em_en2fr) / batch_size\n",
    "                avg_em_fr2en = sum(_em_fr2en) / batch_size\n",
    "                avg_bleu_en2fr = sum(_bleu_en2fr) / batch_size\n",
    "                avg_bleu_fr2en = sum(_bleu_fr2en) / batch_size\n",
    "                avg_loss_en2fr_val = sum(_val_loss_en2fr) / batch_size\n",
    "                avg_loss_fr2en_val = sum(_val_loss_fr2en) / batch_size\n",
    "                \n",
    "                bleu_metrics = {\"en-fr\": avg_bleu_en2fr, \"fr-en\": avg_bleu_fr2en}\n",
    "                write_to_tensorboard(\"BLEU\", bleu_metrics, training=False, step=batch_i, writer=writer)\n",
    "                \n",
    "                em_metrics = {\"en-fr\": avg_em_en2fr, \"fr-en\": avg_em_fr2en}\n",
    "                write_to_tensorboard(\"EM\", em_metrics, training=False, step=batch_i, writer=writer)\n",
    "                \n",
    "                loss_val_metrics = {\"en-fr\": avg_loss_en2fr_val, \"fr-en\": avg_loss_fr2en_val}\n",
    "                write_to_tensorboard(\"CCE\", loss_val_metrics, training=False, step=batch_i, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
