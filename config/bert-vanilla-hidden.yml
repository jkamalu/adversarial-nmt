batch_size: &batch_size 32
minlen: &minlen 2
maxlen: &maxlen 50
n_train: -1
n_valid: -1

regularization: &regularization
    type:
        - hidden
    n_affine: 0

d_model: &d_model 768

data_loader:
    batch_size: *batch_size
    shuffle: True
    num_workers: 4

log_frequency: 100
val_frequency: 1000
checkpoint_frequency: 50000
max_step_num: 100000

adam:
    lr: 0.00015

encoder: bert
encoder_kwargs:
    fairseq: # overwrite fairseq base architecture
        encoder_embed_dim: *d_model
    bert:
        

decoder: fairseq
decoder_kwargs:
    fairseq: # overwrite fairseq base architecture
        encoder_embed_dim: *d_model
        decoder_embed_dim: *d_model
        decoder_ffn_embed_dim: 2048
        max_target_positions: *maxlen
        share_decoder_input_output_embed: True
    gru:
        maxlen: *maxlen
        d_model: *d_model
        dropout: 0.1
        teacher_forcing_prob: 0.5
        attention_mech: bahdanau_attention
        
discriminator:
    << : *regularization
    maxlen: *maxlen
    d_model: *d_model
