small_transformer:
    num_layers: 6
    d_model: 512
    heads: 8
    d_ff: 2048
    copy_attn: False
    self_attn_type: scaled-dot
    dropout: 0.1
    attention_dropout: 0.1