batch_size: &batch_size 8
minlen: &minlen 2
maxlen: &maxlen 50
n_train: -1
n_valid: 100

# experiment variables
use_bert: True
regularization: &regularization
    type:
        - attention
    n_affine: 1

d_model: &d_model 768

data_loader:
    batch_size: *batch_size
    shuffle: True
    num_workers: 4

logging_frequency: 100
val_frequency: 1000
checkpoint_frequency: 50000
max_step_num: 300000

adam:
    lr: 0.001

vanilla_encoder:
    num_layers: 6
    d_model: *d_model
    heads: 8
    d_ff: 2048
    dropout: 0.1
    attention_dropout: 0.1
    max_relative_positions: 0

discriminator:
    << : *regularization
    maxlen: *maxlen
    d_model: *d_model

vanilla_decoder:
    num_layers: 6
    d_model: *d_model
    heads: 8
    d_ff: 2048
    copy_attn: False
    self_attn_type: scaled-dot
    dropout: 0.1
    attention_dropout: 0.1
    max_relative_positions: 0
    aan_useffn: False
