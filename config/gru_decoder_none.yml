batch_size: &batch_size 8
minlen: &minlen 2
maxlen: &maxlen 50
n_train: -1
n_valid: 100

# experiment variables
use_bert: True
regularization: &regularization
    type:
        -
    n_affine: 0

d_model: &d_model 768

data_loader:
    batch_size: *batch_size
    shuffle: True
    num_workers: 4

logging_frequency: 10
val_frequency: 1000
checkpoint_frequency: 50000
max_step_num: 300000

adam:
    lr: 0.001

# NOTE: changed the naming to just encoder and decoder
encoder:
    num_layers: 6
    d_model: *d_model
    heads: 8
    d_ff: 2048
    dropout: 0.1
    attention_dropout: 0.1
    max_relative_positions: 0

discriminator:
    << : *regularization
    maxlen: *maxlen
    d_model: *d_model

decoder:
    type: GRU
    maxlen: *maxlen
    d_model: *d_model
    dropout: 0.1
    teacher_forcing_prob: 0.5
    attention_mech: bahdanau_attention
