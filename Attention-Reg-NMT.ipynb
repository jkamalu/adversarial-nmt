{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a basic training loop - not using attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir is /juice/scr/scr110/scr/nlp/mtl_bert/unidirectional-NMT/BERT\n"
     ]
    }
   ],
   "source": [
    "''' Changing directories '''\n",
    "import os \n",
    "if 'BERT' not in os.getcwd():\n",
    "    os.chdir('BERT')\n",
    "print(\"Current working dir is {}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "import pyaml\n",
    "import onmt\n",
    "import torch\n",
    "from dataset import TextDataset, Collator\n",
    "from encoder import Encoder \n",
    "from decoder import Decoder\n",
    "from discriminator import Discriminator\n",
    "from lib.huggingface.transformers import RobertaTokenizer, CamembertTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.path.dirname(os.getcwd()), \"config\", \"config.yml\"), \"r\") as fd:\n",
    "    config = pyaml.yaml.load(fd, Loader=pyaml.yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_token_length = config[\"maxlen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_fr = CamembertTokenizer.from_pretrained('camembert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(maxlen=sentence_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "175 examples with length < 2 removed.\n",
      "10109 examples with length > 50 removed.\n",
      "0\n",
      "36 examples with length < 2 removed.\n",
      "2547 examples with length > 50 removed.\n"
     ]
    }
   ],
   "source": [
    "text_dataset_train = TextDataset(\"data/europarl-v7/\", tokenizer_en, tokenizer_fr, training=True)\n",
    "text_dataset_val = TextDataset(\"data/europarl-v7/\",  tokenizer_en, tokenizer_fr, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(text_dataset_train, **config[\"data_loader\"], collate_fn=collator)\n",
    "val_dataloader = DataLoader(text_dataset_val, **config[\"data_loader\"], collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the encoding and decoding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA!\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"Using CUDA!\")\n",
    "else:\n",
    "    print(\"Using CPU - Played yourself!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del encoder_en\n",
    "    del encoder_fr\n",
    "except:\n",
    "    pass \n",
    "encoder_en = Encoder(\"english\").to(device=device)\n",
    "encoder_fr = Encoder(\"french\").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same\n",
    "word_padding_idx_en = encoder_en._modules['model'].embeddings.padding_idx\n",
    "word_padding_idx_fr = encoder_fr._modules['model'].embeddings.padding_idx\n",
    "\n",
    "# en > fr\n",
    "word_vocab_size_en = encoder_en._modules['model'].embeddings.word_embeddings.num_embeddings\n",
    "word_vocab_size_fr = encoder_fr._modules['model'].embeddings.word_embeddings.num_embeddings\n",
    "\n",
    "# same\n",
    "word_vec_size_en = encoder_en._modules['model'].embeddings.word_embeddings.embedding_dim\n",
    "word_vec_size_fr = encoder_fr._modules['model'].embeddings.word_embeddings.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_en = onmt.modules.embeddings.Embeddings(\n",
    "    word_vec_size_en, \n",
    "    word_vocab_size_en, \n",
    "    word_padding_idx_en, \n",
    "    position_encoding=True\n",
    ").to(device=device)\n",
    "\n",
    "embeddings_fr = onmt.modules.embeddings.Embeddings(\n",
    "    word_vec_size_fr, \n",
    "    word_vocab_size_fr, \n",
    "    word_padding_idx_fr, \n",
    "    position_encoding=True\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_en = Decoder(**config[\"small_transformer\"], embeddings=embeddings_en).to(device=device)\n",
    "decoder_fr = Decoder(**config[\"small_transformer\"], embeddings=embeddings_fr).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection: standard_sentence_length**2 -> 1\n",
    "discriminator = Discriminator(sentence_token_length**2, 1).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict):\n",
    "    '''Standard machine translation cross entropy loss'''\n",
    "    ce_loss = torch.nn.CrossEntropyLoss(ignore_index = 1) #ignoring padding tokens\n",
    "    \n",
    "    predictions_fr = torch.argmax(french_predict, dim=2)\n",
    "    \n",
    "    loss_english_to_french = ce_loss(english_predict.transpose(1,2), english_gt)\n",
    "    loss_french_to_english = ce_loss(french_predict.transpose(1,2), french_gt)\n",
    "    return loss_english_to_french + loss_french_to_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_single_regularization(english_gt, french_gt, english_predict, french_predict,\n",
    "                                discriminator_gt, discriminator_predict):\n",
    "    '''Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs'''\n",
    "    ce_term = loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict)\n",
    "    \n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    regularizing_term = bce_loss(discriminator_predict, discriminator_gt)\n",
    "    \n",
    "    return ce_term + regularizing_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_multi_regularization(english_gt, french_gt, english_predict, french_predict,\n",
    "                                discriminator_gt_1, discriminator_predict_1,\n",
    "                                discriminator_gt_2, discriminator_predict_2,):\n",
    "    '''Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs'''\n",
    "    ce_term = loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict)\n",
    "    \n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    regularizing_term_1 = bce_loss(discriminator_predict_1, discriminator_gt_1)\n",
    "    regularizing_term_2 = bce_loss(discriminator_predict_2, discriminator_gt_2)\n",
    "    \n",
    "    return ce_term + regularizing_term_1 + regularizing_term_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and defining evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(prediction, gt):\n",
    "    '''Evaluate ground percent exact match '''\n",
    "    mask = gt != 1\n",
    "    return torch.sum((prediction == gt) * mask).item()/torch.sum(mask).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining optimizer and hooks if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(regularize=\"hidden_state\"):\n",
    "    params = list(encoder_en.parameters()) + list(encoder_fr.parameters()) +\\\n",
    "         list(decoder_fr.parameters()) + list(decoder_en.parameters())\n",
    "\n",
    "    if (regularize == \"hidden_state\" or regularize ==\"attention\"):\n",
    "        params += list(discriminator.parameters())\n",
    "\n",
    "    return torch.optim.Adam(params)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(regularize=\"attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hook import Hook\n",
    "from utils import extract_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hooks_english = [Hook(layer[1]) for layer in list(encoder_en.named_modules())]\n",
    "_hooks_french = [Hook(layer[1]) for layer in list(encoder_fr.named_modules())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining primary training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data_iter, val_data_iter, regularize=\"hidden_state\"): \n",
    "    ''' \n",
    "    Train the encoding and decoding models. User needs to pass in a valid iterator over the data,\n",
    "    and also specify a type of adversarial regularization. regularize = [\"hidden_state\", \"attention\"]\n",
    "    '''\n",
    "    \n",
    "    for batch_num, batch in enumerate(train_data_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Reading in input and moving to device\n",
    "\n",
    "        english_batch, french_batch = batch \n",
    "        (english_sentences, english_sentences_no_eos, english_sentences_lengths) = english_batch\n",
    "        (french_sentences, french_sentences_no_eos, french_sentences_lengths) = french_batch\n",
    "\n",
    "        english_sentences = english_sentences.to(device=device)\n",
    "        english_sentences_no_eos = english_sentences_no_eos.to(device=device)\n",
    "        english_sentences_lengths = english_sentences_lengths.to(device=device)\n",
    "        french_sentences = french_sentences.to(device=device)\n",
    "        french_sentences_no_eos = french_sentences_no_eos.to(device=device)\n",
    "        french_sentences_lengths = french_sentences_lengths.to(device=device)\n",
    "\n",
    "        # Encoding - Decoding for English -> French\n",
    "        encoder_outputs_en = encoder_en(english_sentences)\n",
    "        decoder_fr.init_state(english_sentences.unsqueeze(2).transpose(0,1), None, None)\n",
    "        dec_outs_fr, _ = decoder_fr(french_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_en[0].transpose(0,1), memory_lengths=english_sentences_lengths)\n",
    "\n",
    "        # Encoding - Decoding for French -> English\n",
    "        encoder_outputs_fr = encoder_fr(french_sentences)\n",
    "        decoder_en.init_state(french_sentences.unsqueeze(2).transpose(0,1), None, None) \n",
    "        dec_outs_en, _ = decoder_en(english_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_fr[0].transpose(0,1), memory_lengths=french_sentences_lengths)\n",
    "\n",
    "        if (regularize == \"attention\"):\n",
    "            # extracting the attention scores from the datasets; using 7th attention head\n",
    "            # as suggested by Clark et al, 2019 \n",
    "            english_attention = extract_attention_scores(_hooks_english)[6] \n",
    "            french_attention = extract_attention_scores(_hooks_french)[6]\n",
    "            batch_size = english_attention.shape[0]\n",
    "            english_attention_reshaped = english_attention.view(batch_size, -1)\n",
    "            french_attention_reshaped = french_attention.view(batch_size, -1)\n",
    "            \n",
    "            discriminator_outputs_en = attn_discriminator(english_attention_reshaped)\n",
    "            discriminator_outputs_fr = attn_discriminator(french_attention_reshaped)\n",
    "            discriminator_outputs_cat = torch.cat((discriminator_outputs_en, discriminator_outputs_fr))\n",
    "            discriminator_labels = torch.tensor([1.0]*batch_size + [0.0]*batch_size)\n",
    "            discriminator_labels = discriminator_labels.unsqueeze(1).to(device=device)\n",
    "\n",
    "            \n",
    "            loss = loss_fn_single_regularization(english_sentences[:, 1:],\n",
    "                                               french_sentences[:, 1:],\n",
    "                                               dec_outs_en,\n",
    "                                               dec_outs_fr,\n",
    "                                               discriminator_labels,\n",
    "                                               discriminator_outputs_cat,\n",
    "                                              )\n",
    "            \n",
    "        elif (regularize == \"hidden_state\"):\n",
    "            # using the pooled outputs of the encoders for regularizing \n",
    "            discriminator_outputs_en = discriminator(encoder_outputs_en[1])\n",
    "            discriminator_outputs_fr = discriminator(encoder_outputs_fr[1])\n",
    "            discriminator_outputs_cat = torch.cat((discriminator_outputs_en, discriminator_outputs_fr))\n",
    "            discriminator_labels = torch.tensor([1.0]*discriminator_outputs_en.shape[0] + [0.0]*discriminator_outputs_fr.shape[0])\n",
    "            discriminator_labels = discriminator_labels.unsqueeze(1).to(device=device)\n",
    "\n",
    "            loss = loss_fn_single_regularization(english_sentences[:, 1:],\n",
    "                                               french_sentences[:, 1:],\n",
    "                                               dec_outs_en,\n",
    "                                               dec_outs_fr,\n",
    "                                               discriminator_labels,\n",
    "                                               discriminator_outputs_cat,\n",
    "                                              )\n",
    "        else:\n",
    "            loss = loss_fn_no_regularization(english_sentences[:, 1:],\n",
    "                                           french_sentences[:, 1:],\n",
    "                                           dec_outs_en,\n",
    "                                           dec_outs_fr,\n",
    "                                          )\n",
    "        # must be put here to avoid name claim by val loop\n",
    "        print(\"Batch num {}: Loss {}\".format(batch_num, loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Running validation script  \n",
    "        if (batch_num > 0 and batch_num % 100 == 0):\n",
    "            with torch.no_grad():\n",
    "                _blue_scores = []\n",
    "                _exact_matches = []\n",
    "                for batch_num, batch in enumerate(val_data_iter):\n",
    "                    \n",
    "                    if (batch_num == 20):\n",
    "                        break\n",
    "                    \n",
    "                    # Reading in input and moving to device\n",
    "                    english_batch, french_batch = batch \n",
    "                    (english_sentences, english_sentences_no_eos, english_sentences_lengths) = english_batch\n",
    "                    (french_sentences, french_sentences_no_eos, french_sentences_lengths) = french_batch\n",
    "\n",
    "                    english_sentences = english_sentences.to(device=device)\n",
    "                    english_sentences_no_eos = english_sentences_no_eos.to(device=device)\n",
    "                    english_sentences_lengths = english_sentences_lengths.to(device=device)\n",
    "                    french_sentences = french_sentences.to(device=device)\n",
    "                    french_sentences_no_eos = french_sentences_no_eos.to(device=device)\n",
    "                    french_sentences_lengths = french_sentences_lengths.to(device=device)\n",
    "\n",
    "                    # Encoding - Decoding for English -> French\n",
    "                    encoder_outputs_en = encoder_en(english_sentences)\n",
    "                    decoder_fr.init_state(english_sentences.unsqueeze(2).transpose(0,1), None, None)\n",
    "                    dec_outs_fr, _ = decoder_fr(french_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_en[0].transpose(0,1), memory_lengths=english_sentences_lengths)\n",
    "                    \n",
    "                    # Calculate BLUE Scores, EM and Perplexity\n",
    "                    predictions_fr = torch.argmax(dec_outs_fr, dim=2)\n",
    "                    \n",
    "                    for idx in range(french_sentences.shape[0]):\n",
    "                        detokenized_french_gt = tokenizer_fr.convert_tokens_to_string(french_sentences[idx,1:].tolist())\n",
    "                        detokenized_french_pred = tokenizer_fr.convert_tokens_to_string(predictions_fr[idx].tolist())\n",
    "                            \n",
    "                        _blue_score = sentence_bleu(detokenized_french_gt, detokenized_french_pred)\n",
    "                        _blue_scores.append(_blue_score)\n",
    "\n",
    "                    _exact_match = exact_match(predictions_fr, french_sentences[:,1:])\n",
    "                    _exact_matches.append(_exact_match)\n",
    "                    \n",
    "                    \n",
    "                print(\"BLUE {}, EM: {}\".format(sum(_blue_scores)/len(_blue_scores),\n",
    "                                               sum(_exact_matches)/len(_exact_matches)))\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50])\n",
      "torch.Size([2, 49])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-bce9b6d53486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"attention\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-c2289dfc31d0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data_iter, val_data_iter, regularize)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_sentences_no_eos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0menglish_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menglish_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0menglish_sentences_no_eos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menglish_sentences_no_eos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0menglish_sentences_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menglish_sentences_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, val_dataloader, regularize=\"attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: \n",
    "* Extract the attention\n",
    "* Create attention loss \n",
    "* Create combination loss of attention and hidden \n",
    "* Print out accuracy for the encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_num, batch in enumerate(train_dataloader):\n",
    "        english_batch, french_batch = batch \n",
    "        (english_sentences, english_sentences_no_eos, english_sentences_lengths) = english_batch\n",
    "        (french_sentences, french_sentences_no_eos, french_sentences_lengths) = french_batch\n",
    "        print(french_sentences)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fr.encode(\"bonjour <pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
