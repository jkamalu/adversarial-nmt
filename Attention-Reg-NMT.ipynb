{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a basic training loop - not using attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir is /juice/scr/scr110/scr/nlp/mtl_bert/unidirectional-NMT/BERT\n"
     ]
    }
   ],
   "source": [
    "''' Changing directories '''\n",
    "import os \n",
    "if 'BERT' not in os.getcwd():\n",
    "    os.chdir('BERT')\n",
    "print(\"Current working dir is {}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "import pyaml\n",
    "import onmt\n",
    "import math\n",
    "import torch\n",
    "from dataset import TextDataset, Collator\n",
    "from encoder import Encoder \n",
    "from decoder import Decoder\n",
    "from discriminator import Discriminator\n",
    "from lib.huggingface.transformers import RobertaTokenizer, CamembertTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.path.dirname(os.getcwd()), \"config\", \"config.yml\"), \"r\") as fd:\n",
    "    config = pyaml.yaml.load(fd, Loader=pyaml.yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_token_length = config[\"maxlen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_fr = CamembertTokenizer.from_pretrained('camembert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(maxlen=sentence_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "175 examples with length < 2 removed.\n",
      "4191 examples with length > 50 removed.\n",
      "0\n",
      "36 examples with length < 2 removed.\n",
      "1037 examples with length > 50 removed.\n"
     ]
    }
   ],
   "source": [
    "text_dataset_train = TextDataset(\"data/europarl-v7/\", tokenizer_en, tokenizer_fr, training=True, maxlen=sentence_token_length)\n",
    "text_dataset_val = TextDataset(\"data/europarl-v7/\",  tokenizer_en, tokenizer_fr, training=False, maxlen=sentence_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(text_dataset_train, **config[\"data_loader\"], collate_fn=collator)\n",
    "val_dataloader = DataLoader(text_dataset_val, **config[\"data_loader\"], collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the encoding and decoding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA!\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"Using CUDA!\")\n",
    "else:\n",
    "    print(\"Using CPU - Played yourself!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del encoder_en\n",
    "    del encoder_fr\n",
    "except:\n",
    "    pass \n",
    "encoder_en = Encoder(\"english\").to(device=device)\n",
    "encoder_fr = Encoder(\"french\").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same\n",
    "word_padding_idx_en = encoder_en._modules['model'].embeddings.padding_idx\n",
    "word_padding_idx_fr = encoder_fr._modules['model'].embeddings.padding_idx\n",
    "\n",
    "# en > fr\n",
    "word_vocab_size_en = encoder_en._modules['model'].embeddings.word_embeddings.num_embeddings\n",
    "word_vocab_size_fr = encoder_fr._modules['model'].embeddings.word_embeddings.num_embeddings\n",
    "\n",
    "# same\n",
    "word_vec_size_en = encoder_en._modules['model'].embeddings.word_embeddings.embedding_dim\n",
    "word_vec_size_fr = encoder_fr._modules['model'].embeddings.word_embeddings.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_en = onmt.modules.embeddings.Embeddings(\n",
    "    word_vec_size_en, \n",
    "    word_vocab_size_en, \n",
    "    word_padding_idx_en, \n",
    "    position_encoding=True\n",
    ").to(device=device)\n",
    "\n",
    "embeddings_fr = onmt.modules.embeddings.Embeddings(\n",
    "    word_vec_size_fr, \n",
    "    word_vocab_size_fr, \n",
    "    word_padding_idx_fr, \n",
    "    position_encoding=True\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_en = Decoder(**config[\"small_transformer\"], embeddings=embeddings_en).to(device=device)\n",
    "decoder_fr = Decoder(**config[\"small_transformer\"], embeddings=embeddings_fr).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection: standard_sentence_length**2 -> 1\n",
    "attn_discriminator = Discriminator(int(sentence_token_length**2), 1).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict):\n",
    "    '''Standard machine translation cross entropy loss'''\n",
    "    ce_loss = torch.nn.CrossEntropyLoss(ignore_index = 1) #ignoring padding tokens\n",
    "    \n",
    "    predictions_fr = torch.argmax(french_predict, dim=2)\n",
    "    \n",
    "    loss_english_to_french = ce_loss(english_predict.transpose(1,2), english_gt)\n",
    "    loss_french_to_english = ce_loss(french_predict.transpose(1,2), french_gt)\n",
    "    return (loss_english_to_french + loss_french_to_english, loss_english_to_french, loss_french_to_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_single_regularization(english_gt, french_gt, english_predict, french_predict,\n",
    "                                discriminator_gt, discriminator_predict):\n",
    "    '''Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs'''\n",
    "    ce_term, loss_english_to_french, loss_french_to_english = loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict)\n",
    "    \n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    regularizing_term = bce_loss(discriminator_predict, discriminator_gt)\n",
    "    \n",
    "    return (ce_term + regularizing_term, loss_english_to_french, loss_french_to_english, regularizing_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_multi_regularization(english_gt, french_gt, english_predict, french_predict,\n",
    "                                discriminator_gt_1, discriminator_predict_1,\n",
    "                                discriminator_gt_2, discriminator_predict_2,):\n",
    "    '''Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs'''\n",
    "    ce_term = loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict)\n",
    "    \n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    regularizing_term_1 = bce_loss(discriminator_predict_1, discriminator_gt_1)\n",
    "    regularizing_term_2 = bce_loss(discriminator_predict_2, discriminator_gt_2)\n",
    "    \n",
    "    return ce_term + regularizing_term_1 + regularizing_term_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and defining evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(prediction, gt):\n",
    "    '''Evaluate ground percent exact match '''\n",
    "    mask = gt != 1\n",
    "    return torch.sum((prediction == gt) * mask).item()/torch.sum(mask).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining optimizer and hooks if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(regularize=\"None\"):\n",
    "    params = list(encoder_en.parameters()) + list(encoder_fr.parameters()) +\\\n",
    "         list(decoder_fr.parameters()) + list(decoder_en.parameters())\n",
    "\n",
    "    if (regularize == \"hidden_state\"):\n",
    "        params += list(discriminator.parameters())\n",
    "    \n",
    "    if (regularize == \"attention\"):\n",
    "        params += list(attn_discriminator.parameters())\n",
    "\n",
    "    return torch.optim.Adam(params)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(regularize=\"attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining primary training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import write_to_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data_iter, val_data_iter, regularize=\"None\"): \n",
    "    ''' \n",
    "    Train the encoding and decoding models. User needs to pass in a valid iterator over the data,\n",
    "    and also specify a type of adversarial regularization. regularize = [\"hidden_state\", \"attention\"]\n",
    "    '''\n",
    "    writer = SummaryWriter(\"runs/regularize_attention\")\n",
    "                                                   \n",
    "    for batch_num, batch in enumerate(train_data_iter):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # TODO: generalize this to multiple discriminators\n",
    "        if (regularize != \"None\"):\n",
    "            if (batch_num % 2 == 0):\n",
    "                for param in encoder_en.parameters():\n",
    "                    param.requires_grad = False\n",
    "                for param in encoder_fr.parameters():\n",
    "                    param.requires_grad = False    \n",
    "                for param in attn_discriminator.parameters():\n",
    "                    param.requires_grad = True \n",
    "            else:\n",
    "                for param in encoder_en.parameters():\n",
    "                    param.requires_grad = True\n",
    "                for param in encoder_fr.parameters():\n",
    "                    param.requires_grad = True    \n",
    "                for param in attn_discriminator.parameters():\n",
    "                    param.requires_grad = False \n",
    "\n",
    "        # Reading in input and moving to device\n",
    "\n",
    "        english_batch, french_batch = batch \n",
    "        (english_sentences, english_sentences_no_eos, english_sentences_lengths) = english_batch\n",
    "        (french_sentences, french_sentences_no_eos, french_sentences_lengths) = french_batch\n",
    "\n",
    "        english_sentences = english_sentences.to(device=device)\n",
    "        english_sentences_no_eos = english_sentences_no_eos.to(device=device)\n",
    "        english_sentences_lengths = english_sentences_lengths.to(device=device)\n",
    "        french_sentences = french_sentences.to(device=device)\n",
    "        french_sentences_no_eos = french_sentences_no_eos.to(device=device)\n",
    "        french_sentences_lengths = french_sentences_lengths.to(device=device)\n",
    "\n",
    "        # Encoding - Decoding for English -> French\n",
    "        encoder_outputs_en = encoder_en(english_sentences)\n",
    "        decoder_fr.init_state(english_sentences.unsqueeze(2).transpose(0,1), None, None)\n",
    "        dec_outs_fr, _ = decoder_fr(french_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_en[0].transpose(0,1), memory_lengths=english_sentences_lengths)\n",
    "\n",
    "        # Encoding - Decoding for French -> English\n",
    "        encoder_outputs_fr = encoder_fr(french_sentences)\n",
    "        decoder_en.init_state(french_sentences.unsqueeze(2).transpose(0,1), None, None) \n",
    "        dec_outs_en, _ = decoder_en(english_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_fr[0].transpose(0,1), memory_lengths=french_sentences_lengths)\n",
    "\n",
    "        if (regularize == \"attention\"):\n",
    "            # extracting the attention scores from the datasets; using 7th attention head\n",
    "            # as suggested by Clark et al, 2019 \n",
    "            english_attention = extract_attention_scores(_hooks_english)[6] \n",
    "            french_attention = extract_attention_scores(_hooks_french)[6]\n",
    "            batch_size = english_attention.shape[0]\n",
    "            english_attention_reshaped = english_attention.view(batch_size, -1)\n",
    "            french_attention_reshaped = french_attention.view(batch_size, -1)\n",
    "            \n",
    "            discriminator_outputs_en = attn_discriminator(english_attention_reshaped)\n",
    "            discriminator_outputs_fr = attn_discriminator(french_attention_reshaped)\n",
    "            discriminator_outputs_cat = torch.cat((discriminator_outputs_en, discriminator_outputs_fr))\n",
    "            if (batch_num % 2 == 0):\n",
    "                discriminator_labels = torch.tensor([1.0]*batch_size + [0.0]*batch_size)\n",
    "            else:\n",
    "                discriminator_labels = torch.tensor([0.0]*batch_size + [1.0]*batch_size)\n",
    "            discriminator_labels = discriminator_labels.unsqueeze(1).to(device=device)\n",
    "\n",
    "            \n",
    "            all_losses = loss_fn_single_regularization(english_sentences[:, 1:],\n",
    "                                               french_sentences[:, 1:],\n",
    "                                               dec_outs_en,\n",
    "                                               dec_outs_fr,\n",
    "                                               discriminator_labels,\n",
    "                                               discriminator_outputs_cat,\n",
    "                                              )\n",
    "            \n",
    "            (loss, loss_english_to_french, loss_french_to_english, regularizing_term) = all_losses\n",
    "            \n",
    "        elif (regularize == \"hidden_state\"):\n",
    "            # using the pooled outputs of the encoders for regularizing \n",
    "            discriminator_outputs_en = discriminator(encoder_outputs_en[1])\n",
    "            discriminator_outputs_fr = discriminator(encoder_outputs_fr[1])\n",
    "            discriminator_outputs_cat = torch.cat((discriminator_outputs_en, discriminator_outputs_fr))\n",
    "            \n",
    "            if (batch_num % 2 == 0):\n",
    "                discriminator_labels = torch.tensor([1.0]*batch_size + [0.0]*batch_size)\n",
    "            else:\n",
    "                discriminator_labels = torch.tensor([0.0]*batch_size + [1.0]*batch_size)\n",
    "            discriminator_labels = discriminator_labels.unsqueeze(1).to(device=device)\n",
    "\n",
    "            all_losses = loss_fn_single_regularization(english_sentences[:, 1:],\n",
    "                                               french_sentences[:, 1:],\n",
    "                                               dec_outs_en,\n",
    "                                               dec_outs_fr,\n",
    "                                               discriminator_labels,\n",
    "                                               discriminator_outputs_cat,\n",
    "                                              )\n",
    "            (loss, loss_english_to_french, loss_french_to_english, regularizing_term) = all_losses\n",
    "        else:\n",
    "            loss = loss_fn_no_regularization(english_sentences[:, 1:],\n",
    "                                           french_sentences[:, 1:],\n",
    "                                           dec_outs_en,\n",
    "                                           dec_outs_fr,\n",
    "                                          )\n",
    "            \n",
    "        # must be put here to avoid name claim by val loop\n",
    "        if (batch_num % 10 == 0):\n",
    "            print(\"Batch num {}: Loss {}\".format(batch_num, loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                                                   \n",
    "        write_to_tensorboard(\"CCE\", {\"en-fr\": loss_english_to_french.item(), \"fr-en\":loss_french_to_english.item()}, training=True, step=batch_num, writer=writer)\n",
    "        write_to_tensorboard(\"BCE\", {\"attention-regularizing\": regularizing_term.item()}, training=True, step=batch_num, writer=writer)\n",
    "\n",
    "    \n",
    "        # Running validation script  \n",
    "        if (batch_num > 0 and batch_num % 10 == 0):\n",
    "            with torch.no_grad():\n",
    "                _blue_scores_en_fr = []\n",
    "                _exact_matches_en_fr = []\n",
    "                _blue_scores_fr_en = []\n",
    "                _exact_matches_fr_en = []\n",
    "                for batch_num, batch in enumerate(val_data_iter):\n",
    "                    \n",
    "                    if (batch_num == 50):\n",
    "                        break\n",
    "                    \n",
    "                    # Reading in input and moving to device\n",
    "                    english_batch, french_batch = batch \n",
    "                    (english_sentences, english_sentences_no_eos, english_sentences_lengths) = english_batch\n",
    "                    (french_sentences, french_sentences_no_eos, french_sentences_lengths) = french_batch\n",
    "\n",
    "                    english_sentences = english_sentences.to(device=device)\n",
    "                    english_sentences_no_eos = english_sentences_no_eos.to(device=device)\n",
    "                    english_sentences_lengths = english_sentences_lengths.to(device=device)\n",
    "                    french_sentences = french_sentences.to(device=device)\n",
    "                    french_sentences_no_eos = french_sentences_no_eos.to(device=device)\n",
    "                    french_sentences_lengths = french_sentences_lengths.to(device=device)\n",
    "\n",
    "                    # Encoding - Decoding for English -> French\n",
    "                    encoder_outputs_en = encoder_en(english_sentences)\n",
    "                    decoder_fr.init_state(english_sentences.unsqueeze(2).transpose(0,1), None, None)\n",
    "                    dec_outs_fr, _ = decoder_fr(french_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_en[0].transpose(0,1), memory_lengths=english_sentences_lengths)\n",
    "                    \n",
    "                    # Encoding - Decoding for French -> English\n",
    "                    encoder_outputs_fr = encoder_fr(french_sentences)\n",
    "                    decoder_en.init_state(french_sentences.unsqueeze(2).transpose(0,1), None, None) \n",
    "                    dec_outs_en, _ = decoder_en(english_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_fr[0].transpose(0,1), memory_lengths=french_sentences_lengths)\n",
    "\n",
    "                    \n",
    "                    # Calculate BLUE Scores, EM and Perplexity\n",
    "                    predictions_fr = torch.argmax(dec_outs_fr, dim=2)\n",
    "                    predictions_en = torch.argmax(dec_outs_en, dim=2)\n",
    "                    \n",
    "                    for idx in range(french_sentences.shape[0]):\n",
    "                        detokenized_french_gt = tokenizer_fr.convert_tokens_to_string(french_sentences[idx,1:].tolist())\n",
    "                        detokenized_french_pred = tokenizer_fr.convert_tokens_to_string(predictions_fr[idx].tolist())\n",
    "                        \n",
    "                        _blue_score_en_fr = sentence_bleu(detokenized_french_gt, detokenized_french_pred)\n",
    "                        _blue_scores_en_fr.append(_blue_score_en_fr)\n",
    "                        \n",
    "                    for idx in range(english_sentences.shape[0]):\n",
    "           \n",
    "                        detokenized_english_gt = tokenizer_en.decode(english_sentences[idx,1:].tolist())\n",
    "                        detokenized_english_pred = tokenizer_en.decode(predictions_en[idx].tolist())\n",
    "                            \n",
    "                        _blue_score_fr_en = sentence_bleu(detokenized_english_gt, detokenized_english_pred)\n",
    "                        _blue_scores_fr_en.append(_blue_score_fr_en)\n",
    "                        \n",
    "                    _exact_match_en_fr = exact_match(predictions_fr, french_sentences[:,1:])\n",
    "                    _exact_matches_en_fr.append(_exact_match_en_fr)\n",
    "                    \n",
    "                    _exact_match_fr_en = exact_match(predictions_en, english_sentences[:,1:])\n",
    "                    _exact_matches_fr_en.append(_exact_match_fr_en)\n",
    "                    \n",
    "                                                   \n",
    "                avg_bleu_en_fr = sum(_blue_scores_en_fr)/len(_blue_scores_en_fr)\n",
    "                avg_bleu_fr_en = sum(_blue_scores_fr_en)/len(_blue_scores_fr_en)\n",
    "                avg_em_en_fr = sum(_exact_matches_en_fr)/len(_exact_matches_en_fr)\n",
    "                avg_em_fr_en = sum(_exact_matches_fr_en)/len(_exact_matches_fr_en)\n",
    "                                                   \n",
    "                write_to_tensorboard(\"BLEU\", {\"en-fr\": avg_bleu_en_fr, \"fr-en\":avg_bleu_fr_en}, training=False, step=batch_num, writer=writer)\n",
    "                write_to_tensorboard(\"EM\", {\"en-fr\": avg_em_en_fr, \"fr-en\":avg_em_fr_en}, training=False, step=batch_num, writer=writer)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 0: Loss 26.897085189819336\n",
      "Batch num 10: Loss 25.889673233032227\n",
      "Batch num 20: Loss 22.817777633666992\n",
      "Batch num 30: Loss 32.06048583984375\n",
      "Batch num 40: Loss 23.33237648010254\n",
      "Batch num 50: Loss 22.062509536743164\n",
      "Batch num 60: Loss 27.385663986206055\n",
      "Batch num 70: Loss 30.629655838012695\n",
      "Batch num 80: Loss 26.126462936401367\n",
      "Batch num 90: Loss 20.1998233795166\n",
      "Batch num 100: Loss 26.90410041809082\n",
      "Batch num 110: Loss 28.324705123901367\n",
      "Batch num 120: Loss 22.771947860717773\n",
      "Batch num 130: Loss 21.53239631652832\n",
      "Batch num 140: Loss 24.382516860961914\n",
      "Batch num 150: Loss 25.296218872070312\n",
      "Batch num 160: Loss 29.16266632080078\n",
      "Batch num 170: Loss 20.89948844909668\n",
      "Batch num 180: Loss 22.258615493774414\n",
      "Batch num 190: Loss 25.992115020751953\n",
      "Batch num 200: Loss 20.665569305419922\n",
      "Batch num 210: Loss 20.555999755859375\n",
      "Batch num 220: Loss 25.68578338623047\n",
      "Batch num 230: Loss 26.146568298339844\n",
      "Batch num 240: Loss 24.552053451538086\n",
      "Batch num 250: Loss 25.202714920043945\n",
      "Batch num 260: Loss 20.115161895751953\n",
      "Batch num 270: Loss 20.695232391357422\n",
      "Batch num 280: Loss 18.675371170043945\n",
      "Batch num 290: Loss 24.50979232788086\n",
      "Batch num 300: Loss 22.27803611755371\n",
      "Batch num 310: Loss 20.008264541625977\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, val_dataloader, regularize=\"attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do for final paper: \n",
    "* Add Tensorboard stuff \n",
    "* Print out accuracy for the encoder\n",
    "* Create Perplexity evaluation metric\n",
    "* Run example with discriminator over both the attention and hidden\n",
    "* Factorize Code into util and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
