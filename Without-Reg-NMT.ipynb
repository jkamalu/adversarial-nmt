{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a basic training loop - not using attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir is /juice/scr/scr110/scr/nlp/mtl_bert/unidirectional-NMT/BERT\n"
     ]
    }
   ],
   "source": [
    "''' Changing directories '''\n",
    "import os \n",
    "if 'BERT' not in os.getcwd():\n",
    "    os.chdir('BERT')\n",
    "print(\"Current working dir is {}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "import pyaml\n",
    "import onmt\n",
    "import math\n",
    "import torch\n",
    "from dataset import TextDataset, Collator\n",
    "from encoder import Encoder \n",
    "from decoder import Decoder\n",
    "from discriminator import Discriminator\n",
    "from lib.huggingface.transformers import RobertaTokenizer, CamembertTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.path.dirname(os.getcwd()), \"config\", \"config.yml\"), \"r\") as fd:\n",
    "    config = pyaml.yaml.load(fd, Loader=pyaml.yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_token_length = config[\"maxlen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_fr = CamembertTokenizer.from_pretrained('camembert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(maxlen=sentence_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "1704 examples with length < 2 removed.\n",
      "42487 examples with length > 50 removed.\n",
      "0\n",
      "36 examples with length < 2 removed.\n",
      "1037 examples with length > 50 removed.\n"
     ]
    }
   ],
   "source": [
    "text_dataset_train = TextDataset(\"data/europarl-v7/\", tokenizer_en, tokenizer_fr, training=True, maxlen=sentence_token_length)\n",
    "text_dataset_val = TextDataset(\"data/europarl-v7/\",  tokenizer_en, tokenizer_fr, training=False, maxlen=sentence_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(text_dataset_train, **config[\"data_loader\"], collate_fn=collator)\n",
    "val_dataloader = DataLoader(text_dataset_val, **config[\"data_loader\"], collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the encoding and decoding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA!\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"Using CUDA!\")\n",
    "else:\n",
    "    print(\"Using CPU - Played yourself!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del encoder_en\n",
    "    del encoder_fr\n",
    "except:\n",
    "    pass \n",
    "encoder_en = Encoder(\"english\").to(device=device)\n",
    "encoder_fr = Encoder(\"french\").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same\n",
    "word_padding_idx_en = encoder_en._modules['model'].embeddings.padding_idx\n",
    "word_padding_idx_fr = encoder_fr._modules['model'].embeddings.padding_idx\n",
    "\n",
    "# en > fr\n",
    "word_vocab_size_en = encoder_en._modules['model'].embeddings.word_embeddings.num_embeddings\n",
    "word_vocab_size_fr = encoder_fr._modules['model'].embeddings.word_embeddings.num_embeddings\n",
    "\n",
    "# same\n",
    "word_vec_size_en = encoder_en._modules['model'].embeddings.word_embeddings.embedding_dim\n",
    "word_vec_size_fr = encoder_fr._modules['model'].embeddings.word_embeddings.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_en = onmt.modules.embeddings.Embeddings(\n",
    "    word_vec_size_en, \n",
    "    word_vocab_size_en, \n",
    "    word_padding_idx_en, \n",
    "    position_encoding=True\n",
    ").to(device=device)\n",
    "\n",
    "embeddings_fr = onmt.modules.embeddings.Embeddings(\n",
    "    word_vec_size_fr, \n",
    "    word_vocab_size_fr, \n",
    "    word_padding_idx_fr, \n",
    "    position_encoding=True\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_en = Decoder(**config[\"small_transformer\"], embeddings=embeddings_en).to(device=device)\n",
    "decoder_fr = Decoder(**config[\"small_transformer\"], embeddings=embeddings_fr).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict):\n",
    "    '''Standard machine translation cross entropy loss'''\n",
    "    ce_loss = torch.nn.CrossEntropyLoss(ignore_index = 1) #ignoring padding tokens\n",
    "    \n",
    "    predictions_fr = torch.argmax(french_predict, dim=2)\n",
    "    \n",
    "    loss_english_to_french = ce_loss(english_predict.transpose(1,2), english_gt)\n",
    "    loss_french_to_english = ce_loss(french_predict.transpose(1,2), french_gt)\n",
    "    return (loss_english_to_french + loss_french_to_english, loss_english_to_french, loss_french_to_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_single_regularization(english_gt, french_gt, english_predict, french_predict,\n",
    "                                discriminator_gt, discriminator_predict):\n",
    "    '''Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs'''\n",
    "    ce_term, loss_english_to_french, loss_french_to_english = loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict)\n",
    "    \n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    regularizing_term = bce_loss(discriminator_predict, discriminator_gt)\n",
    "    \n",
    "    return (ce_term + regularizing_term, loss_english_to_french, loss_french_to_english, regularizing_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_multi_regularization(english_gt, french_gt, english_predict, french_predict,\n",
    "                                discriminator_gt_1, discriminator_predict_1,\n",
    "                                discriminator_gt_2, discriminator_predict_2,):\n",
    "    '''Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs'''\n",
    "    ce_term = loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict)\n",
    "    \n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    regularizing_term_1 = bce_loss(discriminator_predict_1, discriminator_gt_1)\n",
    "    regularizing_term_2 = bce_loss(discriminator_predict_2, discriminator_gt_2)\n",
    "    \n",
    "    return ce_term + regularizing_term_1 + regularizing_term_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and defining evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(prediction, gt):\n",
    "    '''Evaluate ground percent exact match '''\n",
    "    mask = gt != 1\n",
    "    return torch.sum((prediction == gt) * mask).item()/torch.sum(mask).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining optimizer and hooks if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(regularize=\"None\"):\n",
    "    params = list(encoder_en.parameters()) + list(encoder_fr.parameters()) +\\\n",
    "         list(decoder_fr.parameters()) + list(decoder_en.parameters())\n",
    "\n",
    "    if (regularize == \"hidden_state\"):\n",
    "        params += list(discriminator.parameters())\n",
    "    \n",
    "    if (regularize == \"attention\"):\n",
    "        params += list(attn_discriminator.parameters())\n",
    "\n",
    "    return torch.optim.Adam(params)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(regularize=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining primary training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import write_to_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data_iter, val_data_iter, regularize=\"None\"): \n",
    "    ''' \n",
    "    Train the encoding and decoding models. User needs to pass in a valid iterator over the data,\n",
    "    and also specify a type of adversarial regularization. regularize = [\"hidden_state\", \"attention\"]\n",
    "    '''\n",
    "    writer = SummaryWriter(\"runs/without_regularization\")\n",
    "                                                   \n",
    "    for batch_num, batch in enumerate(train_data_iter):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # TODO: generalize this to multiple discriminators\n",
    "        if (regularize != \"None\"):\n",
    "            if (batch_num % 2 == 0):\n",
    "                for param in encoder_en.parameters():\n",
    "                    param.requires_grad = False\n",
    "                for param in encoder_fr.parameters():\n",
    "                    param.requires_grad = False    \n",
    "                for param in discriminator.parameters():\n",
    "                    param.requires_grad = True \n",
    "            else:\n",
    "                for param in encoder_en.parameters():\n",
    "                    param.requires_grad = True\n",
    "                for param in encoder_fr.parameters():\n",
    "                    param.requires_grad = True    \n",
    "                for param in discriminator.parameters():\n",
    "                    param.requires_grad = False \n",
    "\n",
    "        # Reading in input and moving to device\n",
    "\n",
    "        english_batch, french_batch = batch \n",
    "        (english_sentences, english_sentences_no_eos, english_sentences_lengths) = english_batch\n",
    "        (french_sentences, french_sentences_no_eos, french_sentences_lengths) = french_batch\n",
    "\n",
    "        english_sentences = english_sentences.to(device=device)\n",
    "        english_sentences_no_eos = english_sentences_no_eos.to(device=device)\n",
    "        english_sentences_lengths = english_sentences_lengths.to(device=device)\n",
    "        french_sentences = french_sentences.to(device=device)\n",
    "        french_sentences_no_eos = french_sentences_no_eos.to(device=device)\n",
    "        french_sentences_lengths = french_sentences_lengths.to(device=device)\n",
    "\n",
    "        # Encoding - Decoding for English -> French\n",
    "        encoder_outputs_en = encoder_en(english_sentences)\n",
    "        decoder_fr.init_state(english_sentences.unsqueeze(2).transpose(0,1), None, None)\n",
    "        dec_outs_fr, _ = decoder_fr(french_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_en[0].transpose(0,1), memory_lengths=english_sentences_lengths)\n",
    "\n",
    "        # Encoding - Decoding for French -> English\n",
    "        encoder_outputs_fr = encoder_fr(french_sentences)\n",
    "        decoder_en.init_state(french_sentences.unsqueeze(2).transpose(0,1), None, None) \n",
    "        dec_outs_en, _ = decoder_en(english_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_fr[0].transpose(0,1), memory_lengths=french_sentences_lengths)\n",
    "\n",
    "        if (regularize == \"attention\"):\n",
    "            # extracting the attention scores from the datasets; using 7th attention head\n",
    "            # as suggested by Clark et al, 2019 \n",
    "            english_attention = extract_attention_scores(_hooks_english)[6] \n",
    "            french_attention = extract_attention_scores(_hooks_french)[6]\n",
    "            batch_size = english_attention.shape[0]\n",
    "            english_attention_reshaped = english_attention.view(batch_size, -1)\n",
    "            french_attention_reshaped = french_attention.view(batch_size, -1)\n",
    "            \n",
    "            discriminator_outputs_en = attn_discriminator(english_attention_reshaped)\n",
    "            discriminator_outputs_fr = attn_discriminator(french_attention_reshaped)\n",
    "            discriminator_outputs_cat = torch.cat((discriminator_outputs_en, discriminator_outputs_fr))\n",
    "            if (batch_num % 2 == 0):\n",
    "                discriminator_labels = torch.tensor([1.0]*batch_size + [0.0]*batch_size)\n",
    "            else:\n",
    "                discriminator_labels = torch.tensor([0.0]*batch_size + [1.0]*batch_size)\n",
    "            discriminator_labels = discriminator_labels.unsqueeze(1).to(device=device)\n",
    "\n",
    "            \n",
    "            all_losses = loss_fn_single_regularization(english_sentences[:, 1:],\n",
    "                                               french_sentences[:, 1:],\n",
    "                                               dec_outs_en,\n",
    "                                               dec_outs_fr,\n",
    "                                               discriminator_labels,\n",
    "                                               discriminator_outputs_cat,\n",
    "                                              )\n",
    "            \n",
    "            (loss, loss_english_to_french, loss_french_to_english, regularizing_term) = all_losses\n",
    "            \n",
    "        elif (regularize == \"hidden_state\"):\n",
    "            # using the pooled outputs of the encoders for regularizing \n",
    "            discriminator_outputs_en = discriminator(encoder_outputs_en[1])\n",
    "            discriminator_outputs_fr = discriminator(encoder_outputs_fr[1])\n",
    "            discriminator_outputs_cat = torch.cat((discriminator_outputs_en, discriminator_outputs_fr))\n",
    "            batch_size = discriminator_outputs_en.shape[0]\n",
    "            \n",
    "            if (batch_num % 2 == 0):\n",
    "                discriminator_labels = torch.tensor([1.0]*batch_size + [0.0]*batch_size)\n",
    "            else:\n",
    "                discriminator_labels = torch.tensor([0.0]*batch_size + [1.0]*batch_size)\n",
    "            discriminator_labels = discriminator_labels.unsqueeze(1).to(device=device)\n",
    "\n",
    "            all_losses = loss_fn_single_regularization(english_sentences[:, 1:],\n",
    "                                               french_sentences[:, 1:],\n",
    "                                               dec_outs_en,\n",
    "                                               dec_outs_fr,\n",
    "                                               discriminator_labels,\n",
    "                                               discriminator_outputs_cat,\n",
    "                                              )\n",
    "            (loss, loss_english_to_french, loss_french_to_english, regularizing_term) = all_losses\n",
    "        else:\n",
    "            all_losses = loss_fn_no_regularization(english_sentences[:, 1:],\n",
    "                                           french_sentences[:, 1:],\n",
    "                                           dec_outs_en,\n",
    "                                           dec_outs_fr,\n",
    "                                          )\n",
    "            (loss, loss_english_to_french, loss_french_to_english) = all_losses\n",
    "            \n",
    "        # must be put here to avoid name claim by val loop\n",
    "        if (batch_num % 10 == 0):\n",
    "            print(\"Batch num {}: Loss {}\".format(batch_num, loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                                                   \n",
    "        write_to_tensorboard(\"CCE\", {\"en-fr\": loss_english_to_french.item(), \"fr-en\":loss_french_to_english.item()}, training=True, step=batch_num, writer=writer)\n",
    "    \n",
    "        # Running validation script  \n",
    "        if (batch_num > 0 and batch_num % 100 == 0):\n",
    "            with torch.no_grad():\n",
    "                _blue_scores_en_fr = []\n",
    "                _exact_matches_en_fr = []\n",
    "                _blue_scores_fr_en = []\n",
    "                _exact_matches_fr_en = []\n",
    "                for batch_num, batch in enumerate(val_data_iter):\n",
    "                    \n",
    "                    if (batch_num == 25):\n",
    "                        break\n",
    "                    \n",
    "                    # Reading in input and moving to device\n",
    "                    english_batch, french_batch = batch \n",
    "                    (english_sentences, english_sentences_no_eos, english_sentences_lengths) = english_batch\n",
    "                    (french_sentences, french_sentences_no_eos, french_sentences_lengths) = french_batch\n",
    "\n",
    "                    english_sentences = english_sentences.to(device=device)\n",
    "                    english_sentences_no_eos = english_sentences_no_eos.to(device=device)\n",
    "                    english_sentences_lengths = english_sentences_lengths.to(device=device)\n",
    "                    french_sentences = french_sentences.to(device=device)\n",
    "                    french_sentences_no_eos = french_sentences_no_eos.to(device=device)\n",
    "                    french_sentences_lengths = french_sentences_lengths.to(device=device)\n",
    "\n",
    "                    # Encoding - Decoding for English -> French\n",
    "                    encoder_outputs_en = encoder_en(english_sentences)\n",
    "                    decoder_fr.init_state(english_sentences.unsqueeze(2).transpose(0,1), None, None)\n",
    "                    dec_outs_fr, _ = decoder_fr(french_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_en[0].transpose(0,1), memory_lengths=english_sentences_lengths)\n",
    "                    \n",
    "                    # Encoding - Decoding for French -> English\n",
    "                    encoder_outputs_fr = encoder_fr(french_sentences)\n",
    "                    decoder_en.init_state(french_sentences.unsqueeze(2).transpose(0,1), None, None) \n",
    "                    dec_outs_en, _ = decoder_en(english_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_fr[0].transpose(0,1), memory_lengths=french_sentences_lengths)\n",
    "\n",
    "                    \n",
    "                    # Calculate BLUE Scores, EM and Perplexity\n",
    "                    predictions_fr = torch.argmax(dec_outs_fr, dim=2)\n",
    "                    predictions_en = torch.argmax(dec_outs_en, dim=2)\n",
    "                    \n",
    "                    for idx in range(french_sentences.shape[0]):\n",
    "                        detokenized_french_gt = tokenizer_fr.convert_tokens_to_string(french_sentences[idx,1:].tolist())\n",
    "                        detokenized_french_pred = tokenizer_fr.convert_tokens_to_string(predictions_fr[idx].tolist())\n",
    "                        \n",
    "                        _blue_score_en_fr = sentence_bleu(detokenized_french_gt, detokenized_french_pred)\n",
    "                        _blue_scores_en_fr.append(_blue_score_en_fr)\n",
    "                        \n",
    "                    for idx in range(english_sentences.shape[0]):\n",
    "           \n",
    "                        detokenized_english_gt = tokenizer_en.decode(english_sentences[idx,1:].tolist())\n",
    "                        detokenized_english_pred = tokenizer_en.decode(predictions_en[idx].tolist())\n",
    "                            \n",
    "                        _blue_score_fr_en = sentence_bleu(detokenized_english_gt, detokenized_english_pred)\n",
    "                        _blue_scores_fr_en.append(_blue_score_fr_en)\n",
    "                        \n",
    "                    _exact_match_en_fr = exact_match(predictions_fr, french_sentences[:,1:])\n",
    "                    _exact_matches_en_fr.append(_exact_match_en_fr)\n",
    "                    \n",
    "                    _exact_match_fr_en = exact_match(predictions_en, english_sentences[:,1:])\n",
    "                    _exact_matches_fr_en.append(_exact_match_fr_en)\n",
    "                    \n",
    "                                                   \n",
    "                avg_bleu_en_fr = sum(_blue_scores_en_fr)/len(_blue_scores_en_fr)\n",
    "                avg_bleu_fr_en = sum(_blue_scores_fr_en)/len(_blue_scores_fr_en)\n",
    "                avg_em_en_fr = sum(_exact_matches_en_fr)/len(_exact_matches_en_fr)\n",
    "                avg_em_fr_en = sum(_exact_matches_fr_en)/len(_exact_matches_fr_en)\n",
    "                                                   \n",
    "                write_to_tensorboard(\"BLEU\", {\"en-fr\": avg_bleu_en_fr, \"fr-en\":avg_bleu_fr_en}, training=False, step=batch_num, writer=writer)\n",
    "                write_to_tensorboard(\"EM\", {\"en-fr\": avg_em_en_fr, \"fr-en\":avg_em_fr_en}, training=False, step=batch_num, writer=writer)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 0: Loss 1538.1806640625\n",
      "Batch num 10: Loss 290.81146240234375\n",
      "Batch num 20: Loss 165.44488525390625\n",
      "Batch num 30: Loss 149.76431274414062\n",
      "Batch num 40: Loss 133.50181579589844\n",
      "Batch num 50: Loss 95.65170288085938\n",
      "Batch num 60: Loss 83.58039855957031\n",
      "Batch num 70: Loss 76.00181579589844\n",
      "Batch num 80: Loss 66.68729400634766\n",
      "Batch num 90: Loss 68.71720123291016\n",
      "Batch num 100: Loss 58.46533203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/mtl_bert_environment/lib/python3.5/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/u/nlp/anaconda/main/anaconda3/envs/mtl_bert_environment/lib/python3.5/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/u/nlp/anaconda/main/anaconda3/envs/mtl_bert_environment/lib/python3.5/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 110: Loss 61.50366973876953\n",
      "Batch num 120: Loss 42.91539001464844\n",
      "Batch num 130: Loss 45.67382049560547\n",
      "Batch num 140: Loss 43.91678237915039\n",
      "Batch num 150: Loss 48.81451416015625\n",
      "Batch num 160: Loss 48.088043212890625\n",
      "Batch num 170: Loss 43.744911193847656\n",
      "Batch num 180: Loss 42.92486572265625\n",
      "Batch num 190: Loss 33.52617645263672\n",
      "Batch num 200: Loss 36.62435531616211\n",
      "Batch num 210: Loss 32.46125411987305\n",
      "Batch num 220: Loss 39.31957244873047\n",
      "Batch num 230: Loss 28.299510955810547\n",
      "Batch num 240: Loss 30.110687255859375\n",
      "Batch num 250: Loss 38.564945220947266\n",
      "Batch num 260: Loss 39.400508880615234\n",
      "Batch num 270: Loss 32.4063720703125\n",
      "Batch num 280: Loss 30.811477661132812\n",
      "Batch num 290: Loss 27.167308807373047\n",
      "Batch num 300: Loss 33.816566467285156\n",
      "Batch num 310: Loss 26.51189422607422\n",
      "Batch num 320: Loss 27.344032287597656\n",
      "Batch num 330: Loss 31.231231689453125\n",
      "Batch num 340: Loss 22.12495994567871\n",
      "Batch num 350: Loss 26.32811737060547\n",
      "Batch num 360: Loss 24.62741470336914\n",
      "Batch num 370: Loss 28.834213256835938\n",
      "Batch num 380: Loss 21.111419677734375\n",
      "Batch num 390: Loss 24.99761962890625\n",
      "Batch num 400: Loss 20.95952033996582\n",
      "Batch num 410: Loss 25.248672485351562\n",
      "Batch num 420: Loss 30.535579681396484\n",
      "Batch num 430: Loss 27.625316619873047\n",
      "Batch num 440: Loss 18.592456817626953\n",
      "Batch num 450: Loss 19.869384765625\n",
      "Batch num 460: Loss 26.18659019470215\n",
      "Batch num 470: Loss 23.300052642822266\n",
      "Batch num 480: Loss 22.942991256713867\n",
      "Batch num 490: Loss 20.610443115234375\n",
      "Batch num 500: Loss 25.543777465820312\n",
      "Batch num 510: Loss 23.262351989746094\n",
      "Batch num 520: Loss 24.531116485595703\n",
      "Batch num 530: Loss 18.996807098388672\n",
      "Batch num 540: Loss 22.131175994873047\n",
      "Batch num 550: Loss 29.58319854736328\n",
      "Batch num 560: Loss 24.14466094970703\n",
      "Batch num 570: Loss 22.249996185302734\n",
      "Batch num 580: Loss 24.047019958496094\n",
      "Batch num 590: Loss 18.859737396240234\n",
      "Batch num 600: Loss 23.72793197631836\n",
      "Batch num 610: Loss 24.630023956298828\n",
      "Batch num 620: Loss 21.629905700683594\n",
      "Batch num 630: Loss 24.192155838012695\n",
      "Batch num 640: Loss 20.555618286132812\n",
      "Batch num 650: Loss 24.568252563476562\n",
      "Batch num 660: Loss 22.53305435180664\n",
      "Batch num 670: Loss 23.09902572631836\n",
      "Batch num 680: Loss 26.19011878967285\n",
      "Batch num 690: Loss 22.18256950378418\n",
      "Batch num 700: Loss 20.078174591064453\n",
      "Batch num 710: Loss 23.345928192138672\n",
      "Batch num 720: Loss 20.789297103881836\n",
      "Batch num 730: Loss 21.11205291748047\n",
      "Batch num 740: Loss 21.19115447998047\n",
      "Batch num 750: Loss 22.324655532836914\n",
      "Batch num 760: Loss 27.1357479095459\n",
      "Batch num 770: Loss 24.46829605102539\n",
      "Batch num 780: Loss 23.36711883544922\n",
      "Batch num 790: Loss 23.020328521728516\n",
      "Batch num 800: Loss 19.624053955078125\n",
      "Batch num 810: Loss 24.3631534576416\n",
      "Batch num 820: Loss 18.430688858032227\n",
      "Batch num 830: Loss 18.730743408203125\n",
      "Batch num 840: Loss 19.090959548950195\n",
      "Batch num 850: Loss 20.396034240722656\n",
      "Batch num 860: Loss 26.00402069091797\n",
      "Batch num 870: Loss 23.391902923583984\n",
      "Batch num 880: Loss 20.455717086791992\n",
      "Batch num 890: Loss 21.38846206665039\n",
      "Batch num 900: Loss 19.168476104736328\n",
      "Batch num 910: Loss 20.842784881591797\n",
      "Batch num 920: Loss 21.549158096313477\n",
      "Batch num 930: Loss 20.824932098388672\n",
      "Batch num 940: Loss 18.153141021728516\n",
      "Batch num 950: Loss 16.73032569885254\n",
      "Batch num 960: Loss 19.101686477661133\n",
      "Batch num 970: Loss 18.85748291015625\n",
      "Batch num 980: Loss 19.104198455810547\n",
      "Batch num 990: Loss 20.674453735351562\n",
      "Batch num 1000: Loss 18.916006088256836\n",
      "Batch num 1010: Loss 21.794971466064453\n",
      "Batch num 1020: Loss 20.58797836303711\n",
      "Batch num 1030: Loss 18.84059715270996\n",
      "Batch num 1040: Loss 22.784801483154297\n",
      "Batch num 1050: Loss 18.987403869628906\n",
      "Batch num 1060: Loss 22.67571449279785\n",
      "Batch num 1070: Loss 17.36520004272461\n",
      "Batch num 1080: Loss 21.492082595825195\n",
      "Batch num 1090: Loss 20.957401275634766\n",
      "Batch num 1100: Loss 18.778804779052734\n",
      "Batch num 1110: Loss 19.384201049804688\n",
      "Batch num 1120: Loss 20.39646339416504\n",
      "Batch num 1130: Loss 20.80215835571289\n",
      "Batch num 1140: Loss 20.02151870727539\n",
      "Batch num 1150: Loss 17.375816345214844\n",
      "Batch num 1160: Loss 18.58577537536621\n",
      "Batch num 1170: Loss 18.12148666381836\n",
      "Batch num 1180: Loss 17.856718063354492\n",
      "Batch num 1190: Loss 22.227455139160156\n",
      "Batch num 1200: Loss 17.966140747070312\n",
      "Batch num 1210: Loss 18.280527114868164\n",
      "Batch num 1220: Loss 16.032276153564453\n",
      "Batch num 1230: Loss 19.374473571777344\n",
      "Batch num 1240: Loss 20.23652458190918\n",
      "Batch num 1250: Loss 21.432708740234375\n",
      "Batch num 1260: Loss 17.52001190185547\n",
      "Batch num 1270: Loss 18.782367706298828\n",
      "Batch num 1280: Loss 16.5095157623291\n",
      "Batch num 1290: Loss 21.034915924072266\n",
      "Batch num 1300: Loss 20.1265811920166\n",
      "Batch num 1310: Loss 21.134777069091797\n",
      "Batch num 1320: Loss 17.958282470703125\n",
      "Batch num 1330: Loss 16.805334091186523\n",
      "Batch num 1340: Loss 19.961837768554688\n",
      "Batch num 1350: Loss 22.574827194213867\n",
      "Batch num 1360: Loss 18.493907928466797\n",
      "Batch num 1370: Loss 19.96661376953125\n",
      "Batch num 1380: Loss 19.148101806640625\n",
      "Batch num 1390: Loss 20.634033203125\n",
      "Batch num 1400: Loss 16.40546989440918\n",
      "Batch num 1410: Loss 17.272354125976562\n",
      "Batch num 1420: Loss 18.99626922607422\n",
      "Batch num 1430: Loss 17.592748641967773\n",
      "Batch num 1440: Loss 17.326221466064453\n",
      "Batch num 1450: Loss 19.829200744628906\n",
      "Batch num 1460: Loss 17.58106803894043\n",
      "Batch num 1470: Loss 18.419551849365234\n",
      "Batch num 1480: Loss 18.613731384277344\n",
      "Batch num 1490: Loss 19.47241973876953\n",
      "Batch num 1500: Loss 17.141857147216797\n",
      "Batch num 1510: Loss 14.906232833862305\n",
      "Batch num 1520: Loss 15.22082233428955\n",
      "Batch num 1530: Loss 18.69287109375\n",
      "Batch num 1540: Loss 16.261999130249023\n",
      "Batch num 1550: Loss 16.457508087158203\n",
      "Batch num 1560: Loss 16.442493438720703\n",
      "Batch num 1570: Loss 19.659687042236328\n",
      "Batch num 1580: Loss 16.997148513793945\n",
      "Batch num 1590: Loss 17.244369506835938\n",
      "Batch num 1600: Loss 16.115123748779297\n",
      "Batch num 1610: Loss 15.662162780761719\n",
      "Batch num 1620: Loss 17.443641662597656\n",
      "Batch num 1630: Loss 19.799667358398438\n",
      "Batch num 1640: Loss 17.935462951660156\n",
      "Batch num 1650: Loss 21.99854278564453\n",
      "Batch num 1660: Loss 17.43548583984375\n",
      "Batch num 1670: Loss 16.191425323486328\n",
      "Batch num 1680: Loss 19.533449172973633\n",
      "Batch num 1690: Loss 16.560993194580078\n",
      "Batch num 1700: Loss 20.480453491210938\n",
      "Batch num 1710: Loss 16.174577713012695\n",
      "Batch num 1720: Loss 18.89486312866211\n",
      "Batch num 1730: Loss 21.120956420898438\n",
      "Batch num 1740: Loss 17.95779037475586\n",
      "Batch num 1750: Loss 18.60637092590332\n",
      "Batch num 1760: Loss 17.895282745361328\n",
      "Batch num 1770: Loss 20.506311416625977\n",
      "Batch num 1780: Loss 16.901641845703125\n",
      "Batch num 1790: Loss 15.511831283569336\n",
      "Batch num 1800: Loss 16.058452606201172\n",
      "Batch num 1810: Loss 14.13731575012207\n",
      "Batch num 1820: Loss 19.329235076904297\n",
      "Batch num 1830: Loss 18.447677612304688\n",
      "Batch num 1840: Loss 18.659061431884766\n",
      "Batch num 1850: Loss 16.377708435058594\n",
      "Batch num 1860: Loss 16.690265655517578\n",
      "Batch num 1870: Loss 17.997175216674805\n",
      "Batch num 1880: Loss 15.74837875366211\n",
      "Batch num 1890: Loss 18.312211990356445\n",
      "Batch num 1900: Loss 20.140724182128906\n",
      "Batch num 1910: Loss 16.329526901245117\n",
      "Batch num 1920: Loss 15.032691955566406\n",
      "Batch num 1930: Loss 17.74628448486328\n",
      "Batch num 1940: Loss 18.713512420654297\n",
      "Batch num 1950: Loss 14.673118591308594\n",
      "Batch num 1960: Loss 18.773372650146484\n",
      "Batch num 1970: Loss 19.16938018798828\n",
      "Batch num 1980: Loss 17.297130584716797\n",
      "Batch num 1990: Loss 16.828872680664062\n",
      "Batch num 2000: Loss 17.491872787475586\n",
      "Batch num 2010: Loss 15.355691909790039\n",
      "Batch num 2020: Loss 15.798510551452637\n",
      "Batch num 2030: Loss 16.657978057861328\n",
      "Batch num 2040: Loss 18.23208236694336\n",
      "Batch num 2050: Loss 17.561996459960938\n",
      "Batch num 2060: Loss 17.24563217163086\n",
      "Batch num 2070: Loss 15.569747924804688\n",
      "Batch num 2080: Loss 16.67154312133789\n",
      "Batch num 2090: Loss 17.996509552001953\n",
      "Batch num 2100: Loss 15.926340103149414\n",
      "Batch num 2110: Loss 15.008794784545898\n",
      "Batch num 2120: Loss 15.98030948638916\n",
      "Batch num 2130: Loss 18.069120407104492\n",
      "Batch num 2140: Loss 14.349032402038574\n",
      "Batch num 2150: Loss 18.39031219482422\n",
      "Batch num 2160: Loss 16.616207122802734\n",
      "Batch num 2170: Loss 18.32860565185547\n",
      "Batch num 2180: Loss 17.60454559326172\n",
      "Batch num 2190: Loss 14.066632270812988\n",
      "Batch num 2200: Loss 17.621356964111328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 2210: Loss 20.53413200378418\n",
      "Batch num 2220: Loss 15.628986358642578\n",
      "Batch num 2230: Loss 16.13553810119629\n",
      "Batch num 2240: Loss 18.90988540649414\n",
      "Batch num 2250: Loss 16.247264862060547\n",
      "Batch num 2260: Loss 14.947830200195312\n",
      "Batch num 2270: Loss 16.749595642089844\n",
      "Batch num 2280: Loss 15.873306274414062\n",
      "Batch num 2290: Loss 19.189470291137695\n",
      "Batch num 2300: Loss 16.1698055267334\n",
      "Batch num 2310: Loss 15.115942001342773\n",
      "Batch num 2320: Loss 15.474618911743164\n",
      "Batch num 2330: Loss 17.020959854125977\n",
      "Batch num 2340: Loss 16.400415420532227\n",
      "Batch num 2350: Loss 14.388948440551758\n",
      "Batch num 2360: Loss 14.64815902709961\n",
      "Batch num 2370: Loss 16.385149002075195\n",
      "Batch num 2380: Loss 15.149892807006836\n",
      "Batch num 2390: Loss 18.040380477905273\n",
      "Batch num 2400: Loss 15.09393310546875\n",
      "Batch num 2410: Loss 19.349336624145508\n",
      "Batch num 2420: Loss 17.89707374572754\n",
      "Batch num 2430: Loss 14.088546752929688\n",
      "Batch num 2440: Loss 16.871841430664062\n",
      "Batch num 2450: Loss 14.468758583068848\n",
      "Batch num 2460: Loss 17.40582275390625\n",
      "Batch num 2470: Loss 17.387149810791016\n",
      "Batch num 2480: Loss 17.534626007080078\n",
      "Batch num 2490: Loss 13.800679206848145\n",
      "Batch num 2500: Loss 14.477983474731445\n",
      "Batch num 2510: Loss 13.187765121459961\n",
      "Batch num 2520: Loss 17.061330795288086\n",
      "Batch num 2530: Loss 17.6395263671875\n",
      "Batch num 2540: Loss 15.716602325439453\n",
      "Batch num 2550: Loss 14.233400344848633\n",
      "Batch num 2560: Loss 19.410655975341797\n",
      "Batch num 2570: Loss 16.59975814819336\n",
      "Batch num 2580: Loss 16.992712020874023\n",
      "Batch num 2590: Loss 15.285400390625\n",
      "Batch num 2600: Loss 16.289138793945312\n",
      "Batch num 2610: Loss 17.09667205810547\n",
      "Batch num 2620: Loss 13.274328231811523\n",
      "Batch num 2630: Loss 14.522833824157715\n",
      "Batch num 2640: Loss 12.360214233398438\n",
      "Batch num 2650: Loss 14.281696319580078\n",
      "Batch num 2660: Loss 13.97069263458252\n",
      "Batch num 2670: Loss 14.792985916137695\n",
      "Batch num 2680: Loss 15.860054016113281\n",
      "Batch num 2690: Loss 16.02985382080078\n",
      "Batch num 2700: Loss 16.802560806274414\n",
      "Batch num 2710: Loss 15.314298629760742\n",
      "Batch num 2720: Loss 16.350482940673828\n",
      "Batch num 2730: Loss 15.759303092956543\n",
      "Batch num 2740: Loss 14.84121322631836\n",
      "Batch num 2750: Loss 14.446165084838867\n",
      "Batch num 2760: Loss 16.307558059692383\n",
      "Batch num 2770: Loss 16.399703979492188\n",
      "Batch num 2780: Loss 14.063739776611328\n",
      "Batch num 2790: Loss 13.727777481079102\n",
      "Batch num 2800: Loss 12.910051345825195\n",
      "Batch num 2810: Loss 14.914358139038086\n",
      "Batch num 2820: Loss 16.745357513427734\n",
      "Batch num 2830: Loss 16.069290161132812\n",
      "Batch num 2840: Loss 13.693770408630371\n",
      "Batch num 2850: Loss 18.660667419433594\n",
      "Batch num 2860: Loss 16.9317626953125\n",
      "Batch num 2870: Loss 14.05600357055664\n",
      "Batch num 2880: Loss 17.51519012451172\n",
      "Batch num 2890: Loss 14.708654403686523\n",
      "Batch num 2900: Loss 15.18385124206543\n",
      "Batch num 2910: Loss 15.308271408081055\n",
      "Batch num 2920: Loss 14.616424560546875\n",
      "Batch num 2930: Loss 15.063148498535156\n",
      "Batch num 2940: Loss 15.34027099609375\n",
      "Batch num 2950: Loss 15.762537002563477\n",
      "Batch num 2960: Loss 15.71217155456543\n",
      "Batch num 2970: Loss 17.769149780273438\n",
      "Batch num 2980: Loss 16.746826171875\n",
      "Batch num 2990: Loss 14.462041854858398\n",
      "Batch num 3000: Loss 14.785457611083984\n",
      "Batch num 3010: Loss 16.6969051361084\n",
      "Batch num 3020: Loss 14.296451568603516\n",
      "Batch num 3030: Loss 13.421760559082031\n",
      "Batch num 3040: Loss 14.90938949584961\n",
      "Batch num 3050: Loss 17.970436096191406\n",
      "Batch num 3060: Loss 14.064428329467773\n",
      "Batch num 3070: Loss 14.558006286621094\n",
      "Batch num 3080: Loss 14.055032730102539\n",
      "Batch num 3090: Loss 13.145567893981934\n",
      "Batch num 3100: Loss 15.770353317260742\n",
      "Batch num 3110: Loss 15.905057907104492\n",
      "Batch num 3120: Loss 13.025936126708984\n",
      "Batch num 3130: Loss 15.194135665893555\n",
      "Batch num 3140: Loss 14.264970779418945\n",
      "Batch num 3150: Loss 14.506834030151367\n",
      "Batch num 3160: Loss 13.920679092407227\n",
      "Batch num 3170: Loss 14.214693069458008\n",
      "Batch num 3180: Loss 14.631340980529785\n",
      "Batch num 3190: Loss 16.720661163330078\n",
      "Batch num 3200: Loss 12.585622787475586\n",
      "Batch num 3210: Loss 12.890552520751953\n",
      "Batch num 3220: Loss 12.998329162597656\n",
      "Batch num 3230: Loss 14.302116394042969\n",
      "Batch num 3240: Loss 14.466592788696289\n",
      "Batch num 3250: Loss 14.064275741577148\n",
      "Batch num 3260: Loss 15.281672477722168\n",
      "Batch num 3270: Loss 14.581846237182617\n",
      "Batch num 3280: Loss 16.185420989990234\n",
      "Batch num 3290: Loss 14.79534912109375\n",
      "Batch num 3300: Loss 13.807701110839844\n",
      "Batch num 3310: Loss 12.76762866973877\n",
      "Batch num 3320: Loss 13.095538139343262\n",
      "Batch num 3330: Loss 16.602933883666992\n",
      "Batch num 3340: Loss 15.992167472839355\n",
      "Batch num 3350: Loss 15.076911926269531\n",
      "Batch num 3360: Loss 16.323728561401367\n",
      "Batch num 3370: Loss 13.258305549621582\n",
      "Batch num 3380: Loss 16.304195404052734\n",
      "Batch num 3390: Loss 15.248767852783203\n",
      "Batch num 3400: Loss 15.9169282913208\n",
      "Batch num 3410: Loss 14.626928329467773\n",
      "Batch num 3420: Loss 15.544601440429688\n",
      "Batch num 3430: Loss 14.554739952087402\n",
      "Batch num 3440: Loss 15.60500431060791\n",
      "Batch num 3450: Loss 14.208328247070312\n",
      "Batch num 3460: Loss 13.606088638305664\n",
      "Batch num 3470: Loss 14.161177635192871\n",
      "Batch num 3480: Loss 17.501407623291016\n",
      "Batch num 3490: Loss 13.230705261230469\n",
      "Batch num 3500: Loss 16.032848358154297\n",
      "Batch num 3510: Loss 15.660297393798828\n",
      "Batch num 3520: Loss 16.115013122558594\n",
      "Batch num 3530: Loss 13.673995971679688\n",
      "Batch num 3540: Loss 14.598217010498047\n",
      "Batch num 3550: Loss 15.539751052856445\n",
      "Batch num 3560: Loss 15.820121765136719\n",
      "Batch num 3570: Loss 12.01080322265625\n",
      "Batch num 3580: Loss 11.855263710021973\n",
      "Batch num 3590: Loss 13.935684204101562\n",
      "Batch num 3600: Loss 13.581772804260254\n",
      "Batch num 3610: Loss 15.010663986206055\n",
      "Batch num 3620: Loss 14.637056350708008\n",
      "Batch num 3630: Loss 13.04121208190918\n",
      "Batch num 3640: Loss 16.49620819091797\n",
      "Batch num 3650: Loss 15.022625923156738\n",
      "Batch num 3660: Loss 14.352046012878418\n",
      "Batch num 3670: Loss 13.866999626159668\n",
      "Batch num 3680: Loss 17.851661682128906\n",
      "Batch num 3690: Loss 12.957928657531738\n",
      "Batch num 3700: Loss 17.13581085205078\n",
      "Batch num 3710: Loss 14.319188117980957\n",
      "Batch num 3720: Loss 17.620285034179688\n",
      "Batch num 3730: Loss 13.866241455078125\n",
      "Batch num 3740: Loss 13.267011642456055\n",
      "Batch num 3750: Loss 14.003093719482422\n",
      "Batch num 3760: Loss 14.577764511108398\n",
      "Batch num 3770: Loss 17.112049102783203\n",
      "Batch num 3780: Loss 14.900435447692871\n",
      "Batch num 3790: Loss 13.420431137084961\n",
      "Batch num 3800: Loss 16.86247444152832\n",
      "Batch num 3810: Loss 12.153427124023438\n",
      "Batch num 3820: Loss 13.051471710205078\n",
      "Batch num 3830: Loss 12.301250457763672\n",
      "Batch num 3840: Loss 13.605108261108398\n",
      "Batch num 3850: Loss 14.006107330322266\n",
      "Batch num 3860: Loss 13.366680145263672\n",
      "Batch num 3870: Loss 13.223432540893555\n",
      "Batch num 3880: Loss 13.00161075592041\n",
      "Batch num 3890: Loss 12.110208511352539\n",
      "Batch num 3900: Loss 13.145631790161133\n",
      "Batch num 3910: Loss 11.802970886230469\n",
      "Batch num 3920: Loss 13.597148895263672\n",
      "Batch num 3930: Loss 13.735353469848633\n",
      "Batch num 3940: Loss 12.62684440612793\n",
      "Batch num 3950: Loss 14.682724952697754\n",
      "Batch num 3960: Loss 14.164772033691406\n",
      "Batch num 3970: Loss 14.600906372070312\n",
      "Batch num 3980: Loss 13.241504669189453\n",
      "Batch num 3990: Loss 12.590824127197266\n",
      "Batch num 4000: Loss 14.236509323120117\n",
      "Batch num 4010: Loss 13.819793701171875\n",
      "Batch num 4020: Loss 13.172492980957031\n",
      "Batch num 4030: Loss 14.704513549804688\n",
      "Batch num 4040: Loss 13.760171890258789\n",
      "Batch num 4050: Loss 12.761106491088867\n",
      "Batch num 4060: Loss 12.121461868286133\n",
      "Batch num 4070: Loss 13.470327377319336\n",
      "Batch num 4080: Loss 13.260723114013672\n",
      "Batch num 4090: Loss 14.83321762084961\n",
      "Batch num 4100: Loss 13.210004806518555\n",
      "Batch num 4110: Loss 13.295693397521973\n",
      "Batch num 4120: Loss 13.192094802856445\n",
      "Batch num 4130: Loss 12.94031810760498\n",
      "Batch num 4140: Loss 13.884333610534668\n",
      "Batch num 4150: Loss 13.751436233520508\n",
      "Batch num 4160: Loss 11.51640796661377\n",
      "Batch num 4170: Loss 14.174509048461914\n",
      "Batch num 4180: Loss 14.150209426879883\n",
      "Batch num 4190: Loss 13.107282638549805\n",
      "Batch num 4200: Loss 15.098501205444336\n",
      "Batch num 4210: Loss 11.90700912475586\n",
      "Batch num 4220: Loss 15.213335990905762\n",
      "Batch num 4230: Loss 14.61513614654541\n",
      "Batch num 4240: Loss 16.06719207763672\n",
      "Batch num 4250: Loss 12.621423721313477\n",
      "Batch num 4260: Loss 14.347091674804688\n",
      "Batch num 4270: Loss 12.20667839050293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 4280: Loss 14.837448120117188\n",
      "Batch num 4290: Loss 14.07795238494873\n",
      "Batch num 4300: Loss 13.6301908493042\n",
      "Batch num 4310: Loss 11.659866333007812\n",
      "Batch num 4320: Loss 12.39797306060791\n",
      "Batch num 4330: Loss 12.003469467163086\n",
      "Batch num 4340: Loss 13.75466537475586\n",
      "Batch num 4350: Loss 12.183684349060059\n",
      "Batch num 4360: Loss 12.728195190429688\n",
      "Batch num 4370: Loss 12.197675704956055\n",
      "Batch num 4380: Loss 13.98775863647461\n",
      "Batch num 4390: Loss 12.355978012084961\n",
      "Batch num 4400: Loss 12.436922073364258\n",
      "Batch num 4410: Loss 12.443350791931152\n",
      "Batch num 4420: Loss 12.746835708618164\n",
      "Batch num 4430: Loss 11.20602798461914\n",
      "Batch num 4440: Loss 13.297197341918945\n",
      "Batch num 4450: Loss 13.128600120544434\n",
      "Batch num 4460: Loss 12.414178848266602\n",
      "Batch num 4470: Loss 14.248371124267578\n",
      "Batch num 4480: Loss 13.470954895019531\n",
      "Batch num 4490: Loss 15.86652946472168\n",
      "Batch num 4500: Loss 15.757064819335938\n",
      "Batch num 4510: Loss 11.87980842590332\n",
      "Batch num 4520: Loss 14.84808349609375\n",
      "Batch num 4530: Loss 13.843826293945312\n",
      "Batch num 4540: Loss 10.657146453857422\n",
      "Batch num 4550: Loss 12.558249473571777\n",
      "Batch num 4560: Loss 14.74341106414795\n",
      "Batch num 4570: Loss 16.58482551574707\n",
      "Batch num 4580: Loss 14.454505920410156\n",
      "Batch num 4590: Loss 13.339815139770508\n",
      "Batch num 4600: Loss 10.015767097473145\n",
      "Batch num 4610: Loss 14.54794692993164\n",
      "Batch num 4620: Loss 12.725972175598145\n",
      "Batch num 4630: Loss 13.635818481445312\n",
      "Batch num 4640: Loss 14.04611587524414\n",
      "Batch num 4650: Loss 11.644709587097168\n",
      "Batch num 4660: Loss 13.323051452636719\n",
      "Batch num 4670: Loss 12.875518798828125\n",
      "Batch num 4680: Loss 14.240606307983398\n",
      "Batch num 4690: Loss 12.604121208190918\n",
      "Batch num 4700: Loss 11.877422332763672\n",
      "Batch num 4710: Loss 13.412151336669922\n",
      "Batch num 4720: Loss 13.904671669006348\n",
      "Batch num 4730: Loss 12.926822662353516\n",
      "Batch num 4740: Loss 13.419116020202637\n",
      "Batch num 4750: Loss 11.889954566955566\n",
      "Batch num 4760: Loss 13.937267303466797\n",
      "Batch num 4770: Loss 14.161920547485352\n",
      "Batch num 4780: Loss 11.85411262512207\n",
      "Batch num 4790: Loss 13.686745643615723\n",
      "Batch num 4800: Loss 14.443077087402344\n",
      "Batch num 4810: Loss 10.622852325439453\n",
      "Batch num 4820: Loss 12.090721130371094\n",
      "Batch num 4830: Loss 14.425232887268066\n",
      "Batch num 4840: Loss 12.184070587158203\n",
      "Batch num 4850: Loss 13.152429580688477\n",
      "Batch num 4860: Loss 15.593202590942383\n",
      "Batch num 4870: Loss 12.558317184448242\n",
      "Batch num 4880: Loss 13.115985870361328\n",
      "Batch num 4890: Loss 12.616211891174316\n",
      "Batch num 4900: Loss 11.402924537658691\n",
      "Batch num 4910: Loss 14.492225646972656\n",
      "Batch num 4920: Loss 11.939332008361816\n",
      "Batch num 4930: Loss 12.362032890319824\n",
      "Batch num 4940: Loss 12.82166862487793\n",
      "Batch num 4950: Loss 14.192834854125977\n",
      "Batch num 4960: Loss 12.110844612121582\n",
      "Batch num 4970: Loss 12.974847793579102\n",
      "Batch num 4980: Loss 12.919238090515137\n",
      "Batch num 4990: Loss 11.66210651397705\n",
      "Batch num 5000: Loss 12.386905670166016\n",
      "Batch num 5010: Loss 12.39145278930664\n",
      "Batch num 5020: Loss 15.060016632080078\n",
      "Batch num 5030: Loss 11.350503921508789\n",
      "Batch num 5040: Loss 11.907659530639648\n",
      "Batch num 5050: Loss 13.354326248168945\n",
      "Batch num 5060: Loss 13.324981689453125\n",
      "Batch num 5070: Loss 11.60401439666748\n",
      "Batch num 5080: Loss 13.083932876586914\n",
      "Batch num 5090: Loss 10.066165924072266\n",
      "Batch num 5100: Loss 12.099407196044922\n",
      "Batch num 5110: Loss 15.439358711242676\n",
      "Batch num 5120: Loss 14.246661186218262\n",
      "Batch num 5130: Loss 12.83795166015625\n",
      "Batch num 5140: Loss 14.104571342468262\n",
      "Batch num 5150: Loss 13.741887092590332\n",
      "Batch num 5160: Loss 11.749502182006836\n",
      "Batch num 5170: Loss 11.74024772644043\n",
      "Batch num 5180: Loss 12.627970695495605\n",
      "Batch num 5190: Loss 13.053352355957031\n",
      "Batch num 5200: Loss 13.857494354248047\n",
      "Batch num 5210: Loss 12.023530006408691\n",
      "Batch num 5220: Loss 13.216947555541992\n",
      "Batch num 5230: Loss 13.46034049987793\n",
      "Batch num 5240: Loss 12.50874137878418\n",
      "Batch num 5250: Loss 11.98341178894043\n",
      "Batch num 5260: Loss 11.250378608703613\n",
      "Batch num 5270: Loss 12.50374984741211\n",
      "Batch num 5280: Loss 13.630556106567383\n",
      "Batch num 5290: Loss 13.356571197509766\n",
      "Batch num 5300: Loss 11.97640609741211\n",
      "Batch num 5310: Loss 12.567829132080078\n",
      "Batch num 5320: Loss 12.233802795410156\n",
      "Batch num 5330: Loss 12.635794639587402\n",
      "Batch num 5340: Loss 12.688322067260742\n",
      "Batch num 5350: Loss 12.95250129699707\n",
      "Batch num 5360: Loss 13.405970573425293\n",
      "Batch num 5370: Loss 14.158968925476074\n",
      "Batch num 5380: Loss 12.691120147705078\n",
      "Batch num 5390: Loss 13.043405532836914\n",
      "Batch num 5400: Loss 13.452299118041992\n",
      "Batch num 5410: Loss 11.692916870117188\n",
      "Batch num 5420: Loss 11.731609344482422\n",
      "Batch num 5430: Loss 13.283590316772461\n",
      "Batch num 5440: Loss 11.696771621704102\n",
      "Batch num 5450: Loss 13.167977333068848\n",
      "Batch num 5460: Loss 12.314334869384766\n",
      "Batch num 5470: Loss 11.988872528076172\n",
      "Batch num 5480: Loss 9.87950325012207\n",
      "Batch num 5490: Loss 10.882540702819824\n",
      "Batch num 5500: Loss 13.813827514648438\n",
      "Batch num 5510: Loss 11.621081352233887\n",
      "Batch num 5520: Loss 11.773580551147461\n",
      "Batch num 5530: Loss 11.68301773071289\n",
      "Batch num 5540: Loss 14.36679458618164\n",
      "Batch num 5550: Loss 12.320579528808594\n",
      "Batch num 5560: Loss 12.268820762634277\n",
      "Batch num 5570: Loss 11.266563415527344\n",
      "Batch num 5580: Loss 11.478622436523438\n",
      "Batch num 5590: Loss 11.593785285949707\n",
      "Batch num 5600: Loss 12.556921005249023\n",
      "Batch num 5610: Loss 12.004212379455566\n",
      "Batch num 5620: Loss 12.758705139160156\n",
      "Batch num 5630: Loss 11.132081985473633\n",
      "Batch num 5640: Loss 15.414225578308105\n",
      "Batch num 5650: Loss 12.310516357421875\n",
      "Batch num 5660: Loss 11.684906005859375\n",
      "Batch num 5670: Loss 10.086471557617188\n",
      "Batch num 5680: Loss 11.255805969238281\n",
      "Batch num 5690: Loss 12.589353561401367\n",
      "Batch num 5700: Loss 12.672704696655273\n",
      "Batch num 5710: Loss 12.151366233825684\n",
      "Batch num 5720: Loss 12.342998504638672\n",
      "Batch num 5730: Loss 12.933076858520508\n",
      "Batch num 5740: Loss 14.4317626953125\n",
      "Batch num 5750: Loss 14.111625671386719\n",
      "Batch num 5760: Loss 12.959068298339844\n",
      "Batch num 5770: Loss 11.344955444335938\n",
      "Batch num 5780: Loss 13.093303680419922\n",
      "Batch num 5790: Loss 14.169660568237305\n",
      "Batch num 5800: Loss 12.567178726196289\n",
      "Batch num 5810: Loss 11.003894805908203\n",
      "Batch num 5820: Loss 13.133075714111328\n",
      "Batch num 5830: Loss 12.962844848632812\n",
      "Batch num 5840: Loss 10.818021774291992\n",
      "Batch num 5850: Loss 10.141988754272461\n",
      "Batch num 5860: Loss 12.633235931396484\n",
      "Batch num 5870: Loss 11.940073013305664\n",
      "Batch num 5880: Loss 13.429082870483398\n",
      "Batch num 5890: Loss 12.211527824401855\n",
      "Batch num 5900: Loss 12.722921371459961\n",
      "Batch num 5910: Loss 11.919805526733398\n",
      "Batch num 5920: Loss 12.94243049621582\n",
      "Batch num 5930: Loss 11.809946060180664\n",
      "Batch num 5940: Loss 14.05833625793457\n",
      "Batch num 5950: Loss 13.513912200927734\n",
      "Batch num 5960: Loss 11.807682037353516\n",
      "Batch num 5970: Loss 12.741392135620117\n",
      "Batch num 5980: Loss 13.056756973266602\n",
      "Batch num 5990: Loss 12.029443740844727\n",
      "Batch num 6000: Loss 16.603595733642578\n",
      "Batch num 6010: Loss 13.062847137451172\n",
      "Batch num 6020: Loss 10.371109008789062\n",
      "Batch num 6030: Loss 12.585732460021973\n",
      "Batch num 6040: Loss 10.869394302368164\n",
      "Batch num 6050: Loss 11.814567565917969\n",
      "Batch num 6060: Loss 12.655741691589355\n",
      "Batch num 6070: Loss 12.535659790039062\n",
      "Batch num 6080: Loss 12.988977432250977\n",
      "Batch num 6090: Loss 13.290962219238281\n",
      "Batch num 6100: Loss 11.748479843139648\n",
      "Batch num 6110: Loss 12.063164710998535\n",
      "Batch num 6120: Loss 13.67430305480957\n",
      "Batch num 6130: Loss 12.31576919555664\n",
      "Batch num 6140: Loss 14.196365356445312\n",
      "Batch num 6150: Loss 12.442541122436523\n",
      "Batch num 6160: Loss 12.280385971069336\n",
      "Batch num 6170: Loss 11.905426025390625\n",
      "Batch num 6180: Loss 13.243034362792969\n",
      "Batch num 6190: Loss 11.653541564941406\n",
      "Batch num 6200: Loss 13.31149959564209\n",
      "Batch num 6210: Loss 12.487889289855957\n",
      "Batch num 6220: Loss 13.431536674499512\n",
      "Batch num 6230: Loss 12.663492202758789\n",
      "Batch num 6240: Loss 10.525680541992188\n",
      "Batch num 6250: Loss 11.971363067626953\n",
      "Batch num 6260: Loss 12.170413970947266\n",
      "Batch num 6270: Loss 12.527113914489746\n",
      "Batch num 6280: Loss 11.568017959594727\n",
      "Batch num 6290: Loss 12.870283126831055\n",
      "Batch num 6300: Loss 11.883905410766602\n",
      "Batch num 6310: Loss 12.48651123046875\n",
      "Batch num 6320: Loss 10.053075790405273\n",
      "Batch num 6330: Loss 12.329301834106445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 6340: Loss 12.538497924804688\n",
      "Batch num 6350: Loss 11.171161651611328\n",
      "Batch num 6360: Loss 11.874031066894531\n",
      "Batch num 6370: Loss 10.31435775756836\n",
      "Batch num 6380: Loss 12.271791458129883\n",
      "Batch num 6390: Loss 12.640090942382812\n",
      "Batch num 6400: Loss 11.508548736572266\n",
      "Batch num 6410: Loss 12.406991958618164\n",
      "Batch num 6420: Loss 12.513989448547363\n",
      "Batch num 6430: Loss 11.909392356872559\n",
      "Batch num 6440: Loss 11.9193754196167\n",
      "Batch num 6450: Loss 12.210372924804688\n",
      "Batch num 6460: Loss 12.908041000366211\n",
      "Batch num 6470: Loss 14.682632446289062\n",
      "Batch num 6480: Loss 12.518199920654297\n",
      "Batch num 6490: Loss 11.82234001159668\n",
      "Batch num 6500: Loss 11.53640365600586\n",
      "Batch num 6510: Loss 13.381515502929688\n",
      "Batch num 6520: Loss 12.781628608703613\n",
      "Batch num 6530: Loss 12.057126998901367\n",
      "Batch num 6540: Loss 12.989946365356445\n",
      "Batch num 6550: Loss 14.086227416992188\n",
      "Batch num 6560: Loss 10.890249252319336\n",
      "Batch num 6570: Loss 11.354394912719727\n",
      "Batch num 6580: Loss 12.269386291503906\n",
      "Batch num 6590: Loss 11.52372932434082\n",
      "Batch num 6600: Loss 9.535905838012695\n",
      "Batch num 6610: Loss 12.566106796264648\n",
      "Batch num 6620: Loss 11.847686767578125\n",
      "Batch num 6630: Loss 12.073370933532715\n",
      "Batch num 6640: Loss 13.165046691894531\n",
      "Batch num 6650: Loss 11.496658325195312\n",
      "Batch num 6660: Loss 12.098848342895508\n",
      "Batch num 6670: Loss 11.287014961242676\n",
      "Batch num 6680: Loss 9.29854965209961\n",
      "Batch num 6690: Loss 14.436646461486816\n",
      "Batch num 6700: Loss 10.46108627319336\n",
      "Batch num 6710: Loss 11.62700080871582\n",
      "Batch num 6720: Loss 11.868753433227539\n",
      "Batch num 6730: Loss 12.381349563598633\n",
      "Batch num 6740: Loss 12.853232383728027\n",
      "Batch num 6750: Loss 12.494569778442383\n",
      "Batch num 6760: Loss 12.048904418945312\n",
      "Batch num 6770: Loss 13.300317764282227\n",
      "Batch num 6780: Loss 11.039178848266602\n",
      "Batch num 6790: Loss 11.561532974243164\n",
      "Batch num 6800: Loss 11.966296195983887\n",
      "Batch num 6810: Loss 12.133127212524414\n",
      "Batch num 6820: Loss 12.084999084472656\n",
      "Batch num 6830: Loss 11.64545726776123\n",
      "Batch num 6840: Loss 12.266027450561523\n",
      "Batch num 6850: Loss 12.150506019592285\n",
      "Batch num 6860: Loss 13.309907913208008\n",
      "Batch num 6870: Loss 11.504995346069336\n",
      "Batch num 6880: Loss 11.67418098449707\n",
      "Batch num 6890: Loss 11.616533279418945\n",
      "Batch num 6900: Loss 12.319316864013672\n",
      "Batch num 6910: Loss 12.594298362731934\n",
      "Batch num 6920: Loss 13.625740051269531\n",
      "Batch num 6930: Loss 13.336166381835938\n",
      "Batch num 6940: Loss 10.668594360351562\n",
      "Batch num 6950: Loss 13.047832489013672\n",
      "Batch num 6960: Loss 13.103050231933594\n",
      "Batch num 6970: Loss 13.002467155456543\n",
      "Batch num 6980: Loss 11.849264144897461\n",
      "Batch num 6990: Loss 11.955131530761719\n",
      "Batch num 7000: Loss 12.667720794677734\n",
      "Batch num 7010: Loss 13.008527755737305\n",
      "Batch num 7020: Loss 12.23794937133789\n",
      "Batch num 7030: Loss 11.836917877197266\n",
      "Batch num 7040: Loss 12.719690322875977\n",
      "Batch num 7050: Loss 13.658781051635742\n",
      "Batch num 7060: Loss 10.612364768981934\n",
      "Batch num 7070: Loss 11.843738555908203\n",
      "Batch num 7080: Loss 13.342096328735352\n",
      "Batch num 7090: Loss 10.960493087768555\n",
      "Batch num 7100: Loss 15.952873229980469\n",
      "Batch num 7110: Loss 12.347087860107422\n",
      "Batch num 7120: Loss 11.975862503051758\n",
      "Batch num 7130: Loss 11.726192474365234\n",
      "Batch num 7140: Loss 11.688212394714355\n",
      "Batch num 7150: Loss 10.234240531921387\n",
      "Batch num 7160: Loss 12.825077056884766\n",
      "Batch num 7170: Loss 11.908246994018555\n",
      "Batch num 7180: Loss 12.047977447509766\n",
      "Batch num 7190: Loss 11.590609550476074\n",
      "Batch num 7200: Loss 12.536806106567383\n",
      "Batch num 7210: Loss 12.719995498657227\n",
      "Batch num 7220: Loss 13.493339538574219\n",
      "Batch num 7230: Loss 10.048391342163086\n",
      "Batch num 7240: Loss 12.88427734375\n",
      "Batch num 7250: Loss 11.58009147644043\n",
      "Batch num 7260: Loss 11.305570602416992\n",
      "Batch num 7270: Loss 11.119417190551758\n",
      "Batch num 7280: Loss 10.804546356201172\n",
      "Batch num 7290: Loss 11.72481918334961\n",
      "Batch num 7300: Loss 12.670307159423828\n",
      "Batch num 7310: Loss 11.557587623596191\n",
      "Batch num 7320: Loss 12.509307861328125\n",
      "Batch num 7330: Loss 12.045578002929688\n",
      "Batch num 7340: Loss 12.50323486328125\n",
      "Batch num 7350: Loss 14.411528587341309\n",
      "Batch num 7360: Loss 12.134222030639648\n",
      "Batch num 7370: Loss 12.899965286254883\n",
      "Batch num 7380: Loss 12.588151931762695\n",
      "Batch num 7390: Loss 12.819561004638672\n",
      "Batch num 7400: Loss 12.729484558105469\n",
      "Batch num 7410: Loss 11.708989143371582\n",
      "Batch num 7420: Loss 12.401836395263672\n",
      "Batch num 7430: Loss 11.178884506225586\n",
      "Batch num 7440: Loss 11.576146125793457\n",
      "Batch num 7450: Loss 13.917825698852539\n",
      "Batch num 7460: Loss 12.656013488769531\n",
      "Batch num 7470: Loss 11.696282386779785\n",
      "Batch num 7480: Loss 11.318573951721191\n",
      "Batch num 7490: Loss 12.959842681884766\n",
      "Batch num 7500: Loss 11.883041381835938\n",
      "Batch num 7510: Loss 13.342897415161133\n",
      "Batch num 7520: Loss 12.558730125427246\n",
      "Batch num 7530: Loss 12.211465835571289\n",
      "Batch num 7540: Loss 12.209868431091309\n",
      "Batch num 7550: Loss 12.195637702941895\n",
      "Batch num 7560: Loss 11.07126235961914\n",
      "Batch num 7570: Loss 11.702049255371094\n",
      "Batch num 7580: Loss 11.014945030212402\n",
      "Batch num 7590: Loss 12.618974685668945\n",
      "Batch num 7600: Loss 10.788748741149902\n",
      "Batch num 7610: Loss 10.821977615356445\n",
      "Batch num 7620: Loss 12.881794929504395\n",
      "Batch num 7630: Loss 11.410598754882812\n",
      "Batch num 7640: Loss 12.200675964355469\n",
      "Batch num 7650: Loss 13.980417251586914\n",
      "Batch num 7660: Loss 9.485800743103027\n",
      "Batch num 7670: Loss 11.495744705200195\n",
      "Batch num 7680: Loss 12.984451293945312\n",
      "Batch num 7690: Loss 10.549830436706543\n",
      "Batch num 7700: Loss 13.015832901000977\n",
      "Batch num 7710: Loss 11.584367752075195\n",
      "Batch num 7720: Loss 12.655340194702148\n",
      "Batch num 7730: Loss 12.69479751586914\n",
      "Batch num 7740: Loss 11.660284042358398\n",
      "Batch num 7750: Loss 10.48292350769043\n",
      "Batch num 7760: Loss 14.915433883666992\n",
      "Batch num 7770: Loss 10.890270233154297\n",
      "Batch num 7780: Loss 11.97018814086914\n",
      "Batch num 7790: Loss 9.704663276672363\n",
      "Batch num 7800: Loss 13.10865592956543\n",
      "Batch num 7810: Loss 13.712709426879883\n",
      "Batch num 7820: Loss 11.253032684326172\n",
      "Batch num 7830: Loss 10.681215286254883\n",
      "Batch num 7840: Loss 12.922952651977539\n",
      "Batch num 7850: Loss 12.590367317199707\n",
      "Batch num 7860: Loss 8.807994842529297\n",
      "Batch num 7870: Loss 11.747377395629883\n",
      "Batch num 7880: Loss 12.73006820678711\n",
      "Batch num 7890: Loss 12.396734237670898\n",
      "Batch num 7900: Loss 11.333136558532715\n",
      "Batch num 7910: Loss 11.830305099487305\n",
      "Batch num 7920: Loss 11.687006950378418\n",
      "Batch num 7930: Loss 13.090690612792969\n",
      "Batch num 7940: Loss 11.22178840637207\n",
      "Batch num 7950: Loss 12.642780303955078\n",
      "Batch num 7960: Loss 10.629595756530762\n",
      "Batch num 7970: Loss 10.678201675415039\n",
      "Batch num 7980: Loss 12.389408111572266\n",
      "Batch num 7990: Loss 12.169979095458984\n",
      "Batch num 8000: Loss 11.094961166381836\n",
      "Batch num 8010: Loss 12.16285514831543\n",
      "Batch num 8020: Loss 13.652565002441406\n",
      "Batch num 8030: Loss 12.637731552124023\n",
      "Batch num 8040: Loss 10.764300346374512\n",
      "Batch num 8050: Loss 10.365214347839355\n",
      "Batch num 8060: Loss 12.304866790771484\n",
      "Batch num 8070: Loss 11.792771339416504\n",
      "Batch num 8080: Loss 11.256900787353516\n",
      "Batch num 8090: Loss 13.62448501586914\n",
      "Batch num 8100: Loss 10.146944999694824\n",
      "Batch num 8110: Loss 11.540170669555664\n",
      "Batch num 8120: Loss 12.892523765563965\n",
      "Batch num 8130: Loss 12.087644577026367\n",
      "Batch num 8140: Loss 10.633487701416016\n",
      "Batch num 8150: Loss 9.667428970336914\n",
      "Batch num 8160: Loss 12.707088470458984\n",
      "Batch num 8170: Loss 8.035600662231445\n",
      "Batch num 8180: Loss 12.88176155090332\n",
      "Batch num 8190: Loss 12.51219367980957\n",
      "Batch num 8200: Loss 10.584352493286133\n",
      "Batch num 8210: Loss 11.869836807250977\n",
      "Batch num 8220: Loss 11.944835662841797\n",
      "Batch num 8230: Loss 11.571285247802734\n",
      "Batch num 8240: Loss 11.012153625488281\n",
      "Batch num 8250: Loss 11.682597160339355\n",
      "Batch num 8260: Loss 9.9903564453125\n",
      "Batch num 8270: Loss 11.392147064208984\n",
      "Batch num 8280: Loss 13.07436466217041\n",
      "Batch num 8290: Loss 11.824209213256836\n",
      "Batch num 8300: Loss 13.079341888427734\n",
      "Batch num 8310: Loss 10.964597702026367\n",
      "Batch num 8320: Loss 11.79283332824707\n",
      "Batch num 8330: Loss 12.44680118560791\n",
      "Batch num 8340: Loss 11.726066589355469\n",
      "Batch num 8350: Loss 11.959821701049805\n",
      "Batch num 8360: Loss 11.269668579101562\n",
      "Batch num 8370: Loss 11.450888633728027\n",
      "Batch num 8380: Loss 10.99606704711914\n",
      "Batch num 8390: Loss 12.48377799987793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 8400: Loss 12.0958251953125\n",
      "Batch num 8410: Loss 9.964010238647461\n",
      "Batch num 8420: Loss 12.56396484375\n",
      "Batch num 8430: Loss 12.472752571105957\n",
      "Batch num 8440: Loss 11.968271255493164\n",
      "Batch num 8450: Loss 12.842056274414062\n",
      "Batch num 8460: Loss 14.150684356689453\n",
      "Batch num 8470: Loss 12.351936340332031\n",
      "Batch num 8480: Loss 13.734625816345215\n",
      "Batch num 8490: Loss 12.33475399017334\n",
      "Batch num 8500: Loss 10.920330047607422\n",
      "Batch num 8510: Loss 10.494182586669922\n",
      "Batch num 8520: Loss 12.374107360839844\n",
      "Batch num 8530: Loss 13.06254768371582\n",
      "Batch num 8540: Loss 11.628233909606934\n",
      "Batch num 8550: Loss 9.647408485412598\n",
      "Batch num 8560: Loss 12.325315475463867\n",
      "Batch num 8570: Loss 11.383443832397461\n",
      "Batch num 8580: Loss 12.634618759155273\n",
      "Batch num 8590: Loss 11.958566665649414\n",
      "Batch num 8600: Loss 12.466405868530273\n",
      "Batch num 8610: Loss 11.249820709228516\n",
      "Batch num 8620: Loss 11.21460247039795\n",
      "Batch num 8630: Loss 12.28781509399414\n",
      "Batch num 8640: Loss 10.765907287597656\n",
      "Batch num 8650: Loss 10.667098999023438\n",
      "Batch num 8660: Loss 12.804492950439453\n",
      "Batch num 8670: Loss 10.918428421020508\n",
      "Batch num 8680: Loss 11.948885917663574\n",
      "Batch num 8690: Loss 10.196318626403809\n",
      "Batch num 8700: Loss 13.814189910888672\n",
      "Batch num 8710: Loss 11.898515701293945\n",
      "Batch num 8720: Loss 11.091791152954102\n",
      "Batch num 8730: Loss 10.84945297241211\n",
      "Batch num 8740: Loss 12.750923156738281\n",
      "Batch num 8750: Loss 11.647171020507812\n",
      "Batch num 8760: Loss 12.087348937988281\n",
      "Batch num 8770: Loss 11.49057388305664\n",
      "Batch num 8780: Loss 11.506150245666504\n",
      "Batch num 8790: Loss 11.809656143188477\n",
      "Batch num 8800: Loss 12.212121963500977\n",
      "Batch num 8810: Loss 12.921892166137695\n",
      "Batch num 8820: Loss 12.48055648803711\n",
      "Batch num 8830: Loss 12.414058685302734\n",
      "Batch num 8840: Loss 11.754446029663086\n",
      "Batch num 8850: Loss 12.151081085205078\n",
      "Batch num 8860: Loss 11.278456687927246\n",
      "Batch num 8870: Loss 10.812029838562012\n",
      "Batch num 8880: Loss 10.848976135253906\n",
      "Batch num 8890: Loss 9.51475715637207\n",
      "Batch num 8900: Loss 12.01661491394043\n",
      "Batch num 8910: Loss 10.653050422668457\n",
      "Batch num 8920: Loss 12.660512924194336\n",
      "Batch num 8930: Loss 12.512092590332031\n",
      "Batch num 8940: Loss 12.233561515808105\n",
      "Batch num 8950: Loss 10.862688064575195\n",
      "Batch num 8960: Loss 10.417083740234375\n",
      "Batch num 8970: Loss 14.284204483032227\n",
      "Batch num 8980: Loss 12.496959686279297\n",
      "Batch num 8990: Loss 11.275712013244629\n",
      "Batch num 9000: Loss 11.606157302856445\n",
      "Batch num 9010: Loss 11.711116790771484\n",
      "Batch num 9020: Loss 11.913167953491211\n",
      "Batch num 9030: Loss 10.095155715942383\n",
      "Batch num 9040: Loss 10.948549270629883\n",
      "Batch num 9050: Loss 11.572021484375\n",
      "Batch num 9060: Loss 9.684953689575195\n",
      "Batch num 9070: Loss 12.010381698608398\n",
      "Batch num 9080: Loss 13.410171508789062\n",
      "Batch num 9090: Loss 11.902323722839355\n",
      "Batch num 9100: Loss 11.662537574768066\n",
      "Batch num 9110: Loss 12.80525016784668\n",
      "Batch num 9120: Loss 10.45533561706543\n",
      "Batch num 9130: Loss 12.531936645507812\n",
      "Batch num 9140: Loss 11.206844329833984\n",
      "Batch num 9150: Loss 11.37861156463623\n",
      "Batch num 9160: Loss 11.614349365234375\n",
      "Batch num 9170: Loss 12.846508026123047\n",
      "Batch num 9180: Loss 10.070293426513672\n",
      "Batch num 9190: Loss 13.061460494995117\n",
      "Batch num 9200: Loss 11.466902732849121\n",
      "Batch num 9210: Loss 10.135541915893555\n",
      "Batch num 9220: Loss 11.503761291503906\n",
      "Batch num 9230: Loss 8.087321281433105\n",
      "Batch num 9240: Loss 11.825151443481445\n",
      "Batch num 9250: Loss 11.529601097106934\n",
      "Batch num 9260: Loss 10.607102394104004\n",
      "Batch num 9270: Loss 11.592750549316406\n",
      "Batch num 9280: Loss 10.792684555053711\n",
      "Batch num 9290: Loss 13.254132270812988\n",
      "Batch num 9300: Loss 10.664388656616211\n",
      "Batch num 9310: Loss 11.623259544372559\n",
      "Batch num 9320: Loss 11.959107398986816\n",
      "Batch num 9330: Loss 12.39954948425293\n",
      "Batch num 9340: Loss 12.386245727539062\n",
      "Batch num 9350: Loss 11.10084342956543\n",
      "Batch num 9360: Loss 10.772015571594238\n",
      "Batch num 9370: Loss 13.825067520141602\n",
      "Batch num 9380: Loss 11.486618995666504\n",
      "Batch num 9390: Loss 11.499374389648438\n",
      "Batch num 9400: Loss 9.13640308380127\n",
      "Batch num 9410: Loss 9.519180297851562\n",
      "Batch num 9420: Loss 11.91019058227539\n",
      "Batch num 9430: Loss 10.283998489379883\n",
      "Batch num 9440: Loss 12.582857131958008\n",
      "Batch num 9450: Loss 11.089941024780273\n",
      "Batch num 9460: Loss 10.865790367126465\n",
      "Batch num 9470: Loss 13.311269760131836\n",
      "Batch num 9480: Loss 10.497056007385254\n",
      "Batch num 9490: Loss 11.954427719116211\n",
      "Batch num 9500: Loss 10.363944053649902\n",
      "Batch num 9510: Loss 9.557573318481445\n",
      "Batch num 9520: Loss 12.384642601013184\n",
      "Batch num 9530: Loss 10.943074226379395\n",
      "Batch num 9540: Loss 11.916702270507812\n",
      "Batch num 9550: Loss 11.150568962097168\n",
      "Batch num 9560: Loss 11.035791397094727\n",
      "Batch num 9570: Loss 13.693510055541992\n",
      "Batch num 9580: Loss 11.587259292602539\n",
      "Batch num 9590: Loss 10.41356086730957\n",
      "Batch num 9600: Loss 13.527759552001953\n",
      "Batch num 9610: Loss 12.179058074951172\n",
      "Batch num 9620: Loss 12.0007963180542\n",
      "Batch num 9630: Loss 11.107097625732422\n",
      "Batch num 9640: Loss 11.587258338928223\n",
      "Batch num 9650: Loss 13.121011734008789\n",
      "Batch num 9660: Loss 11.015880584716797\n",
      "Batch num 9670: Loss 10.895368576049805\n",
      "Batch num 9680: Loss 11.468384742736816\n",
      "Batch num 9690: Loss 12.177692413330078\n",
      "Batch num 9700: Loss 13.913270950317383\n",
      "Batch num 9710: Loss 12.005414962768555\n",
      "Batch num 9720: Loss 13.346324920654297\n",
      "Batch num 9730: Loss 13.501599311828613\n",
      "Batch num 9740: Loss 12.027685165405273\n",
      "Batch num 9750: Loss 12.652843475341797\n",
      "Batch num 9760: Loss 9.460363388061523\n",
      "Batch num 9770: Loss 13.987175941467285\n",
      "Batch num 9780: Loss 11.088811874389648\n",
      "Batch num 9790: Loss 10.230691909790039\n",
      "Batch num 9800: Loss 11.367303848266602\n",
      "Batch num 9810: Loss 11.308467864990234\n",
      "Batch num 9820: Loss 11.897920608520508\n",
      "Batch num 9830: Loss 11.349638938903809\n",
      "Batch num 9840: Loss 13.336186408996582\n",
      "Batch num 9850: Loss 10.761274337768555\n",
      "Batch num 9860: Loss 9.173768997192383\n",
      "Batch num 9870: Loss 10.708337783813477\n",
      "Batch num 9880: Loss 11.316017150878906\n",
      "Batch num 9890: Loss 10.761014938354492\n",
      "Batch num 9900: Loss 13.347543716430664\n",
      "Batch num 9910: Loss 12.085803031921387\n",
      "Batch num 9920: Loss 12.73507308959961\n",
      "Batch num 9930: Loss 11.46910572052002\n",
      "Batch num 9940: Loss 11.338586807250977\n",
      "Batch num 9950: Loss 10.728702545166016\n",
      "Batch num 9960: Loss 13.274775505065918\n",
      "Batch num 9970: Loss 10.795719146728516\n",
      "Batch num 9980: Loss 12.651535987854004\n",
      "Batch num 9990: Loss 10.610152244567871\n",
      "Batch num 10000: Loss 11.5546875\n",
      "Batch num 10010: Loss 11.93862533569336\n",
      "Batch num 10020: Loss 11.138608932495117\n",
      "Batch num 10030: Loss 11.601882934570312\n",
      "Batch num 10040: Loss 10.9766845703125\n",
      "Batch num 10050: Loss 10.44686508178711\n",
      "Batch num 10060: Loss 11.38963508605957\n",
      "Batch num 10070: Loss 9.960777282714844\n",
      "Batch num 10080: Loss 10.982457160949707\n",
      "Batch num 10090: Loss 10.34261417388916\n",
      "Batch num 10100: Loss 10.97506332397461\n",
      "Batch num 10110: Loss 10.822444915771484\n",
      "Batch num 10120: Loss 10.46711540222168\n",
      "Batch num 10130: Loss 10.732437133789062\n",
      "Batch num 10140: Loss 9.403451919555664\n",
      "Batch num 10150: Loss 12.943879127502441\n",
      "Batch num 10160: Loss 10.392335891723633\n",
      "Batch num 10170: Loss 10.43396282196045\n",
      "Batch num 10180: Loss 12.236236572265625\n",
      "Batch num 10190: Loss 11.182415008544922\n",
      "Batch num 10200: Loss 10.802976608276367\n",
      "Batch num 10210: Loss 11.103950500488281\n",
      "Batch num 10220: Loss 11.679031372070312\n",
      "Batch num 10230: Loss 12.326302528381348\n",
      "Batch num 10240: Loss 11.363850593566895\n",
      "Batch num 10250: Loss 12.04909896850586\n",
      "Batch num 10260: Loss 12.364681243896484\n",
      "Batch num 10270: Loss 11.395484924316406\n",
      "Batch num 10280: Loss 10.354772567749023\n",
      "Batch num 10290: Loss 12.158834457397461\n",
      "Batch num 10300: Loss 10.87263298034668\n",
      "Batch num 10310: Loss 11.943120956420898\n",
      "Batch num 10320: Loss 12.522392272949219\n",
      "Batch num 10330: Loss 11.706396102905273\n",
      "Batch num 10340: Loss 11.825246810913086\n",
      "Batch num 10350: Loss 11.350116729736328\n",
      "Batch num 10360: Loss 10.115029335021973\n",
      "Batch num 10370: Loss 11.493562698364258\n",
      "Batch num 10380: Loss 12.771585464477539\n",
      "Batch num 10390: Loss 11.766931533813477\n",
      "Batch num 10400: Loss 11.219633102416992\n",
      "Batch num 10410: Loss 11.07750129699707\n",
      "Batch num 10420: Loss 8.484992980957031\n",
      "Batch num 10430: Loss 9.904378890991211\n",
      "Batch num 10440: Loss 11.779521942138672\n",
      "Batch num 10450: Loss 10.384506225585938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 10460: Loss 12.073395729064941\n",
      "Batch num 10470: Loss 12.153554916381836\n",
      "Batch num 10480: Loss 11.529979705810547\n",
      "Batch num 10490: Loss 12.735363006591797\n",
      "Batch num 10500: Loss 9.886653900146484\n",
      "Batch num 10510: Loss 10.285593032836914\n",
      "Batch num 10520: Loss 11.181818962097168\n",
      "Batch num 10530: Loss 12.831205368041992\n",
      "Batch num 10540: Loss 10.808584213256836\n",
      "Batch num 10550: Loss 10.475578308105469\n",
      "Batch num 10560: Loss 11.30087661743164\n",
      "Batch num 10570: Loss 10.983662605285645\n",
      "Batch num 10580: Loss 11.271774291992188\n",
      "Batch num 10590: Loss 11.35125732421875\n",
      "Batch num 10600: Loss 12.12234115600586\n",
      "Batch num 10610: Loss 10.910028457641602\n",
      "Batch num 10620: Loss 10.427988052368164\n",
      "Batch num 10630: Loss 11.918657302856445\n",
      "Batch num 10640: Loss 11.722230911254883\n",
      "Batch num 10650: Loss 10.75849437713623\n",
      "Batch num 10660: Loss 10.7138671875\n",
      "Batch num 10670: Loss 12.755048751831055\n",
      "Batch num 10680: Loss 9.921062469482422\n",
      "Batch num 10690: Loss 10.595979690551758\n",
      "Batch num 10700: Loss 13.754317283630371\n",
      "Batch num 10710: Loss 11.337995529174805\n",
      "Batch num 10720: Loss 12.049986839294434\n",
      "Batch num 10730: Loss 11.295066833496094\n",
      "Batch num 10740: Loss 10.010400772094727\n",
      "Batch num 10750: Loss 11.405323028564453\n",
      "Batch num 10760: Loss 12.504441261291504\n",
      "Batch num 10770: Loss 11.864007949829102\n",
      "Batch num 10780: Loss 11.363067626953125\n",
      "Batch num 10790: Loss 11.374752044677734\n",
      "Batch num 10800: Loss 12.510761260986328\n",
      "Batch num 10810: Loss 12.402318954467773\n",
      "Batch num 10820: Loss 10.714160919189453\n",
      "Batch num 10830: Loss 11.518501281738281\n",
      "Batch num 10840: Loss 11.681680679321289\n",
      "Batch num 10850: Loss 12.89725399017334\n",
      "Batch num 10860: Loss 10.82004165649414\n",
      "Batch num 10870: Loss 10.773439407348633\n",
      "Batch num 10880: Loss 12.272310256958008\n",
      "Batch num 10890: Loss 11.905546188354492\n",
      "Batch num 10900: Loss 12.205066680908203\n",
      "Batch num 10910: Loss 9.868362426757812\n",
      "Batch num 10920: Loss 11.880134582519531\n",
      "Batch num 10930: Loss 12.046819686889648\n",
      "Batch num 10940: Loss 10.528982162475586\n",
      "Batch num 10950: Loss 10.529144287109375\n",
      "Batch num 10960: Loss 11.729978561401367\n",
      "Batch num 10970: Loss 11.123284339904785\n",
      "Batch num 10980: Loss 11.693439483642578\n",
      "Batch num 10990: Loss 13.230294227600098\n",
      "Batch num 11000: Loss 11.855990409851074\n",
      "Batch num 11010: Loss 12.253938674926758\n",
      "Batch num 11020: Loss 10.755207061767578\n",
      "Batch num 11030: Loss 11.67463207244873\n",
      "Batch num 11040: Loss 10.768842697143555\n",
      "Batch num 11050: Loss 12.944271087646484\n",
      "Batch num 11060: Loss 13.157700538635254\n",
      "Batch num 11070: Loss 10.69016170501709\n",
      "Batch num 11080: Loss 12.420597076416016\n",
      "Batch num 11090: Loss 9.367992401123047\n",
      "Batch num 11100: Loss 11.583179473876953\n",
      "Batch num 11110: Loss 11.238937377929688\n",
      "Batch num 11120: Loss 11.33796501159668\n",
      "Batch num 11130: Loss 11.302560806274414\n",
      "Batch num 11140: Loss 11.801795959472656\n",
      "Batch num 11150: Loss 13.212162017822266\n",
      "Batch num 11160: Loss 12.685333251953125\n",
      "Batch num 11170: Loss 11.484498977661133\n",
      "Batch num 11180: Loss 11.43404483795166\n",
      "Batch num 11190: Loss 10.21987247467041\n",
      "Batch num 11200: Loss 12.370073318481445\n",
      "Batch num 11210: Loss 11.650577545166016\n",
      "Batch num 11220: Loss 13.759237289428711\n",
      "Batch num 11230: Loss 11.254042625427246\n",
      "Batch num 11240: Loss 11.846721649169922\n",
      "Batch num 11250: Loss 9.751537322998047\n",
      "Batch num 11260: Loss 10.01981258392334\n",
      "Batch num 11270: Loss 11.297050476074219\n",
      "Batch num 11280: Loss 12.757884979248047\n",
      "Batch num 11290: Loss 10.501029968261719\n",
      "Batch num 11300: Loss 11.42347526550293\n",
      "Batch num 11310: Loss 12.398757934570312\n",
      "Batch num 11320: Loss 11.9364652633667\n",
      "Batch num 11330: Loss 11.601991653442383\n",
      "Batch num 11340: Loss 12.236761093139648\n",
      "Batch num 11350: Loss 13.162559509277344\n",
      "Batch num 11360: Loss 11.794897079467773\n",
      "Batch num 11370: Loss 12.191398620605469\n",
      "Batch num 11380: Loss 10.764825820922852\n",
      "Batch num 11390: Loss 11.219303131103516\n",
      "Batch num 11400: Loss 14.368739128112793\n",
      "Batch num 11410: Loss 13.088525772094727\n",
      "Batch num 11420: Loss 10.773646354675293\n",
      "Batch num 11430: Loss 11.315597534179688\n",
      "Batch num 11440: Loss 9.982059478759766\n",
      "Batch num 11450: Loss 13.257149696350098\n",
      "Batch num 11460: Loss 11.475687026977539\n",
      "Batch num 11470: Loss 10.214497566223145\n",
      "Batch num 11480: Loss 11.626611709594727\n",
      "Batch num 11490: Loss 11.172883033752441\n",
      "Batch num 11500: Loss 12.414772987365723\n",
      "Batch num 11510: Loss 11.187887191772461\n",
      "Batch num 11520: Loss 11.222044944763184\n",
      "Batch num 11530: Loss 11.259567260742188\n",
      "Batch num 11540: Loss 14.31303596496582\n",
      "Batch num 11550: Loss 9.65990924835205\n",
      "Batch num 11560: Loss 11.699214935302734\n",
      "Batch num 11570: Loss 13.167767524719238\n",
      "Batch num 11580: Loss 10.95340633392334\n",
      "Batch num 11590: Loss 11.522106170654297\n",
      "Batch num 11600: Loss 10.673609733581543\n",
      "Batch num 11610: Loss 9.806276321411133\n",
      "Batch num 11620: Loss 10.835136413574219\n",
      "Batch num 11630: Loss 11.266950607299805\n",
      "Batch num 11640: Loss 8.92715072631836\n",
      "Batch num 11650: Loss 9.193004608154297\n",
      "Batch num 11660: Loss 12.083989143371582\n",
      "Batch num 11670: Loss 9.796612739562988\n",
      "Batch num 11680: Loss 13.132244110107422\n",
      "Batch num 11690: Loss 12.307513236999512\n",
      "Batch num 11700: Loss 12.168113708496094\n",
      "Batch num 11710: Loss 11.804679870605469\n",
      "Batch num 11720: Loss 10.73153305053711\n",
      "Batch num 11730: Loss 9.9678316116333\n",
      "Batch num 11740: Loss 11.025453567504883\n",
      "Batch num 11750: Loss 12.784526824951172\n",
      "Batch num 11760: Loss 11.76681900024414\n",
      "Batch num 11770: Loss 11.648515701293945\n",
      "Batch num 11780: Loss 9.767998695373535\n",
      "Batch num 11790: Loss 11.158714294433594\n",
      "Batch num 11800: Loss 11.301475524902344\n",
      "Batch num 11810: Loss 10.334997177124023\n",
      "Batch num 11820: Loss 12.220322608947754\n",
      "Batch num 11830: Loss 11.407689094543457\n",
      "Batch num 11840: Loss 13.979728698730469\n",
      "Batch num 11850: Loss 12.415826797485352\n",
      "Batch num 11860: Loss 9.643234252929688\n",
      "Batch num 11870: Loss 10.29537296295166\n",
      "Batch num 11880: Loss 12.684730529785156\n",
      "Batch num 11890: Loss 12.278602600097656\n",
      "Batch num 11900: Loss 12.435539245605469\n",
      "Batch num 11910: Loss 12.163448333740234\n",
      "Batch num 11920: Loss 10.312880516052246\n",
      "Batch num 11930: Loss 10.812198638916016\n",
      "Batch num 11940: Loss 10.727014541625977\n",
      "Batch num 11950: Loss 13.404431343078613\n",
      "Batch num 11960: Loss 10.914505958557129\n",
      "Batch num 11970: Loss 10.758455276489258\n",
      "Batch num 11980: Loss 11.094880104064941\n",
      "Batch num 11990: Loss 12.012675285339355\n",
      "Batch num 12000: Loss 11.422163009643555\n",
      "Batch num 12010: Loss 10.272457122802734\n",
      "Batch num 12020: Loss 11.498334884643555\n",
      "Batch num 12030: Loss 10.846909523010254\n",
      "Batch num 12040: Loss 14.058197975158691\n",
      "Batch num 12050: Loss 12.981752395629883\n",
      "Batch num 12060: Loss 11.252971649169922\n",
      "Batch num 12070: Loss 12.806916236877441\n",
      "Batch num 12080: Loss 11.292083740234375\n",
      "Batch num 12090: Loss 11.406024932861328\n",
      "Batch num 12100: Loss 11.520593643188477\n",
      "Batch num 12110: Loss 12.044626235961914\n",
      "Batch num 12120: Loss 12.74117660522461\n",
      "Batch num 12130: Loss 10.831258773803711\n",
      "Batch num 12140: Loss 13.572834014892578\n",
      "Batch num 12150: Loss 11.614749908447266\n",
      "Batch num 12160: Loss 11.270757675170898\n",
      "Batch num 12170: Loss 11.533526420593262\n",
      "Batch num 12180: Loss 10.164518356323242\n",
      "Batch num 12190: Loss 10.570777893066406\n",
      "Batch num 12200: Loss 11.633590698242188\n",
      "Batch num 12210: Loss 11.44395637512207\n",
      "Batch num 12220: Loss 12.303027153015137\n",
      "Batch num 12230: Loss 10.578725814819336\n",
      "Batch num 12240: Loss 11.165020942687988\n",
      "Batch num 12250: Loss 11.621038436889648\n",
      "Batch num 12260: Loss 10.99686336517334\n",
      "Batch num 12270: Loss 12.938678741455078\n",
      "Batch num 12280: Loss 10.66019058227539\n",
      "Batch num 12290: Loss 11.209484100341797\n",
      "Batch num 12300: Loss 11.581438064575195\n",
      "Batch num 12310: Loss 12.660802841186523\n",
      "Batch num 12320: Loss 11.192349433898926\n",
      "Batch num 12330: Loss 11.224309921264648\n",
      "Batch num 12340: Loss 11.282596588134766\n",
      "Batch num 12350: Loss 11.79699420928955\n",
      "Batch num 12360: Loss 11.93088150024414\n",
      "Batch num 12370: Loss 10.965993881225586\n",
      "Batch num 12380: Loss 10.937487602233887\n",
      "Batch num 12390: Loss 11.360204696655273\n",
      "Batch num 12400: Loss 12.04455280303955\n",
      "Batch num 12410: Loss 10.722949981689453\n",
      "Batch num 12420: Loss 10.633363723754883\n",
      "Batch num 12430: Loss 11.744894981384277\n",
      "Batch num 12440: Loss 11.600101470947266\n",
      "Batch num 12450: Loss 11.688329696655273\n",
      "Batch num 12460: Loss 12.174492835998535\n",
      "Batch num 12470: Loss 10.970162391662598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 12480: Loss 12.044109344482422\n",
      "Batch num 12490: Loss 9.679555892944336\n",
      "Batch num 12500: Loss 10.499136924743652\n",
      "Batch num 12510: Loss 10.075279235839844\n",
      "Batch num 12520: Loss 9.989789962768555\n",
      "Batch num 12530: Loss 11.120414733886719\n",
      "Batch num 12540: Loss 13.10830020904541\n",
      "Batch num 12550: Loss 10.332477569580078\n",
      "Batch num 12560: Loss 9.501270294189453\n",
      "Batch num 12570: Loss 9.79444408416748\n",
      "Batch num 12580: Loss 10.072305679321289\n",
      "Batch num 12590: Loss 13.048086166381836\n",
      "Batch num 12600: Loss 13.01939582824707\n",
      "Batch num 12610: Loss 11.56692886352539\n",
      "Batch num 12620: Loss 11.57791805267334\n",
      "Batch num 12630: Loss 11.758646011352539\n",
      "Batch num 12640: Loss 11.011640548706055\n",
      "Batch num 12650: Loss 11.860480308532715\n",
      "Batch num 12660: Loss 10.122438430786133\n",
      "Batch num 12670: Loss 11.436981201171875\n",
      "Batch num 12680: Loss 9.903020858764648\n",
      "Batch num 12690: Loss 9.1346435546875\n",
      "Batch num 12700: Loss 10.555830001831055\n",
      "Batch num 12710: Loss 11.594884872436523\n",
      "Batch num 12720: Loss 11.163681030273438\n",
      "Batch num 12730: Loss 10.126249313354492\n",
      "Batch num 12740: Loss 11.00284481048584\n",
      "Batch num 12750: Loss 11.390335083007812\n",
      "Batch num 12760: Loss 11.40240478515625\n",
      "Batch num 12770: Loss 11.746976852416992\n",
      "Batch num 12780: Loss 10.953473091125488\n",
      "Batch num 12790: Loss 10.984395980834961\n",
      "Batch num 12800: Loss 9.859585762023926\n",
      "Batch num 12810: Loss 10.162068367004395\n",
      "Batch num 12820: Loss 10.563596725463867\n",
      "Batch num 12830: Loss 12.8646821975708\n",
      "Batch num 12840: Loss 13.645025253295898\n",
      "Batch num 12850: Loss 11.988521575927734\n",
      "Batch num 12860: Loss 10.299304962158203\n",
      "Batch num 12870: Loss 12.119892120361328\n",
      "Batch num 12880: Loss 10.515195846557617\n",
      "Batch num 12890: Loss 11.266748428344727\n",
      "Batch num 12900: Loss 10.618066787719727\n",
      "Batch num 12910: Loss 10.497527122497559\n",
      "Batch num 12920: Loss 10.775188446044922\n",
      "Batch num 12930: Loss 10.192240715026855\n",
      "Batch num 12940: Loss 10.045663833618164\n",
      "Batch num 12950: Loss 8.754271507263184\n",
      "Batch num 12960: Loss 9.870523452758789\n",
      "Batch num 12970: Loss 12.150915145874023\n",
      "Batch num 12980: Loss 12.108257293701172\n",
      "Batch num 12990: Loss 10.46855354309082\n",
      "Batch num 13000: Loss 12.908346176147461\n",
      "Batch num 13010: Loss 11.075767517089844\n",
      "Batch num 13020: Loss 11.90548324584961\n",
      "Batch num 13030: Loss 11.718759536743164\n",
      "Batch num 13040: Loss 11.530878067016602\n",
      "Batch num 13050: Loss 13.501652717590332\n",
      "Batch num 13060: Loss 10.758709907531738\n",
      "Batch num 13070: Loss 10.716341018676758\n",
      "Batch num 13080: Loss 11.474691390991211\n",
      "Batch num 13090: Loss 11.111396789550781\n",
      "Batch num 13100: Loss 6.327919006347656\n",
      "Batch num 13110: Loss 10.559915542602539\n",
      "Batch num 13120: Loss 10.678298950195312\n",
      "Batch num 13130: Loss 11.258302688598633\n",
      "Batch num 13140: Loss 11.39927864074707\n",
      "Batch num 13150: Loss 9.804333686828613\n",
      "Batch num 13160: Loss 11.411861419677734\n",
      "Batch num 13170: Loss 9.427010536193848\n",
      "Batch num 13180: Loss 11.16298770904541\n",
      "Batch num 13190: Loss 12.268722534179688\n",
      "Batch num 13200: Loss 8.779533386230469\n",
      "Batch num 13210: Loss 11.64645004272461\n",
      "Batch num 13220: Loss 12.251562118530273\n",
      "Batch num 13230: Loss 10.190193176269531\n",
      "Batch num 13240: Loss 11.033852577209473\n",
      "Batch num 13250: Loss 11.55578899383545\n",
      "Batch num 13260: Loss 10.481866836547852\n",
      "Batch num 13270: Loss 11.969018936157227\n",
      "Batch num 13280: Loss 12.188758850097656\n",
      "Batch num 13290: Loss 11.640884399414062\n",
      "Batch num 13300: Loss 12.346067428588867\n",
      "Batch num 13310: Loss 9.398052215576172\n",
      "Batch num 13320: Loss 10.7630615234375\n",
      "Batch num 13330: Loss 10.257328033447266\n",
      "Batch num 13340: Loss 10.512508392333984\n",
      "Batch num 13350: Loss 12.850624084472656\n",
      "Batch num 13360: Loss 11.412071228027344\n",
      "Batch num 13370: Loss 12.120068550109863\n",
      "Batch num 13380: Loss 10.624096870422363\n",
      "Batch num 13390: Loss 9.004279136657715\n",
      "Batch num 13400: Loss 11.329652786254883\n",
      "Batch num 13410: Loss 9.441646575927734\n",
      "Batch num 13420: Loss 10.6370849609375\n",
      "Batch num 13430: Loss 11.634808540344238\n",
      "Batch num 13440: Loss 11.898738861083984\n",
      "Batch num 13450: Loss 11.945680618286133\n",
      "Batch num 13460: Loss 9.362030982971191\n",
      "Batch num 13470: Loss 9.960346221923828\n",
      "Batch num 13480: Loss 10.286136627197266\n",
      "Batch num 13490: Loss 12.417879104614258\n",
      "Batch num 13500: Loss 10.545079231262207\n",
      "Batch num 13510: Loss 11.611457824707031\n",
      "Batch num 13520: Loss 9.692861557006836\n",
      "Batch num 13530: Loss 11.9878511428833\n",
      "Batch num 13540: Loss 9.37802505493164\n",
      "Batch num 13550: Loss 11.066740036010742\n",
      "Batch num 13560: Loss 12.07665729522705\n",
      "Batch num 13570: Loss 11.273560523986816\n",
      "Batch num 13580: Loss 10.869815826416016\n",
      "Batch num 13590: Loss 12.17898941040039\n",
      "Batch num 13600: Loss 10.268987655639648\n",
      "Batch num 13610: Loss 13.744062423706055\n",
      "Batch num 13620: Loss 11.42819595336914\n",
      "Batch num 13630: Loss 10.020196914672852\n",
      "Batch num 13640: Loss 10.833744049072266\n",
      "Batch num 13650: Loss 12.025714874267578\n",
      "Batch num 13660: Loss 13.134233474731445\n",
      "Batch num 13670: Loss 11.04289436340332\n",
      "Batch num 13680: Loss 9.835334777832031\n",
      "Batch num 13690: Loss 10.370892524719238\n",
      "Batch num 13700: Loss 12.09781265258789\n",
      "Batch num 13710: Loss 10.47990608215332\n",
      "Batch num 13720: Loss 11.726177215576172\n",
      "Batch num 13730: Loss 11.684197425842285\n",
      "Batch num 13740: Loss 11.563518524169922\n",
      "Batch num 13750: Loss 11.558691024780273\n",
      "Batch num 13760: Loss 11.578950881958008\n",
      "Batch num 13770: Loss 11.179891586303711\n",
      "Batch num 13780: Loss 10.4601469039917\n",
      "Batch num 13790: Loss 13.318058013916016\n",
      "Batch num 13800: Loss 11.323785781860352\n",
      "Batch num 13810: Loss 10.83310317993164\n",
      "Batch num 13820: Loss 11.995353698730469\n",
      "Batch num 13830: Loss 12.925212860107422\n",
      "Batch num 13840: Loss 9.785791397094727\n",
      "Batch num 13850: Loss 12.872760772705078\n",
      "Batch num 13860: Loss 11.506404876708984\n",
      "Batch num 13870: Loss 10.97316837310791\n",
      "Batch num 13880: Loss 11.573722839355469\n",
      "Batch num 13890: Loss 11.830490112304688\n",
      "Batch num 13900: Loss 12.58487319946289\n",
      "Batch num 13910: Loss 11.474027633666992\n",
      "Batch num 13920: Loss 10.918498992919922\n",
      "Batch num 13930: Loss 10.157821655273438\n",
      "Batch num 13940: Loss 10.415825843811035\n",
      "Batch num 13950: Loss 11.377641677856445\n",
      "Batch num 13960: Loss 11.86104965209961\n",
      "Batch num 13970: Loss 11.999473571777344\n",
      "Batch num 13980: Loss 12.097721099853516\n",
      "Batch num 13990: Loss 10.858234405517578\n",
      "Batch num 14000: Loss 10.757842063903809\n",
      "Batch num 14010: Loss 13.232685089111328\n",
      "Batch num 14020: Loss 10.719411849975586\n",
      "Batch num 14030: Loss 11.38003158569336\n",
      "Batch num 14040: Loss 9.981985092163086\n",
      "Batch num 14050: Loss 11.861830711364746\n",
      "Batch num 14060: Loss 10.096420288085938\n",
      "Batch num 14070: Loss 9.384571075439453\n",
      "Batch num 14080: Loss 11.825892448425293\n",
      "Batch num 14090: Loss 9.7767333984375\n",
      "Batch num 14100: Loss 11.983577728271484\n",
      "Batch num 14110: Loss 13.04995346069336\n",
      "Batch num 14120: Loss 10.113395690917969\n",
      "Batch num 14130: Loss 11.421703338623047\n",
      "Batch num 14140: Loss 12.686985969543457\n",
      "Batch num 14150: Loss 10.306934356689453\n",
      "Batch num 14160: Loss 10.956932067871094\n",
      "Batch num 14170: Loss 11.333024978637695\n",
      "Batch num 14180: Loss 10.58164119720459\n",
      "Batch num 14190: Loss 12.788352966308594\n",
      "Batch num 14200: Loss 11.245397567749023\n",
      "Batch num 14210: Loss 11.628095626831055\n",
      "Batch num 14220: Loss 12.322673797607422\n",
      "Batch num 14230: Loss 11.463395118713379\n",
      "Batch num 14240: Loss 11.17392349243164\n",
      "Batch num 14250: Loss 10.261957168579102\n",
      "Batch num 14260: Loss 11.01255989074707\n",
      "Batch num 14270: Loss 11.450942993164062\n",
      "Batch num 14280: Loss 12.802631378173828\n",
      "Batch num 14290: Loss 11.635431289672852\n",
      "Batch num 14300: Loss 9.997539520263672\n",
      "Batch num 14310: Loss 10.847393035888672\n",
      "Batch num 14320: Loss 12.09419059753418\n",
      "Batch num 14330: Loss 10.93692684173584\n",
      "Batch num 14340: Loss 11.940309524536133\n",
      "Batch num 14350: Loss 10.166756629943848\n",
      "Batch num 14360: Loss 11.190168380737305\n",
      "Batch num 14370: Loss 9.744755744934082\n",
      "Batch num 14380: Loss 8.485641479492188\n",
      "Batch num 14390: Loss 11.504132270812988\n",
      "Batch num 14400: Loss 11.971090316772461\n",
      "Batch num 14410: Loss 12.52219009399414\n",
      "Batch num 14420: Loss 12.354564666748047\n",
      "Batch num 14430: Loss 8.860250473022461\n",
      "Batch num 14440: Loss 10.983903884887695\n",
      "Batch num 14450: Loss 11.408906936645508\n",
      "Batch num 14460: Loss 12.481667518615723\n",
      "Batch num 14470: Loss 10.743612289428711\n",
      "Batch num 14480: Loss 12.08088493347168\n",
      "Batch num 14490: Loss 11.99979019165039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 14500: Loss 10.531059265136719\n",
      "Batch num 14510: Loss 12.52972412109375\n",
      "Batch num 14520: Loss 10.58328914642334\n",
      "Batch num 14530: Loss 12.168708801269531\n",
      "Batch num 14540: Loss 10.61160659790039\n",
      "Batch num 14550: Loss 9.130583763122559\n",
      "Batch num 14560: Loss 11.298576354980469\n",
      "Batch num 14570: Loss 10.495771408081055\n",
      "Batch num 14580: Loss 12.02033805847168\n",
      "Batch num 14590: Loss 9.761374473571777\n",
      "Batch num 14600: Loss 10.262524604797363\n",
      "Batch num 14610: Loss 12.392995834350586\n",
      "Batch num 14620: Loss 12.135869979858398\n",
      "Batch num 14630: Loss 12.516550064086914\n",
      "Batch num 14640: Loss 11.138627052307129\n",
      "Batch num 14650: Loss 10.737394332885742\n",
      "Batch num 14660: Loss 11.963736534118652\n",
      "Batch num 14670: Loss 10.230920791625977\n",
      "Batch num 14680: Loss 12.182991027832031\n",
      "Batch num 14690: Loss 9.43471908569336\n",
      "Batch num 14700: Loss 10.30388069152832\n",
      "Batch num 14710: Loss 9.68006706237793\n",
      "Batch num 14720: Loss 10.843214988708496\n",
      "Batch num 14730: Loss 10.35739803314209\n",
      "Batch num 14740: Loss 12.085565567016602\n",
      "Batch num 14750: Loss 10.514904022216797\n",
      "Batch num 14760: Loss 11.012099266052246\n",
      "Batch num 14770: Loss 11.9239501953125\n",
      "Batch num 14780: Loss 11.198101997375488\n",
      "Batch num 14790: Loss 9.46212387084961\n",
      "Batch num 14800: Loss 10.654256820678711\n",
      "Batch num 14810: Loss 10.524589538574219\n",
      "Batch num 14820: Loss 11.225915908813477\n",
      "Batch num 14830: Loss 12.18084716796875\n",
      "Batch num 14840: Loss 11.865707397460938\n",
      "Batch num 14850: Loss 11.10572624206543\n",
      "Batch num 14860: Loss 10.082880020141602\n",
      "Batch num 14870: Loss 10.328224182128906\n",
      "Batch num 14880: Loss 10.511613845825195\n",
      "Batch num 14890: Loss 12.015617370605469\n",
      "Batch num 14900: Loss 10.308947563171387\n",
      "Batch num 14910: Loss 11.605886459350586\n",
      "Batch num 14920: Loss 10.832487106323242\n",
      "Batch num 14930: Loss 11.114835739135742\n",
      "Batch num 14940: Loss 10.454975128173828\n",
      "Batch num 14950: Loss 9.761533737182617\n",
      "Batch num 14960: Loss 11.248095512390137\n",
      "Batch num 14970: Loss 10.224788665771484\n",
      "Batch num 14980: Loss 10.709714889526367\n",
      "Batch num 14990: Loss 12.393678665161133\n",
      "Batch num 15000: Loss 11.36739730834961\n",
      "Batch num 15010: Loss 13.524944305419922\n",
      "Batch num 15020: Loss 10.98244571685791\n",
      "Batch num 15030: Loss 11.227630615234375\n",
      "Batch num 15040: Loss 11.435538291931152\n",
      "Batch num 15050: Loss 11.794737815856934\n",
      "Batch num 15060: Loss 10.902576446533203\n",
      "Batch num 15070: Loss 11.071365356445312\n",
      "Batch num 15080: Loss 9.34324836730957\n",
      "Batch num 15090: Loss 11.274200439453125\n",
      "Batch num 15100: Loss 11.587345123291016\n",
      "Batch num 15110: Loss 9.646951675415039\n",
      "Batch num 15120: Loss 10.387749671936035\n",
      "Batch num 15130: Loss 10.433399200439453\n",
      "Batch num 15140: Loss 9.450323104858398\n",
      "Batch num 15150: Loss 9.567012786865234\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, val_dataloader, regularize=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do for final paper: \n",
    "* Add Tensorboard stuff \n",
    "* Print out accuracy for the encoder\n",
    "* Create Perplexity evaluation metric\n",
    "* Run example with discriminator over both the attention and hidden\n",
    "* Factorize Code into util and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
