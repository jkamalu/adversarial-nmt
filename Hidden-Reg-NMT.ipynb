{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a basic training loop - not using attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir is /juice/scr/scr110/scr/nlp/mtl_bert/unidirectional-NMT/BERT\n"
     ]
    }
   ],
   "source": [
    "''' Changing directories '''\n",
    "import os \n",
    "if 'BERT' not in os.getcwd():\n",
    "    os.chdir('BERT')\n",
    "print(\"Current working dir is {}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "import pyaml\n",
    "import onmt\n",
    "import math\n",
    "import torch\n",
    "from dataset import TextDataset, Collator\n",
    "from encoder import Encoder \n",
    "from decoder import Decoder\n",
    "from discriminator import Discriminator\n",
    "from lib.huggingface.transformers import RobertaTokenizer, CamembertTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.path.dirname(os.getcwd()), \"config\", \"config.yml\"), \"r\") as fd:\n",
    "    config = pyaml.yaml.load(fd, Loader=pyaml.yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_token_length = config[\"maxlen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_fr = CamembertTokenizer.from_pretrained('camembert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(maxlen=sentence_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "1704 examples with length < 2 removed.\n",
      "42487 examples with length > 50 removed.\n",
      "0\n",
      "36 examples with length < 2 removed.\n",
      "1037 examples with length > 50 removed.\n"
     ]
    }
   ],
   "source": [
    "text_dataset_train = TextDataset(\"data/europarl-v7/\", tokenizer_en, tokenizer_fr, training=True, maxlen=sentence_token_length)\n",
    "text_dataset_val = TextDataset(\"data/europarl-v7/\",  tokenizer_en, tokenizer_fr, training=False, maxlen=sentence_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(text_dataset_train, **config[\"data_loader\"], collate_fn=collator)\n",
    "val_dataloader = DataLoader(text_dataset_val, **config[\"data_loader\"], collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the encoding and decoding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA!\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"Using CUDA!\")\n",
    "else:\n",
    "    print(\"Using CPU - Played yourself!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del encoder_en\n",
    "    del encoder_fr\n",
    "except:\n",
    "    pass \n",
    "encoder_en = Encoder(\"english\").to(device=device)\n",
    "encoder_fr = Encoder(\"french\").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same\n",
    "word_padding_idx_en = encoder_en._modules['model'].embeddings.padding_idx\n",
    "word_padding_idx_fr = encoder_fr._modules['model'].embeddings.padding_idx\n",
    "\n",
    "# en > fr\n",
    "word_vocab_size_en = encoder_en._modules['model'].embeddings.word_embeddings.num_embeddings\n",
    "word_vocab_size_fr = encoder_fr._modules['model'].embeddings.word_embeddings.num_embeddings\n",
    "\n",
    "# same\n",
    "word_vec_size_en = encoder_en._modules['model'].embeddings.word_embeddings.embedding_dim\n",
    "word_vec_size_fr = encoder_fr._modules['model'].embeddings.word_embeddings.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_en = onmt.modules.embeddings.Embeddings(\n",
    "    word_vec_size_en, \n",
    "    word_vocab_size_en, \n",
    "    word_padding_idx_en, \n",
    "    position_encoding=True\n",
    ").to(device=device)\n",
    "\n",
    "embeddings_fr = onmt.modules.embeddings.Embeddings(\n",
    "    word_vec_size_fr, \n",
    "    word_vocab_size_fr, \n",
    "    word_padding_idx_fr, \n",
    "    position_encoding=True\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_en = Decoder(**config[\"small_transformer\"], embeddings=embeddings_en).to(device=device)\n",
    "decoder_fr = Decoder(**config[\"small_transformer\"], embeddings=embeddings_fr).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection: 768 -> 1\n",
    "discriminator = Discriminator(config[\"small_transformer\"]['d_model'], 1).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict):\n",
    "    '''Standard machine translation cross entropy loss'''\n",
    "    ce_loss = torch.nn.CrossEntropyLoss(ignore_index = 1) #ignoring padding tokens\n",
    "    \n",
    "    predictions_fr = torch.argmax(french_predict, dim=2)\n",
    "    \n",
    "    loss_english_to_french = ce_loss(english_predict.transpose(1,2), english_gt)\n",
    "    loss_french_to_english = ce_loss(french_predict.transpose(1,2), french_gt)\n",
    "    return (loss_english_to_french + loss_french_to_english, loss_english_to_french, loss_french_to_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_single_regularization(english_gt, french_gt, english_predict, french_predict,\n",
    "                                discriminator_gt, discriminator_predict):\n",
    "    '''Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs'''\n",
    "    ce_term, loss_english_to_french, loss_french_to_english = loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict)\n",
    "    \n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    regularizing_term = bce_loss(discriminator_predict, discriminator_gt)\n",
    "    \n",
    "    return (ce_term + regularizing_term, loss_english_to_french, loss_french_to_english, regularizing_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_multi_regularization(english_gt, french_gt, english_predict, french_predict,\n",
    "                                discriminator_gt_1, discriminator_predict_1,\n",
    "                                discriminator_gt_2, discriminator_predict_2,):\n",
    "    '''Adversarial Loss: standard loss with binary cross entropy on top of the discriminator outputs'''\n",
    "    ce_term = loss_fn_no_regularization(english_gt, french_gt, english_predict, french_predict)\n",
    "    \n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    regularizing_term_1 = bce_loss(discriminator_predict_1, discriminator_gt_1)\n",
    "    regularizing_term_2 = bce_loss(discriminator_predict_2, discriminator_gt_2)\n",
    "    \n",
    "    return ce_term + regularizing_term_1 + regularizing_term_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and defining evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(prediction, gt):\n",
    "    '''Evaluate ground percent exact match '''\n",
    "    mask = gt != 1\n",
    "    return torch.sum((prediction == gt) * mask).item()/torch.sum(mask).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining optimizer and hooks if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(regularize=\"None\"):\n",
    "    params = list(encoder_en.parameters()) + list(encoder_fr.parameters()) +\\\n",
    "         list(decoder_fr.parameters()) + list(decoder_en.parameters())\n",
    "\n",
    "    if (regularize == \"hidden_state\"):\n",
    "        params += list(discriminator.parameters())\n",
    "    \n",
    "    if (regularize == \"attention\"):\n",
    "        params += list(attn_discriminator.parameters())\n",
    "\n",
    "    return torch.optim.Adam(params)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(regularize=\"hidden_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining primary training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import write_to_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data_iter, val_data_iter, regularize=\"None\"): \n",
    "    ''' \n",
    "    Train the encoding and decoding models. User needs to pass in a valid iterator over the data,\n",
    "    and also specify a type of adversarial regularization. regularize = [\"hidden_state\", \"attention\"]\n",
    "    '''\n",
    "    writer = SummaryWriter(\"runs/regularize_hidden\")\n",
    "                                                   \n",
    "    for batch_num, batch in enumerate(train_data_iter):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # TODO: generalize this to multiple discriminators\n",
    "        if (regularize != \"None\"):\n",
    "            if (batch_num % 2 == 0):\n",
    "                for param in encoder_en.parameters():\n",
    "                    param.requires_grad = False\n",
    "                for param in encoder_fr.parameters():\n",
    "                    param.requires_grad = False    \n",
    "                for param in discriminator.parameters():\n",
    "                    param.requires_grad = True \n",
    "            else:\n",
    "                for param in encoder_en.parameters():\n",
    "                    param.requires_grad = True\n",
    "                for param in encoder_fr.parameters():\n",
    "                    param.requires_grad = True    \n",
    "                for param in discriminator.parameters():\n",
    "                    param.requires_grad = False \n",
    "\n",
    "        # Reading in input and moving to device\n",
    "\n",
    "        english_batch, french_batch = batch \n",
    "        (english_sentences, english_sentences_no_eos, english_sentences_lengths) = english_batch\n",
    "        (french_sentences, french_sentences_no_eos, french_sentences_lengths) = french_batch\n",
    "\n",
    "        english_sentences = english_sentences.to(device=device)\n",
    "        english_sentences_no_eos = english_sentences_no_eos.to(device=device)\n",
    "        english_sentences_lengths = english_sentences_lengths.to(device=device)\n",
    "        french_sentences = french_sentences.to(device=device)\n",
    "        french_sentences_no_eos = french_sentences_no_eos.to(device=device)\n",
    "        french_sentences_lengths = french_sentences_lengths.to(device=device)\n",
    "\n",
    "        # Encoding - Decoding for English -> French\n",
    "        encoder_outputs_en = encoder_en(english_sentences)\n",
    "        decoder_fr.init_state(english_sentences.unsqueeze(2).transpose(0,1), None, None)\n",
    "        dec_outs_fr, _ = decoder_fr(french_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_en[0].transpose(0,1), memory_lengths=english_sentences_lengths)\n",
    "\n",
    "        # Encoding - Decoding for French -> English\n",
    "        encoder_outputs_fr = encoder_fr(french_sentences)\n",
    "        decoder_en.init_state(french_sentences.unsqueeze(2).transpose(0,1), None, None) \n",
    "        dec_outs_en, _ = decoder_en(english_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_fr[0].transpose(0,1), memory_lengths=french_sentences_lengths)\n",
    "\n",
    "        if (regularize == \"attention\"):\n",
    "            # extracting the attention scores from the datasets; using 7th attention head\n",
    "            # as suggested by Clark et al, 2019 \n",
    "            english_attention = extract_attention_scores(_hooks_english)[6] \n",
    "            french_attention = extract_attention_scores(_hooks_french)[6]\n",
    "            batch_size = english_attention.shape[0]\n",
    "            english_attention_reshaped = english_attention.view(batch_size, -1)\n",
    "            french_attention_reshaped = french_attention.view(batch_size, -1)\n",
    "            \n",
    "            discriminator_outputs_en = attn_discriminator(english_attention_reshaped)\n",
    "            discriminator_outputs_fr = attn_discriminator(french_attention_reshaped)\n",
    "            discriminator_outputs_cat = torch.cat((discriminator_outputs_en, discriminator_outputs_fr))\n",
    "            if (batch_num % 2 == 0):\n",
    "                discriminator_labels = torch.tensor([1.0]*batch_size + [0.0]*batch_size)\n",
    "            else:\n",
    "                discriminator_labels = torch.tensor([0.0]*batch_size + [1.0]*batch_size)\n",
    "            discriminator_labels = discriminator_labels.unsqueeze(1).to(device=device)\n",
    "\n",
    "            \n",
    "            all_losses = loss_fn_single_regularization(english_sentences[:, 1:],\n",
    "                                               french_sentences[:, 1:],\n",
    "                                               dec_outs_en,\n",
    "                                               dec_outs_fr,\n",
    "                                               discriminator_labels,\n",
    "                                               discriminator_outputs_cat,\n",
    "                                              )\n",
    "            \n",
    "            (loss, loss_english_to_french, loss_french_to_english, regularizing_term) = all_losses\n",
    "            \n",
    "        elif (regularize == \"hidden_state\"):\n",
    "            # using the pooled outputs of the encoders for regularizing \n",
    "            discriminator_outputs_en = discriminator(encoder_outputs_en[1])\n",
    "            discriminator_outputs_fr = discriminator(encoder_outputs_fr[1])\n",
    "            discriminator_outputs_cat = torch.cat((discriminator_outputs_en, discriminator_outputs_fr))\n",
    "            batch_size = discriminator_outputs_en.shape[0]\n",
    "            \n",
    "            if (batch_num % 2 == 0):\n",
    "                discriminator_labels = torch.tensor([1.0]*batch_size + [0.0]*batch_size)\n",
    "            else:\n",
    "                discriminator_labels = torch.tensor([0.0]*batch_size + [1.0]*batch_size)\n",
    "            discriminator_labels = discriminator_labels.unsqueeze(1).to(device=device)\n",
    "\n",
    "            all_losses = loss_fn_single_regularization(english_sentences[:, 1:],\n",
    "                                               french_sentences[:, 1:],\n",
    "                                               dec_outs_en,\n",
    "                                               dec_outs_fr,\n",
    "                                               discriminator_labels,\n",
    "                                               discriminator_outputs_cat,\n",
    "                                              )\n",
    "            (loss, loss_english_to_french, loss_french_to_english, regularizing_term) = all_losses\n",
    "        else:\n",
    "            loss = loss_fn_no_regularization(english_sentences[:, 1:],\n",
    "                                           french_sentences[:, 1:],\n",
    "                                           dec_outs_en,\n",
    "                                           dec_outs_fr,\n",
    "                                          )\n",
    "            \n",
    "        # must be put here to avoid name claim by val loop\n",
    "        if (batch_num % 10 == 0):\n",
    "            print(\"Batch num {}: Loss {}\".format(batch_num, loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                                                   \n",
    "        write_to_tensorboard(\"CCE\", {\"en-fr\": loss_english_to_french.item(), \"fr-en\":loss_french_to_english.item()}, training=True, step=batch_num, writer=writer)\n",
    "        write_to_tensorboard(\"BCE\", {\"attention-regularizing\": regularizing_term.item()}, training=True, step=batch_num, writer=writer)\n",
    "\n",
    "    \n",
    "        # Running validation script  \n",
    "        if (batch_num > 0 and batch_num % 100 == 0):\n",
    "            with torch.no_grad():\n",
    "                _blue_scores_en_fr = []\n",
    "                _exact_matches_en_fr = []\n",
    "                _blue_scores_fr_en = []\n",
    "                _exact_matches_fr_en = []\n",
    "                for batch_num, batch in enumerate(val_data_iter):\n",
    "                    \n",
    "                    if (batch_num == 25):\n",
    "                        break\n",
    "                    \n",
    "                    # Reading in input and moving to device\n",
    "                    english_batch, french_batch = batch \n",
    "                    (english_sentences, english_sentences_no_eos, english_sentences_lengths) = english_batch\n",
    "                    (french_sentences, french_sentences_no_eos, french_sentences_lengths) = french_batch\n",
    "\n",
    "                    english_sentences = english_sentences.to(device=device)\n",
    "                    english_sentences_no_eos = english_sentences_no_eos.to(device=device)\n",
    "                    english_sentences_lengths = english_sentences_lengths.to(device=device)\n",
    "                    french_sentences = french_sentences.to(device=device)\n",
    "                    french_sentences_no_eos = french_sentences_no_eos.to(device=device)\n",
    "                    french_sentences_lengths = french_sentences_lengths.to(device=device)\n",
    "\n",
    "                    # Encoding - Decoding for English -> French\n",
    "                    encoder_outputs_en = encoder_en(english_sentences)\n",
    "                    decoder_fr.init_state(english_sentences.unsqueeze(2).transpose(0,1), None, None)\n",
    "                    dec_outs_fr, _ = decoder_fr(french_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_en[0].transpose(0,1), memory_lengths=english_sentences_lengths)\n",
    "                    \n",
    "                    # Encoding - Decoding for French -> English\n",
    "                    encoder_outputs_fr = encoder_fr(french_sentences)\n",
    "                    decoder_en.init_state(french_sentences.unsqueeze(2).transpose(0,1), None, None) \n",
    "                    dec_outs_en, _ = decoder_en(english_sentences_no_eos.unsqueeze(2).transpose(0,1), encoder_outputs_fr[0].transpose(0,1), memory_lengths=french_sentences_lengths)\n",
    "\n",
    "                    \n",
    "                    # Calculate BLUE Scores, EM and Perplexity\n",
    "                    predictions_fr = torch.argmax(dec_outs_fr, dim=2)\n",
    "                    predictions_en = torch.argmax(dec_outs_en, dim=2)\n",
    "                    \n",
    "                    for idx in range(french_sentences.shape[0]):\n",
    "                        detokenized_french_gt = tokenizer_fr.convert_tokens_to_string(french_sentences[idx,1:].tolist())\n",
    "                        detokenized_french_pred = tokenizer_fr.convert_tokens_to_string(predictions_fr[idx].tolist())\n",
    "                        \n",
    "                        _blue_score_en_fr = sentence_bleu(detokenized_french_gt, detokenized_french_pred)\n",
    "                        _blue_scores_en_fr.append(_blue_score_en_fr)\n",
    "                        \n",
    "                    for idx in range(english_sentences.shape[0]):\n",
    "           \n",
    "                        detokenized_english_gt = tokenizer_en.decode(english_sentences[idx,1:].tolist())\n",
    "                        detokenized_english_pred = tokenizer_en.decode(predictions_en[idx].tolist())\n",
    "                            \n",
    "                        _blue_score_fr_en = sentence_bleu(detokenized_english_gt, detokenized_english_pred)\n",
    "                        _blue_scores_fr_en.append(_blue_score_fr_en)\n",
    "                        \n",
    "                    _exact_match_en_fr = exact_match(predictions_fr, french_sentences[:,1:])\n",
    "                    _exact_matches_en_fr.append(_exact_match_en_fr)\n",
    "                    \n",
    "                    _exact_match_fr_en = exact_match(predictions_en, english_sentences[:,1:])\n",
    "                    _exact_matches_fr_en.append(_exact_match_fr_en)\n",
    "                    \n",
    "                                                   \n",
    "                avg_bleu_en_fr = sum(_blue_scores_en_fr)/len(_blue_scores_en_fr)\n",
    "                avg_bleu_fr_en = sum(_blue_scores_fr_en)/len(_blue_scores_fr_en)\n",
    "                avg_em_en_fr = sum(_exact_matches_en_fr)/len(_exact_matches_en_fr)\n",
    "                avg_em_fr_en = sum(_exact_matches_fr_en)/len(_exact_matches_fr_en)\n",
    "                                                   \n",
    "                write_to_tensorboard(\"BLEU\", {\"en-fr\": avg_bleu_en_fr, \"fr-en\":avg_bleu_fr_en}, training=False, step=batch_num, writer=writer)\n",
    "                write_to_tensorboard(\"EM\", {\"en-fr\": avg_em_en_fr, \"fr-en\":avg_em_fr_en}, training=False, step=batch_num, writer=writer)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 0: Loss 1527.726318359375\n",
      "Batch num 10: Loss 312.41156005859375\n",
      "Batch num 20: Loss 168.8173370361328\n",
      "Batch num 30: Loss 126.66352081298828\n",
      "Batch num 40: Loss 133.12794494628906\n",
      "Batch num 50: Loss 106.6199951171875\n",
      "Batch num 60: Loss 94.66155242919922\n",
      "Batch num 70: Loss 80.3686294555664\n",
      "Batch num 80: Loss 61.28822708129883\n",
      "Batch num 90: Loss 56.556060791015625\n",
      "Batch num 100: Loss 52.40564727783203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/mtl_bert_environment/lib/python3.5/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/u/nlp/anaconda/main/anaconda3/envs/mtl_bert_environment/lib/python3.5/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/u/nlp/anaconda/main/anaconda3/envs/mtl_bert_environment/lib/python3.5/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 110: Loss 56.741024017333984\n",
      "Batch num 120: Loss 42.57631301879883\n",
      "Batch num 130: Loss 54.447750091552734\n",
      "Batch num 140: Loss 55.74707794189453\n",
      "Batch num 150: Loss 46.81306457519531\n",
      "Batch num 160: Loss 52.101295471191406\n",
      "Batch num 170: Loss 58.495052337646484\n",
      "Batch num 180: Loss 41.351593017578125\n",
      "Batch num 190: Loss 42.04888916015625\n",
      "Batch num 200: Loss 39.13063430786133\n",
      "Batch num 210: Loss 35.858245849609375\n",
      "Batch num 220: Loss 38.01784133911133\n",
      "Batch num 230: Loss 32.590301513671875\n",
      "Batch num 240: Loss 33.58393859863281\n",
      "Batch num 250: Loss 33.45176315307617\n",
      "Batch num 260: Loss 35.55852127075195\n",
      "Batch num 270: Loss 70.44762420654297\n",
      "Batch num 280: Loss 60.3500862121582\n",
      "Batch num 290: Loss 60.22654724121094\n",
      "Batch num 300: Loss 58.03997802734375\n",
      "Batch num 310: Loss 74.3868408203125\n",
      "Batch num 320: Loss 48.261207580566406\n",
      "Batch num 330: Loss 30.621891021728516\n",
      "Batch num 340: Loss 30.636999130249023\n",
      "Batch num 350: Loss 33.46067428588867\n",
      "Batch num 360: Loss 32.2385139465332\n",
      "Batch num 370: Loss 31.845062255859375\n",
      "Batch num 380: Loss 49.981727600097656\n",
      "Batch num 390: Loss 28.246286392211914\n",
      "Batch num 400: Loss 36.30032730102539\n",
      "Batch num 410: Loss 28.86387825012207\n",
      "Batch num 420: Loss 30.025875091552734\n",
      "Batch num 430: Loss 30.23914909362793\n",
      "Batch num 440: Loss 56.34006881713867\n",
      "Batch num 450: Loss 43.94021987915039\n",
      "Batch num 460: Loss 31.073089599609375\n",
      "Batch num 470: Loss 39.531673431396484\n",
      "Batch num 480: Loss 25.106182098388672\n",
      "Batch num 490: Loss 46.928436279296875\n",
      "Batch num 500: Loss 43.12566375732422\n",
      "Batch num 510: Loss 58.4229736328125\n",
      "Batch num 520: Loss 37.617488861083984\n",
      "Batch num 530: Loss 28.20722198486328\n",
      "Batch num 540: Loss 21.16611099243164\n",
      "Batch num 550: Loss 30.490108489990234\n",
      "Batch num 560: Loss 24.46024513244629\n",
      "Batch num 570: Loss 24.037511825561523\n",
      "Batch num 580: Loss 29.753644943237305\n",
      "Batch num 590: Loss 27.9915714263916\n",
      "Batch num 600: Loss 25.604028701782227\n",
      "Batch num 610: Loss 25.466625213623047\n",
      "Batch num 620: Loss 25.969209671020508\n",
      "Batch num 630: Loss 27.839223861694336\n",
      "Batch num 640: Loss 23.044109344482422\n",
      "Batch num 650: Loss 23.037912368774414\n",
      "Batch num 660: Loss 24.329086303710938\n",
      "Batch num 670: Loss 24.808570861816406\n",
      "Batch num 680: Loss 25.1697998046875\n",
      "Batch num 690: Loss 26.878170013427734\n",
      "Batch num 700: Loss 22.11865997314453\n",
      "Batch num 710: Loss 20.763463973999023\n",
      "Batch num 720: Loss 22.692718505859375\n",
      "Batch num 730: Loss 23.726186752319336\n",
      "Batch num 740: Loss 20.98401641845703\n",
      "Batch num 750: Loss 25.67523193359375\n",
      "Batch num 760: Loss 21.076126098632812\n",
      "Batch num 770: Loss 26.200759887695312\n",
      "Batch num 780: Loss 19.3143367767334\n",
      "Batch num 790: Loss 27.699970245361328\n",
      "Batch num 800: Loss 22.996496200561523\n",
      "Batch num 810: Loss 26.15166664123535\n",
      "Batch num 820: Loss 25.079986572265625\n",
      "Batch num 830: Loss 20.412248611450195\n",
      "Batch num 840: Loss 21.0384521484375\n",
      "Batch num 850: Loss 21.666534423828125\n",
      "Batch num 860: Loss 19.619590759277344\n",
      "Batch num 870: Loss 21.576583862304688\n",
      "Batch num 880: Loss 24.64080810546875\n",
      "Batch num 890: Loss 22.542831420898438\n",
      "Batch num 900: Loss 23.96428108215332\n",
      "Batch num 910: Loss 24.64309310913086\n",
      "Batch num 920: Loss 22.107187271118164\n",
      "Batch num 930: Loss 21.16016960144043\n",
      "Batch num 940: Loss 19.14514923095703\n",
      "Batch num 950: Loss 20.191713333129883\n",
      "Batch num 960: Loss 19.97873878479004\n",
      "Batch num 970: Loss 22.720094680786133\n",
      "Batch num 980: Loss 24.496736526489258\n",
      "Batch num 990: Loss 25.11952781677246\n",
      "Batch num 1000: Loss 20.435062408447266\n",
      "Batch num 1010: Loss 18.38896369934082\n",
      "Batch num 1020: Loss 19.624393463134766\n",
      "Batch num 1030: Loss 19.292430877685547\n",
      "Batch num 1040: Loss 16.717832565307617\n",
      "Batch num 1050: Loss 18.294477462768555\n",
      "Batch num 1060: Loss 23.390113830566406\n",
      "Batch num 1070: Loss 19.910018920898438\n",
      "Batch num 1080: Loss 19.650754928588867\n",
      "Batch num 1090: Loss 19.630403518676758\n",
      "Batch num 1100: Loss 17.206649780273438\n",
      "Batch num 1110: Loss 20.373741149902344\n",
      "Batch num 1120: Loss 18.598711013793945\n",
      "Batch num 1130: Loss 19.782512664794922\n",
      "Batch num 1140: Loss 20.937793731689453\n",
      "Batch num 1150: Loss 20.660507202148438\n",
      "Batch num 1160: Loss 17.91613006591797\n",
      "Batch num 1170: Loss 17.92158317565918\n",
      "Batch num 1180: Loss 19.359481811523438\n",
      "Batch num 1190: Loss 20.453411102294922\n",
      "Batch num 1200: Loss 18.15662956237793\n",
      "Batch num 1210: Loss 22.556535720825195\n",
      "Batch num 1220: Loss 19.27695655822754\n",
      "Batch num 1230: Loss 23.650503158569336\n",
      "Batch num 1240: Loss 19.50476837158203\n",
      "Batch num 1250: Loss 21.075368881225586\n",
      "Batch num 1260: Loss 21.822507858276367\n",
      "Batch num 1270: Loss 21.092546463012695\n",
      "Batch num 1280: Loss 18.937183380126953\n",
      "Batch num 1290: Loss 20.44337272644043\n",
      "Batch num 1300: Loss 17.03173065185547\n",
      "Batch num 1310: Loss 19.513145446777344\n",
      "Batch num 1320: Loss 18.77480697631836\n",
      "Batch num 1330: Loss 20.308486938476562\n",
      "Batch num 1340: Loss 18.26129913330078\n",
      "Batch num 1350: Loss 22.97978401184082\n",
      "Batch num 1360: Loss 22.277271270751953\n",
      "Batch num 1370: Loss 26.448884963989258\n",
      "Batch num 1380: Loss 23.516420364379883\n",
      "Batch num 1390: Loss 18.142948150634766\n",
      "Batch num 1400: Loss 18.226322174072266\n",
      "Batch num 1410: Loss 20.4207763671875\n",
      "Batch num 1420: Loss 19.610214233398438\n",
      "Batch num 1430: Loss 19.745676040649414\n",
      "Batch num 1440: Loss 20.016014099121094\n",
      "Batch num 1450: Loss 17.850605010986328\n",
      "Batch num 1460: Loss 17.690954208374023\n",
      "Batch num 1470: Loss 16.968393325805664\n",
      "Batch num 1480: Loss 15.151330947875977\n",
      "Batch num 1490: Loss 21.329448699951172\n",
      "Batch num 1500: Loss 17.050140380859375\n",
      "Batch num 1510: Loss 19.973478317260742\n",
      "Batch num 1520: Loss 19.148427963256836\n",
      "Batch num 1530: Loss 19.132078170776367\n",
      "Batch num 1540: Loss 22.520681381225586\n",
      "Batch num 1550: Loss 20.06507682800293\n",
      "Batch num 1560: Loss 19.600509643554688\n",
      "Batch num 1570: Loss 16.859737396240234\n",
      "Batch num 1580: Loss 16.951663970947266\n",
      "Batch num 1590: Loss 19.922216415405273\n",
      "Batch num 1600: Loss 21.922637939453125\n",
      "Batch num 1610: Loss 18.327001571655273\n",
      "Batch num 1620: Loss 19.7785701751709\n",
      "Batch num 1630: Loss 18.069704055786133\n",
      "Batch num 1640: Loss 19.36762237548828\n",
      "Batch num 1650: Loss 15.687050819396973\n",
      "Batch num 1660: Loss 17.64505386352539\n",
      "Batch num 1670: Loss 21.66314125061035\n",
      "Batch num 1680: Loss 18.877553939819336\n",
      "Batch num 1690: Loss 18.09563446044922\n",
      "Batch num 1700: Loss 18.930259704589844\n",
      "Batch num 1710: Loss 17.457788467407227\n",
      "Batch num 1720: Loss 19.392322540283203\n",
      "Batch num 1730: Loss 20.51759910583496\n",
      "Batch num 1740: Loss 19.372282028198242\n",
      "Batch num 1750: Loss 17.844257354736328\n",
      "Batch num 1760: Loss 21.782920837402344\n",
      "Batch num 1770: Loss 31.74432373046875\n",
      "Batch num 1780: Loss 24.902427673339844\n",
      "Batch num 1790: Loss 22.184814453125\n",
      "Batch num 1800: Loss 18.431236267089844\n",
      "Batch num 1810: Loss 23.03376007080078\n",
      "Batch num 1820: Loss 18.992040634155273\n",
      "Batch num 1830: Loss 21.330060958862305\n",
      "Batch num 1840: Loss 19.864458084106445\n",
      "Batch num 1850: Loss 17.810453414916992\n",
      "Batch num 1860: Loss 13.168752670288086\n",
      "Batch num 1870: Loss 16.637252807617188\n",
      "Batch num 1880: Loss 17.717388153076172\n",
      "Batch num 1890: Loss 17.172449111938477\n",
      "Batch num 1900: Loss 23.86597442626953\n",
      "Batch num 1910: Loss 31.115345001220703\n",
      "Batch num 1920: Loss 46.20664978027344\n",
      "Batch num 1930: Loss 80.55863189697266\n",
      "Batch num 1940: Loss 34.04141616821289\n",
      "Batch num 1950: Loss 15.508020401000977\n",
      "Batch num 1960: Loss 17.368762969970703\n",
      "Batch num 1970: Loss 16.92430305480957\n",
      "Batch num 1980: Loss 52.21950149536133\n",
      "Batch num 1990: Loss 22.364103317260742\n",
      "Batch num 2000: Loss 27.376649856567383\n",
      "Batch num 2010: Loss 17.98455238342285\n",
      "Batch num 2020: Loss 14.80020809173584\n",
      "Batch num 2030: Loss 30.274856567382812\n",
      "Batch num 2040: Loss 18.74180793762207\n",
      "Batch num 2050: Loss 18.24542236328125\n",
      "Batch num 2060: Loss 16.410478591918945\n",
      "Batch num 2070: Loss 15.942442893981934\n",
      "Batch num 2080: Loss 16.944440841674805\n",
      "Batch num 2090: Loss 16.690963745117188\n",
      "Batch num 2100: Loss 12.31650161743164\n",
      "Batch num 2110: Loss 17.546409606933594\n",
      "Batch num 2120: Loss 14.866449356079102\n",
      "Batch num 2130: Loss 16.480850219726562\n",
      "Batch num 2140: Loss 14.766797065734863\n",
      "Batch num 2150: Loss 18.847440719604492\n",
      "Batch num 2160: Loss 18.56244468688965\n",
      "Batch num 2170: Loss 15.222742080688477\n",
      "Batch num 2180: Loss 22.247745513916016\n",
      "Batch num 2190: Loss 15.733229637145996\n",
      "Batch num 2200: Loss 15.418807983398438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 2210: Loss 18.396495819091797\n",
      "Batch num 2220: Loss 43.610572814941406\n",
      "Batch num 2230: Loss 20.209150314331055\n",
      "Batch num 2240: Loss 19.795150756835938\n",
      "Batch num 2250: Loss 13.492600440979004\n",
      "Batch num 2260: Loss 15.825240135192871\n",
      "Batch num 2270: Loss 15.930524826049805\n",
      "Batch num 2280: Loss 18.3903865814209\n",
      "Batch num 2290: Loss 16.120758056640625\n",
      "Batch num 2300: Loss 15.938831329345703\n",
      "Batch num 2310: Loss 15.440675735473633\n",
      "Batch num 2320: Loss 17.215120315551758\n",
      "Batch num 2330: Loss 13.7899169921875\n",
      "Batch num 2340: Loss 14.8385648727417\n",
      "Batch num 2350: Loss 17.871795654296875\n",
      "Batch num 2360: Loss 16.957395553588867\n",
      "Batch num 2370: Loss 16.784107208251953\n",
      "Batch num 2380: Loss 14.868050575256348\n",
      "Batch num 2390: Loss 12.899948120117188\n",
      "Batch num 2400: Loss 14.81849193572998\n",
      "Batch num 2410: Loss 17.079845428466797\n",
      "Batch num 2420: Loss 17.25981903076172\n",
      "Batch num 2430: Loss 14.801520347595215\n",
      "Batch num 2440: Loss 15.136960983276367\n",
      "Batch num 2450: Loss 17.773427963256836\n",
      "Batch num 2460: Loss 14.465787887573242\n",
      "Batch num 2470: Loss 15.68819522857666\n",
      "Batch num 2480: Loss 14.77039909362793\n",
      "Batch num 2490: Loss 15.49446964263916\n",
      "Batch num 2500: Loss 15.801053047180176\n",
      "Batch num 2510: Loss 14.48426342010498\n",
      "Batch num 2520: Loss 13.959476470947266\n",
      "Batch num 2530: Loss 12.30798625946045\n",
      "Batch num 2540: Loss 15.1077299118042\n",
      "Batch num 2550: Loss 17.248109817504883\n",
      "Batch num 2560: Loss 14.955156326293945\n",
      "Batch num 2570: Loss 21.60028839111328\n",
      "Batch num 2580: Loss 45.98041915893555\n",
      "Batch num 2590: Loss 16.61214828491211\n",
      "Batch num 2600: Loss 14.323274612426758\n",
      "Batch num 2610: Loss 23.513134002685547\n",
      "Batch num 2620: Loss 99.85887908935547\n",
      "Batch num 2630: Loss 39.71381378173828\n",
      "Batch num 2640: Loss 20.522123336791992\n",
      "Batch num 2650: Loss 15.501962661743164\n",
      "Batch num 2660: Loss 15.521018028259277\n",
      "Batch num 2670: Loss 22.687849044799805\n",
      "Batch num 2680: Loss 14.110209465026855\n",
      "Batch num 2690: Loss 41.11893081665039\n",
      "Batch num 2700: Loss 21.164308547973633\n",
      "Batch num 2710: Loss 14.6130952835083\n",
      "Batch num 2720: Loss 16.05302619934082\n",
      "Batch num 2730: Loss 15.639389038085938\n",
      "Batch num 2740: Loss 19.190736770629883\n",
      "Batch num 2750: Loss 20.46255874633789\n",
      "Batch num 2760: Loss 14.60755729675293\n",
      "Batch num 2770: Loss 15.513001441955566\n",
      "Batch num 2780: Loss 14.06506061553955\n",
      "Batch num 2790: Loss 14.297839164733887\n",
      "Batch num 2800: Loss 14.8482027053833\n",
      "Batch num 2810: Loss 14.99892807006836\n",
      "Batch num 2820: Loss 15.778068542480469\n",
      "Batch num 2830: Loss 13.463850021362305\n",
      "Batch num 2840: Loss 17.79688835144043\n",
      "Batch num 2850: Loss 15.048619270324707\n",
      "Batch num 2860: Loss 15.706583023071289\n",
      "Batch num 2870: Loss 17.143993377685547\n",
      "Batch num 2880: Loss 17.70538330078125\n",
      "Batch num 2890: Loss 15.9285888671875\n",
      "Batch num 2900: Loss 13.946828842163086\n",
      "Batch num 2910: Loss 13.390085220336914\n",
      "Batch num 2920: Loss 14.862942695617676\n",
      "Batch num 2930: Loss 15.459210395812988\n",
      "Batch num 2940: Loss 15.252603530883789\n",
      "Batch num 2950: Loss 14.523624420166016\n",
      "Batch num 2960: Loss 13.189475059509277\n",
      "Batch num 2970: Loss 13.688117027282715\n",
      "Batch num 2980: Loss 15.462854385375977\n",
      "Batch num 2990: Loss 15.645691871643066\n",
      "Batch num 3000: Loss 15.494641304016113\n",
      "Batch num 3010: Loss 13.913053512573242\n",
      "Batch num 3020: Loss 16.133949279785156\n",
      "Batch num 3030: Loss 14.486722946166992\n",
      "Batch num 3040: Loss 12.556574821472168\n",
      "Batch num 3050: Loss 14.295385360717773\n",
      "Batch num 3060: Loss 13.913253784179688\n",
      "Batch num 3070: Loss 14.761595726013184\n",
      "Batch num 3080: Loss 16.610748291015625\n",
      "Batch num 3090: Loss 12.782984733581543\n",
      "Batch num 3100: Loss 14.854930877685547\n",
      "Batch num 3110: Loss 17.747512817382812\n",
      "Batch num 3120: Loss 16.353038787841797\n",
      "Batch num 3130: Loss 13.44797134399414\n",
      "Batch num 3140: Loss 12.760844230651855\n",
      "Batch num 3150: Loss 14.08997631072998\n",
      "Batch num 3160: Loss 14.789588928222656\n",
      "Batch num 3170: Loss 13.927848815917969\n",
      "Batch num 3180: Loss 13.963764190673828\n",
      "Batch num 3190: Loss 14.945334434509277\n",
      "Batch num 3200: Loss 15.946118354797363\n",
      "Batch num 3210: Loss 14.51105785369873\n",
      "Batch num 3220: Loss 14.542137145996094\n",
      "Batch num 3230: Loss 12.911022186279297\n",
      "Batch num 3240: Loss 15.482232093811035\n",
      "Batch num 3250: Loss 13.732335090637207\n",
      "Batch num 3260: Loss 13.561613082885742\n",
      "Batch num 3270: Loss 15.740275382995605\n",
      "Batch num 3280: Loss 13.278786659240723\n",
      "Batch num 3290: Loss 13.254175186157227\n",
      "Batch num 3300: Loss 11.118359565734863\n",
      "Batch num 3310: Loss 14.264933586120605\n",
      "Batch num 3320: Loss 14.402456283569336\n",
      "Batch num 3330: Loss 14.413660049438477\n",
      "Batch num 3340: Loss 15.477031707763672\n",
      "Batch num 3350: Loss 13.730978965759277\n",
      "Batch num 3360: Loss 13.743736267089844\n",
      "Batch num 3370: Loss 15.423654556274414\n",
      "Batch num 3380: Loss 13.578400611877441\n",
      "Batch num 3390: Loss 12.464658737182617\n",
      "Batch num 3400: Loss 15.018919944763184\n",
      "Batch num 3410: Loss 14.027017593383789\n",
      "Batch num 3420: Loss 15.908699989318848\n",
      "Batch num 3430: Loss 14.664726257324219\n",
      "Batch num 3440: Loss 14.914508819580078\n",
      "Batch num 3450: Loss 13.69226360321045\n",
      "Batch num 3460: Loss 13.406700134277344\n",
      "Batch num 3470: Loss 13.991181373596191\n",
      "Batch num 3480: Loss 12.655731201171875\n",
      "Batch num 3490: Loss 14.03329849243164\n",
      "Batch num 3500: Loss 12.163009643554688\n",
      "Batch num 3510: Loss 15.821309089660645\n",
      "Batch num 3520: Loss 14.37016487121582\n",
      "Batch num 3530: Loss 11.877778053283691\n",
      "Batch num 3540: Loss 14.119611740112305\n",
      "Batch num 3550: Loss 13.188746452331543\n",
      "Batch num 3560: Loss 12.43510913848877\n",
      "Batch num 3570: Loss 13.074178695678711\n",
      "Batch num 3580: Loss 15.025802612304688\n",
      "Batch num 3590: Loss 12.837218284606934\n",
      "Batch num 3600: Loss 14.885820388793945\n",
      "Batch num 3610: Loss 16.97252655029297\n",
      "Batch num 3620: Loss 13.233172416687012\n",
      "Batch num 3630: Loss 13.259142875671387\n",
      "Batch num 3640: Loss 12.533951759338379\n",
      "Batch num 3650: Loss 15.432801246643066\n",
      "Batch num 3660: Loss 13.260963439941406\n",
      "Batch num 3670: Loss 14.606759071350098\n",
      "Batch num 3680: Loss 12.293004035949707\n",
      "Batch num 3690: Loss 13.039331436157227\n",
      "Batch num 3700: Loss 12.464353561401367\n",
      "Batch num 3710: Loss 14.07323169708252\n",
      "Batch num 3720: Loss 12.351264953613281\n",
      "Batch num 3730: Loss 12.233981132507324\n",
      "Batch num 3740: Loss 15.5143404006958\n",
      "Batch num 3750: Loss 13.197932243347168\n",
      "Batch num 3760: Loss 12.759681701660156\n",
      "Batch num 3770: Loss 13.486136436462402\n",
      "Batch num 3780: Loss 12.06684398651123\n",
      "Batch num 3790: Loss 12.686241149902344\n",
      "Batch num 3800: Loss 10.5283784866333\n",
      "Batch num 3810: Loss 13.677231788635254\n",
      "Batch num 3820: Loss 13.312186241149902\n",
      "Batch num 3830: Loss 14.25075626373291\n",
      "Batch num 3840: Loss 12.865043640136719\n",
      "Batch num 3850: Loss 14.505257606506348\n",
      "Batch num 3860: Loss 12.44808292388916\n",
      "Batch num 3870: Loss 13.574313163757324\n",
      "Batch num 3880: Loss 14.093567848205566\n",
      "Batch num 3890: Loss 12.214906692504883\n",
      "Batch num 3900: Loss 10.503960609436035\n",
      "Batch num 3910: Loss 13.139575004577637\n",
      "Batch num 3920: Loss 13.88011646270752\n",
      "Batch num 3930: Loss 15.731274604797363\n",
      "Batch num 3940: Loss 12.894414901733398\n",
      "Batch num 3950: Loss 13.345189094543457\n",
      "Batch num 3960: Loss 13.771903038024902\n",
      "Batch num 3970: Loss 14.54675006866455\n",
      "Batch num 3980: Loss 11.44200611114502\n",
      "Batch num 3990: Loss 14.14822769165039\n",
      "Batch num 4000: Loss 13.117481231689453\n",
      "Batch num 4010: Loss 11.74238395690918\n",
      "Batch num 4020: Loss 11.822527885437012\n",
      "Batch num 4030: Loss 13.483890533447266\n",
      "Batch num 4040: Loss 14.11492919921875\n",
      "Batch num 4050: Loss 13.127321243286133\n",
      "Batch num 4060: Loss 14.067873001098633\n",
      "Batch num 4070: Loss 16.931995391845703\n",
      "Batch num 4080: Loss 25.37618637084961\n",
      "Batch num 4090: Loss 16.067466735839844\n",
      "Batch num 4100: Loss 12.515896797180176\n",
      "Batch num 4110: Loss 14.102338790893555\n",
      "Batch num 4120: Loss 12.840910911560059\n",
      "Batch num 4130: Loss 12.96409797668457\n",
      "Batch num 4140: Loss 14.32745361328125\n",
      "Batch num 4150: Loss 12.978560447692871\n",
      "Batch num 4160: Loss 12.308603286743164\n",
      "Batch num 4170: Loss 14.70404052734375\n",
      "Batch num 4180: Loss 14.098237991333008\n",
      "Batch num 4190: Loss 14.171523094177246\n",
      "Batch num 4200: Loss 15.882994651794434\n",
      "Batch num 4210: Loss 13.939909934997559\n",
      "Batch num 4220: Loss 14.418790817260742\n",
      "Batch num 4230: Loss 15.124649047851562\n",
      "Batch num 4240: Loss 13.126882553100586\n",
      "Batch num 4250: Loss 11.822977066040039\n",
      "Batch num 4260: Loss 15.60789966583252\n",
      "Batch num 4270: Loss 13.365589141845703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 4280: Loss 12.61088752746582\n",
      "Batch num 4290: Loss 13.184579849243164\n",
      "Batch num 4300: Loss 12.108922958374023\n",
      "Batch num 4310: Loss 14.227204322814941\n",
      "Batch num 4320: Loss 11.626322746276855\n",
      "Batch num 4330: Loss 13.085803985595703\n",
      "Batch num 4340: Loss 19.80148696899414\n",
      "Batch num 4350: Loss 14.107999801635742\n",
      "Batch num 4360: Loss 11.632828712463379\n",
      "Batch num 4370: Loss 14.383805274963379\n",
      "Batch num 4380: Loss 11.610433578491211\n",
      "Batch num 4390: Loss 12.432424545288086\n",
      "Batch num 4400: Loss 8.887800216674805\n",
      "Batch num 4410: Loss 10.532144546508789\n",
      "Batch num 4420: Loss 13.726178169250488\n",
      "Batch num 4430: Loss 11.312039375305176\n",
      "Batch num 4440: Loss 17.447341918945312\n",
      "Batch num 4450: Loss 12.232714653015137\n",
      "Batch num 4460: Loss 12.683088302612305\n",
      "Batch num 4470: Loss 12.823184967041016\n",
      "Batch num 4480: Loss 13.608194351196289\n",
      "Batch num 4490: Loss 13.919692039489746\n",
      "Batch num 4500: Loss 12.998674392700195\n",
      "Batch num 4510: Loss 14.62230110168457\n",
      "Batch num 4520: Loss 12.694602966308594\n",
      "Batch num 4530: Loss 13.773037910461426\n",
      "Batch num 4540: Loss 14.295601844787598\n",
      "Batch num 4550: Loss 19.48650550842285\n",
      "Batch num 4560: Loss 12.391839981079102\n",
      "Batch num 4570: Loss 11.570430755615234\n",
      "Batch num 4580: Loss 16.71202850341797\n",
      "Batch num 4590: Loss 13.747335433959961\n",
      "Batch num 4600: Loss 12.465027809143066\n",
      "Batch num 4610: Loss 13.749066352844238\n",
      "Batch num 4620: Loss 14.032197952270508\n",
      "Batch num 4630: Loss 11.494815826416016\n",
      "Batch num 4640: Loss 11.728328704833984\n",
      "Batch num 4650: Loss 11.920085906982422\n",
      "Batch num 4660: Loss 13.465341567993164\n",
      "Batch num 4670: Loss 12.232717514038086\n",
      "Batch num 4680: Loss 12.715059280395508\n",
      "Batch num 4690: Loss 11.998766899108887\n",
      "Batch num 4700: Loss 12.609780311584473\n",
      "Batch num 4710: Loss 15.291407585144043\n",
      "Batch num 4720: Loss 12.644062042236328\n",
      "Batch num 4730: Loss 14.802310943603516\n",
      "Batch num 4740: Loss 14.89515209197998\n",
      "Batch num 4750: Loss 12.960358619689941\n",
      "Batch num 4760: Loss 12.76572036743164\n",
      "Batch num 4770: Loss 12.121644020080566\n",
      "Batch num 4780: Loss 12.08082103729248\n",
      "Batch num 4790: Loss 14.460753440856934\n",
      "Batch num 4800: Loss 13.951205253601074\n",
      "Batch num 4810: Loss 13.34997272491455\n",
      "Batch num 4820: Loss 11.88549518585205\n",
      "Batch num 4830: Loss 13.095051765441895\n",
      "Batch num 4840: Loss 11.149454116821289\n",
      "Batch num 4850: Loss 12.537908554077148\n",
      "Batch num 4860: Loss 12.092042922973633\n",
      "Batch num 4870: Loss 13.329510688781738\n",
      "Batch num 4880: Loss 11.856630325317383\n",
      "Batch num 4890: Loss 12.103666305541992\n",
      "Batch num 4900: Loss 12.565960884094238\n",
      "Batch num 4910: Loss 13.567201614379883\n",
      "Batch num 4920: Loss 10.778763771057129\n",
      "Batch num 4930: Loss 10.624150276184082\n",
      "Batch num 4940: Loss 14.067362785339355\n",
      "Batch num 4950: Loss 13.847241401672363\n",
      "Batch num 4960: Loss 12.621785163879395\n",
      "Batch num 4970: Loss 13.620349884033203\n",
      "Batch num 4980: Loss 13.579605102539062\n",
      "Batch num 4990: Loss 12.905862808227539\n",
      "Batch num 5000: Loss 11.71962833404541\n",
      "Batch num 5010: Loss 13.355478286743164\n",
      "Batch num 5020: Loss 11.833802223205566\n",
      "Batch num 5030: Loss 12.942463874816895\n",
      "Batch num 5040: Loss 12.188580513000488\n",
      "Batch num 5050: Loss 12.81082534790039\n",
      "Batch num 5060: Loss 11.236867904663086\n",
      "Batch num 5070: Loss 12.330328941345215\n",
      "Batch num 5080: Loss 12.935680389404297\n",
      "Batch num 5090: Loss 15.195162773132324\n",
      "Batch num 5100: Loss 12.988959312438965\n",
      "Batch num 5110: Loss 12.551812171936035\n",
      "Batch num 5120: Loss 12.699610710144043\n",
      "Batch num 5130: Loss 12.642440795898438\n",
      "Batch num 5140: Loss 12.146562576293945\n",
      "Batch num 5150: Loss 11.843690872192383\n",
      "Batch num 5160: Loss 11.322975158691406\n",
      "Batch num 5170: Loss 12.220173835754395\n",
      "Batch num 5180: Loss 13.016444206237793\n",
      "Batch num 5190: Loss 12.015311241149902\n",
      "Batch num 5200: Loss 12.225204467773438\n",
      "Batch num 5210: Loss 11.360953330993652\n",
      "Batch num 5220: Loss 13.643399238586426\n",
      "Batch num 5230: Loss 14.254128456115723\n",
      "Batch num 5240: Loss 12.185111045837402\n",
      "Batch num 5250: Loss 12.33871078491211\n",
      "Batch num 5260: Loss 12.036459922790527\n",
      "Batch num 5270: Loss 12.61197566986084\n",
      "Batch num 5280: Loss 14.217367172241211\n",
      "Batch num 5290: Loss 12.902642250061035\n",
      "Batch num 5300: Loss 10.935391426086426\n",
      "Batch num 5310: Loss 11.553926467895508\n",
      "Batch num 5320: Loss 12.355063438415527\n",
      "Batch num 5330: Loss 9.762370109558105\n",
      "Batch num 5340: Loss 10.421831130981445\n",
      "Batch num 5350: Loss 13.476095199584961\n",
      "Batch num 5360: Loss 11.105916023254395\n",
      "Batch num 5370: Loss 12.489587783813477\n",
      "Batch num 5380: Loss 12.964303970336914\n",
      "Batch num 5390: Loss 11.615943908691406\n",
      "Batch num 5400: Loss 12.783708572387695\n",
      "Batch num 5410: Loss 14.087600708007812\n",
      "Batch num 5420: Loss 13.324814796447754\n",
      "Batch num 5430: Loss 12.892146110534668\n",
      "Batch num 5440: Loss 12.944022178649902\n",
      "Batch num 5450: Loss 11.802639961242676\n",
      "Batch num 5460: Loss 12.946575164794922\n",
      "Batch num 5470: Loss 12.8148832321167\n",
      "Batch num 5480: Loss 12.004404067993164\n",
      "Batch num 5490: Loss 13.378573417663574\n",
      "Batch num 5500: Loss 15.25886344909668\n",
      "Batch num 5510: Loss 24.720291137695312\n",
      "Batch num 5520: Loss 13.312734603881836\n",
      "Batch num 5530: Loss 12.071983337402344\n",
      "Batch num 5540: Loss 12.191896438598633\n",
      "Batch num 5550: Loss 11.239458084106445\n",
      "Batch num 5560: Loss 13.523540496826172\n",
      "Batch num 5570: Loss 11.853477478027344\n",
      "Batch num 5580: Loss 12.352328300476074\n",
      "Batch num 5590: Loss 13.714174270629883\n",
      "Batch num 5600: Loss 11.575873374938965\n",
      "Batch num 5610: Loss 13.475175857543945\n",
      "Batch num 5620: Loss 16.381563186645508\n",
      "Batch num 5630: Loss 30.38560676574707\n",
      "Batch num 5640: Loss 40.857791900634766\n",
      "Batch num 5650: Loss 46.83815002441406\n",
      "Batch num 5660: Loss 16.316255569458008\n",
      "Batch num 5670: Loss 11.532953262329102\n",
      "Batch num 5680: Loss 11.57440185546875\n",
      "Batch num 5690: Loss 11.636711120605469\n",
      "Batch num 5700: Loss 13.065003395080566\n",
      "Batch num 5710: Loss 11.814460754394531\n",
      "Batch num 5720: Loss 10.900979995727539\n",
      "Batch num 5730: Loss 12.870869636535645\n",
      "Batch num 5740: Loss 15.600717544555664\n",
      "Batch num 5750: Loss 11.813250541687012\n",
      "Batch num 5760: Loss 13.583268165588379\n",
      "Batch num 5770: Loss 11.740025520324707\n",
      "Batch num 5780: Loss 10.90622329711914\n",
      "Batch num 5790: Loss 10.981295585632324\n",
      "Batch num 5800: Loss 11.250452995300293\n",
      "Batch num 5810: Loss 10.630352973937988\n",
      "Batch num 5820: Loss 10.693413734436035\n",
      "Batch num 5830: Loss 13.524949073791504\n",
      "Batch num 5840: Loss 12.098445892333984\n",
      "Batch num 5850: Loss 12.720556259155273\n",
      "Batch num 5860: Loss 11.784090995788574\n",
      "Batch num 5870: Loss 13.1864652633667\n",
      "Batch num 5880: Loss 11.531830787658691\n",
      "Batch num 5890: Loss 12.344884872436523\n",
      "Batch num 5900: Loss 13.148256301879883\n",
      "Batch num 5910: Loss 13.14668083190918\n",
      "Batch num 5920: Loss 13.107467651367188\n",
      "Batch num 5930: Loss 12.544196128845215\n",
      "Batch num 5940: Loss 12.7247953414917\n",
      "Batch num 5950: Loss 12.502339363098145\n",
      "Batch num 5960: Loss 12.976869583129883\n",
      "Batch num 5970: Loss 12.311469078063965\n",
      "Batch num 5980: Loss 13.30259895324707\n",
      "Batch num 5990: Loss 12.506922721862793\n",
      "Batch num 6000: Loss 13.138556480407715\n",
      "Batch num 6010: Loss 17.330223083496094\n",
      "Batch num 6020: Loss 11.77269172668457\n",
      "Batch num 6030: Loss 14.46639347076416\n",
      "Batch num 6040: Loss 22.557668685913086\n",
      "Batch num 6050: Loss 12.920475959777832\n",
      "Batch num 6060: Loss 13.164910316467285\n",
      "Batch num 6070: Loss 36.88816452026367\n",
      "Batch num 6080: Loss 11.141976356506348\n",
      "Batch num 6090: Loss 29.121047973632812\n",
      "Batch num 6100: Loss 13.439498901367188\n",
      "Batch num 6110: Loss 12.950474739074707\n",
      "Batch num 6120: Loss 20.092586517333984\n",
      "Batch num 6130: Loss 32.25904083251953\n",
      "Batch num 6140: Loss 23.823253631591797\n",
      "Batch num 6150: Loss 23.189105987548828\n",
      "Batch num 6160: Loss 13.275924682617188\n",
      "Batch num 6170: Loss 19.007186889648438\n",
      "Batch num 6180: Loss 11.474056243896484\n",
      "Batch num 6190: Loss 13.338850021362305\n",
      "Batch num 6200: Loss 11.813072204589844\n",
      "Batch num 6210: Loss 11.870697975158691\n",
      "Batch num 6220: Loss 12.552640914916992\n",
      "Batch num 6230: Loss 12.081247329711914\n",
      "Batch num 6240: Loss 52.820701599121094\n",
      "Batch num 6250: Loss 24.35268783569336\n",
      "Batch num 6260: Loss 29.415851593017578\n",
      "Batch num 6270: Loss 12.606740951538086\n",
      "Batch num 6280: Loss 12.124504089355469\n",
      "Batch num 6290: Loss 13.784612655639648\n",
      "Batch num 6300: Loss 13.683521270751953\n",
      "Batch num 6310: Loss 12.323047637939453\n",
      "Batch num 6320: Loss 16.65228271484375\n",
      "Batch num 6330: Loss 11.990411758422852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 6340: Loss 13.133964538574219\n",
      "Batch num 6350: Loss 12.204507827758789\n",
      "Batch num 6360: Loss 12.166802406311035\n",
      "Batch num 6370: Loss 11.86599063873291\n",
      "Batch num 6380: Loss 10.782913208007812\n",
      "Batch num 6390: Loss 13.821427345275879\n",
      "Batch num 6400: Loss 12.586196899414062\n",
      "Batch num 6410: Loss 12.225229263305664\n",
      "Batch num 6420: Loss 14.119084358215332\n",
      "Batch num 6430: Loss 11.910850524902344\n",
      "Batch num 6440: Loss 11.559564590454102\n",
      "Batch num 6450: Loss 11.296067237854004\n",
      "Batch num 6460: Loss 13.032496452331543\n",
      "Batch num 6470: Loss 11.89477825164795\n",
      "Batch num 6480: Loss 11.508818626403809\n",
      "Batch num 6490: Loss 11.307814598083496\n",
      "Batch num 6500: Loss 11.252508163452148\n",
      "Batch num 6510: Loss 12.159656524658203\n",
      "Batch num 6520: Loss 12.10630989074707\n",
      "Batch num 6530: Loss 11.729484558105469\n",
      "Batch num 6540: Loss 13.03402328491211\n",
      "Batch num 6550: Loss 12.81981086730957\n",
      "Batch num 6560: Loss 12.594045639038086\n",
      "Batch num 6570: Loss 10.844291687011719\n",
      "Batch num 6580: Loss 11.735206604003906\n",
      "Batch num 6590: Loss 13.000560760498047\n",
      "Batch num 6600: Loss 12.146659851074219\n",
      "Batch num 6610: Loss 12.159242630004883\n",
      "Batch num 6620: Loss 12.411209106445312\n",
      "Batch num 6630: Loss 12.104544639587402\n",
      "Batch num 6640: Loss 10.79910945892334\n",
      "Batch num 6650: Loss 10.810441017150879\n",
      "Batch num 6660: Loss 11.628873825073242\n",
      "Batch num 6670: Loss 12.516274452209473\n",
      "Batch num 6680: Loss 11.790095329284668\n",
      "Batch num 6690: Loss 13.018027305603027\n",
      "Batch num 6700: Loss 13.064986228942871\n",
      "Batch num 6710: Loss 13.096585273742676\n",
      "Batch num 6720: Loss 11.985579490661621\n",
      "Batch num 6730: Loss 12.641532897949219\n",
      "Batch num 6740: Loss 11.594642639160156\n",
      "Batch num 6750: Loss 10.369014739990234\n",
      "Batch num 6760: Loss 13.751075744628906\n",
      "Batch num 6770: Loss 11.632451057434082\n",
      "Batch num 6780: Loss 11.719646453857422\n",
      "Batch num 6790: Loss 12.895870208740234\n",
      "Batch num 6800: Loss 10.767285346984863\n",
      "Batch num 6810: Loss 11.345099449157715\n",
      "Batch num 6820: Loss 11.804840087890625\n",
      "Batch num 6830: Loss 11.440293312072754\n",
      "Batch num 6840: Loss 11.783424377441406\n",
      "Batch num 6850: Loss 12.22354793548584\n",
      "Batch num 6860: Loss 12.25859260559082\n",
      "Batch num 6870: Loss 10.805044174194336\n",
      "Batch num 6880: Loss 12.753076553344727\n",
      "Batch num 6890: Loss 11.332215309143066\n",
      "Batch num 6900: Loss 11.98525619506836\n",
      "Batch num 6910: Loss 12.697160720825195\n",
      "Batch num 6920: Loss 11.925358772277832\n",
      "Batch num 6930: Loss 11.341007232666016\n",
      "Batch num 6940: Loss 11.396661758422852\n",
      "Batch num 6950: Loss 12.217273712158203\n",
      "Batch num 6960: Loss 12.83855152130127\n",
      "Batch num 6970: Loss 12.271841049194336\n",
      "Batch num 6980: Loss 13.092569351196289\n",
      "Batch num 6990: Loss 11.640042304992676\n",
      "Batch num 7000: Loss 12.084931373596191\n",
      "Batch num 7010: Loss 10.597328186035156\n",
      "Batch num 7020: Loss 11.91722583770752\n",
      "Batch num 7030: Loss 11.565248489379883\n",
      "Batch num 7040: Loss 13.160322189331055\n",
      "Batch num 7050: Loss 11.426088333129883\n",
      "Batch num 7060: Loss 11.775751113891602\n",
      "Batch num 7070: Loss 13.505500793457031\n",
      "Batch num 7080: Loss 12.638615608215332\n",
      "Batch num 7090: Loss 11.946060180664062\n",
      "Batch num 7100: Loss 11.060850143432617\n",
      "Batch num 7110: Loss 12.432415962219238\n",
      "Batch num 7120: Loss 11.613423347473145\n",
      "Batch num 7130: Loss 12.236066818237305\n",
      "Batch num 7140: Loss 13.081459045410156\n",
      "Batch num 7150: Loss 12.024025917053223\n",
      "Batch num 7160: Loss 13.467451095581055\n",
      "Batch num 7170: Loss 12.188739776611328\n",
      "Batch num 7180: Loss 11.901959419250488\n",
      "Batch num 7190: Loss 10.720918655395508\n",
      "Batch num 7200: Loss 11.348715782165527\n",
      "Batch num 7210: Loss 10.892271041870117\n",
      "Batch num 7220: Loss 14.536011695861816\n",
      "Batch num 7230: Loss 12.689128875732422\n",
      "Batch num 7240: Loss 10.622323989868164\n",
      "Batch num 7250: Loss 11.937054634094238\n",
      "Batch num 7260: Loss 8.867778778076172\n",
      "Batch num 7270: Loss 11.950342178344727\n",
      "Batch num 7280: Loss 12.97254467010498\n",
      "Batch num 7290: Loss 12.559654235839844\n",
      "Batch num 7300: Loss 10.16585922241211\n",
      "Batch num 7310: Loss 12.45056438446045\n",
      "Batch num 7320: Loss 12.969234466552734\n",
      "Batch num 7330: Loss 12.292842864990234\n",
      "Batch num 7340: Loss 13.092107772827148\n",
      "Batch num 7350: Loss 11.103938102722168\n",
      "Batch num 7360: Loss 12.840377807617188\n",
      "Batch num 7370: Loss 11.655900955200195\n",
      "Batch num 7380: Loss 9.82909870147705\n",
      "Batch num 7390: Loss 10.61727523803711\n",
      "Batch num 7400: Loss 13.475065231323242\n",
      "Batch num 7410: Loss 11.218310356140137\n",
      "Batch num 7420: Loss 25.049049377441406\n",
      "Batch num 7430: Loss 14.782649040222168\n",
      "Batch num 7440: Loss 12.596165657043457\n",
      "Batch num 7450: Loss 98.58657836914062\n",
      "Batch num 7460: Loss 43.87646484375\n",
      "Batch num 7470: Loss 12.77098274230957\n",
      "Batch num 7480: Loss 12.237040519714355\n",
      "Batch num 7490: Loss 15.942943572998047\n",
      "Batch num 7500: Loss 13.508787155151367\n",
      "Batch num 7510: Loss 10.94960880279541\n",
      "Batch num 7520: Loss 14.599793434143066\n",
      "Batch num 7530: Loss 10.846099853515625\n",
      "Batch num 7540: Loss 12.517776489257812\n",
      "Batch num 7550: Loss 11.552446365356445\n",
      "Batch num 7560: Loss 12.011406898498535\n",
      "Batch num 7570: Loss 10.664836883544922\n",
      "Batch num 7580: Loss 10.914308547973633\n",
      "Batch num 7590: Loss 12.156698226928711\n",
      "Batch num 7600: Loss 11.753168106079102\n",
      "Batch num 7610: Loss 11.115682601928711\n",
      "Batch num 7620: Loss 10.14931869506836\n",
      "Batch num 7630: Loss 10.332386016845703\n",
      "Batch num 7640: Loss 9.655332565307617\n",
      "Batch num 7650: Loss 11.62152099609375\n",
      "Batch num 7660: Loss 13.776233673095703\n",
      "Batch num 7670: Loss 11.644926071166992\n",
      "Batch num 7680: Loss 15.108240127563477\n",
      "Batch num 7690: Loss 12.710229873657227\n",
      "Batch num 7700: Loss 12.115659713745117\n",
      "Batch num 7710: Loss 8.400347709655762\n",
      "Batch num 7720: Loss 11.82652759552002\n",
      "Batch num 7730: Loss 11.984395980834961\n",
      "Batch num 7740: Loss 12.421884536743164\n",
      "Batch num 7750: Loss 12.373940467834473\n",
      "Batch num 7760: Loss 10.225927352905273\n",
      "Batch num 7770: Loss 11.676695823669434\n",
      "Batch num 7780: Loss 11.461677551269531\n",
      "Batch num 7790: Loss 11.874871253967285\n",
      "Batch num 7800: Loss 11.065018653869629\n",
      "Batch num 7810: Loss 13.647614479064941\n",
      "Batch num 7820: Loss 12.736358642578125\n",
      "Batch num 7830: Loss 12.759258270263672\n",
      "Batch num 7840: Loss 11.118459701538086\n",
      "Batch num 7850: Loss 13.635005950927734\n",
      "Batch num 7860: Loss 11.763666152954102\n",
      "Batch num 7870: Loss 12.253557205200195\n",
      "Batch num 7880: Loss 11.77442455291748\n",
      "Batch num 7890: Loss 13.009458541870117\n",
      "Batch num 7900: Loss 13.954012870788574\n",
      "Batch num 7910: Loss 11.908368110656738\n",
      "Batch num 7920: Loss 11.404609680175781\n",
      "Batch num 7930: Loss 11.316926956176758\n",
      "Batch num 7940: Loss 10.617622375488281\n",
      "Batch num 7950: Loss 12.200469970703125\n",
      "Batch num 7960: Loss 11.238523483276367\n",
      "Batch num 7970: Loss 12.053889274597168\n",
      "Batch num 7980: Loss 11.797622680664062\n",
      "Batch num 7990: Loss 12.775972366333008\n",
      "Batch num 8000: Loss 12.362722396850586\n",
      "Batch num 8010: Loss 12.711389541625977\n",
      "Batch num 8020: Loss 12.181873321533203\n",
      "Batch num 8030: Loss 11.041165351867676\n",
      "Batch num 8040: Loss 12.359357833862305\n",
      "Batch num 8050: Loss 13.43429946899414\n",
      "Batch num 8060: Loss 12.180933952331543\n",
      "Batch num 8070: Loss 10.589723587036133\n",
      "Batch num 8080: Loss 10.831058502197266\n",
      "Batch num 8090: Loss 10.664409637451172\n",
      "Batch num 8100: Loss 12.137571334838867\n",
      "Batch num 8110: Loss 12.007081985473633\n",
      "Batch num 8120: Loss 11.616211891174316\n",
      "Batch num 8130: Loss 11.665712356567383\n",
      "Batch num 8140: Loss 12.205044746398926\n",
      "Batch num 8150: Loss 11.959537506103516\n",
      "Batch num 8160: Loss 12.862154006958008\n",
      "Batch num 8170: Loss 11.40977668762207\n",
      "Batch num 8180: Loss 12.737921714782715\n",
      "Batch num 8190: Loss 10.446791648864746\n",
      "Batch num 8200: Loss 11.538583755493164\n",
      "Batch num 8210: Loss 10.347906112670898\n",
      "Batch num 8220: Loss 11.247966766357422\n",
      "Batch num 8230: Loss 10.996809005737305\n",
      "Batch num 8240: Loss 11.351394653320312\n",
      "Batch num 8250: Loss 11.017361640930176\n",
      "Batch num 8260: Loss 10.91434097290039\n",
      "Batch num 8270: Loss 12.045209884643555\n",
      "Batch num 8280: Loss 10.502821922302246\n",
      "Batch num 8290: Loss 10.523874282836914\n",
      "Batch num 8300: Loss 11.835803985595703\n",
      "Batch num 8310: Loss 12.141570091247559\n",
      "Batch num 8320: Loss 11.628425598144531\n",
      "Batch num 8330: Loss 12.290237426757812\n",
      "Batch num 8340: Loss 10.15384292602539\n",
      "Batch num 8350: Loss 12.221927642822266\n",
      "Batch num 8360: Loss 11.827653884887695\n",
      "Batch num 8370: Loss 13.30190372467041\n",
      "Batch num 8380: Loss 13.315324783325195\n",
      "Batch num 8390: Loss 11.16758918762207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 8400: Loss 12.320418357849121\n",
      "Batch num 8410: Loss 12.38836669921875\n",
      "Batch num 8420: Loss 10.007856369018555\n",
      "Batch num 8430: Loss 13.679346084594727\n",
      "Batch num 8440: Loss 11.993042945861816\n",
      "Batch num 8450: Loss 13.118221282958984\n",
      "Batch num 8460: Loss 11.46019172668457\n",
      "Batch num 8470: Loss 12.805059432983398\n",
      "Batch num 8480: Loss 12.113720893859863\n",
      "Batch num 8490: Loss 11.27938175201416\n",
      "Batch num 8500: Loss 8.68008041381836\n",
      "Batch num 8510: Loss 11.838997840881348\n",
      "Batch num 8520: Loss 12.409638404846191\n",
      "Batch num 8530: Loss 12.982776641845703\n",
      "Batch num 8540: Loss 12.772794723510742\n",
      "Batch num 8550: Loss 12.830733299255371\n",
      "Batch num 8560: Loss 13.170856475830078\n",
      "Batch num 8570: Loss 13.268133163452148\n",
      "Batch num 8580: Loss 11.373501777648926\n",
      "Batch num 8590: Loss 11.413782119750977\n",
      "Batch num 8600: Loss 11.44261360168457\n",
      "Batch num 8610: Loss 14.212586402893066\n",
      "Batch num 8620: Loss 10.674121856689453\n",
      "Batch num 8630: Loss 11.238775253295898\n",
      "Batch num 8640: Loss 12.628458023071289\n",
      "Batch num 8650: Loss 14.004133224487305\n",
      "Batch num 8660: Loss 14.358037948608398\n",
      "Batch num 8670: Loss 11.892967224121094\n",
      "Batch num 8680: Loss 11.335477828979492\n",
      "Batch num 8690: Loss 11.161057472229004\n",
      "Batch num 8700: Loss 11.135860443115234\n",
      "Batch num 8710: Loss 11.941154479980469\n",
      "Batch num 8720: Loss 11.14487075805664\n",
      "Batch num 8730: Loss 12.115262031555176\n",
      "Batch num 8740: Loss 10.507986068725586\n",
      "Batch num 8750: Loss 11.149852752685547\n",
      "Batch num 8760: Loss 10.74576187133789\n",
      "Batch num 8770: Loss 12.182140350341797\n",
      "Batch num 8780: Loss 13.248519897460938\n",
      "Batch num 8790: Loss 9.336503982543945\n",
      "Batch num 8800: Loss 11.664383888244629\n",
      "Batch num 8810: Loss 11.60824966430664\n",
      "Batch num 8820: Loss 11.031789779663086\n",
      "Batch num 8830: Loss 12.055062294006348\n",
      "Batch num 8840: Loss 10.676925659179688\n",
      "Batch num 8850: Loss 13.318842887878418\n",
      "Batch num 8860: Loss 14.519835472106934\n",
      "Batch num 8870: Loss 12.007659912109375\n",
      "Batch num 8880: Loss 12.99850845336914\n",
      "Batch num 8890: Loss 10.916838645935059\n",
      "Batch num 8900: Loss 12.9600830078125\n",
      "Batch num 8910: Loss 8.948614120483398\n",
      "Batch num 8920: Loss 11.3406982421875\n",
      "Batch num 8930: Loss 11.835290908813477\n",
      "Batch num 8940: Loss 11.797834396362305\n",
      "Batch num 8950: Loss 11.153299331665039\n",
      "Batch num 8960: Loss 10.937860488891602\n",
      "Batch num 8970: Loss 11.962507247924805\n",
      "Batch num 8980: Loss 11.417390823364258\n",
      "Batch num 8990: Loss 11.398600578308105\n",
      "Batch num 9000: Loss 12.789039611816406\n",
      "Batch num 9010: Loss 9.453222274780273\n",
      "Batch num 9020: Loss 11.431987762451172\n",
      "Batch num 9030: Loss 12.244094848632812\n",
      "Batch num 9040: Loss 10.540372848510742\n",
      "Batch num 9050: Loss 15.851434707641602\n",
      "Batch num 9060: Loss 12.01567268371582\n",
      "Batch num 9070: Loss 12.960103988647461\n",
      "Batch num 9080: Loss 12.128547668457031\n",
      "Batch num 9090: Loss 12.828125\n",
      "Batch num 9100: Loss 10.774252891540527\n",
      "Batch num 9110: Loss 11.468086242675781\n",
      "Batch num 9120: Loss 11.203544616699219\n",
      "Batch num 9130: Loss 11.492469787597656\n",
      "Batch num 9140: Loss 11.85245132446289\n",
      "Batch num 9150: Loss 10.060290336608887\n",
      "Batch num 9160: Loss 11.802983283996582\n",
      "Batch num 9170: Loss 12.1760892868042\n",
      "Batch num 9180: Loss 12.07110595703125\n",
      "Batch num 9190: Loss 11.440192222595215\n",
      "Batch num 9200: Loss 11.345640182495117\n",
      "Batch num 9210: Loss 10.950946807861328\n",
      "Batch num 9220: Loss 11.294702529907227\n",
      "Batch num 9230: Loss 8.73885726928711\n",
      "Batch num 9240: Loss 10.986722946166992\n",
      "Batch num 9250: Loss 10.1954927444458\n",
      "Batch num 9260: Loss 10.327438354492188\n",
      "Batch num 9270: Loss 11.135921478271484\n",
      "Batch num 9280: Loss 10.942193031311035\n",
      "Batch num 9290: Loss 10.987466812133789\n",
      "Batch num 9300: Loss 11.970399856567383\n",
      "Batch num 9310: Loss 10.767566680908203\n",
      "Batch num 9320: Loss 10.260934829711914\n",
      "Batch num 9330: Loss 11.73544692993164\n",
      "Batch num 9340: Loss 11.146738052368164\n",
      "Batch num 9350: Loss 11.309492111206055\n",
      "Batch num 9360: Loss 11.613521575927734\n",
      "Batch num 9370: Loss 12.517759323120117\n",
      "Batch num 9380: Loss 11.65853214263916\n",
      "Batch num 9390: Loss 11.107300758361816\n",
      "Batch num 9400: Loss 11.021897315979004\n",
      "Batch num 9410: Loss 11.2588472366333\n",
      "Batch num 9420: Loss 11.00546646118164\n",
      "Batch num 9430: Loss 12.526363372802734\n",
      "Batch num 9440: Loss 11.257549285888672\n",
      "Batch num 9450: Loss 11.426773071289062\n",
      "Batch num 9460: Loss 10.381546020507812\n",
      "Batch num 9470: Loss 9.683820724487305\n",
      "Batch num 9480: Loss 11.981834411621094\n",
      "Batch num 9490: Loss 11.424642562866211\n",
      "Batch num 9500: Loss 10.930112838745117\n",
      "Batch num 9510: Loss 12.145468711853027\n",
      "Batch num 9520: Loss 10.326827049255371\n",
      "Batch num 9530: Loss 12.459606170654297\n",
      "Batch num 9540: Loss 10.3221435546875\n",
      "Batch num 9550: Loss 12.377758979797363\n",
      "Batch num 9560: Loss 10.943124771118164\n",
      "Batch num 9570: Loss 13.658904075622559\n",
      "Batch num 9580: Loss 10.269889831542969\n",
      "Batch num 9590: Loss 10.724518775939941\n",
      "Batch num 9600: Loss 10.399663925170898\n",
      "Batch num 9610: Loss 12.855379104614258\n",
      "Batch num 9620: Loss 11.609864234924316\n",
      "Batch num 9630: Loss 11.795133590698242\n",
      "Batch num 9640: Loss 11.020374298095703\n",
      "Batch num 9650: Loss 14.146566390991211\n",
      "Batch num 9660: Loss 11.415666580200195\n",
      "Batch num 9670: Loss 11.130542755126953\n",
      "Batch num 9680: Loss 9.714298248291016\n",
      "Batch num 9690: Loss 10.816387176513672\n",
      "Batch num 9700: Loss 11.809000015258789\n",
      "Batch num 9710: Loss 10.82851791381836\n",
      "Batch num 9720: Loss 12.470685958862305\n",
      "Batch num 9730: Loss 10.944302558898926\n",
      "Batch num 9740: Loss 10.681727409362793\n",
      "Batch num 9750: Loss 11.011590957641602\n",
      "Batch num 9760: Loss 12.051297187805176\n",
      "Batch num 9770: Loss 10.995390892028809\n",
      "Batch num 9780: Loss 10.615816116333008\n",
      "Batch num 9790: Loss 11.213590621948242\n",
      "Batch num 9800: Loss 10.975664138793945\n",
      "Batch num 9810: Loss 11.318806648254395\n",
      "Batch num 9820: Loss 9.751628875732422\n",
      "Batch num 9830: Loss 11.855712890625\n",
      "Batch num 9840: Loss 12.711517333984375\n",
      "Batch num 9850: Loss 11.105138778686523\n",
      "Batch num 9860: Loss 11.926132202148438\n",
      "Batch num 9870: Loss 10.57430362701416\n",
      "Batch num 9880: Loss 10.97844123840332\n",
      "Batch num 9890: Loss 10.13226318359375\n",
      "Batch num 9900: Loss 10.556892395019531\n",
      "Batch num 9910: Loss 11.753423690795898\n",
      "Batch num 9920: Loss 12.610125541687012\n",
      "Batch num 9930: Loss 11.809903144836426\n",
      "Batch num 9940: Loss 8.528910636901855\n",
      "Batch num 9950: Loss 9.706110000610352\n",
      "Batch num 9960: Loss 12.307327270507812\n",
      "Batch num 9970: Loss 12.517071723937988\n",
      "Batch num 9980: Loss 11.098888397216797\n",
      "Batch num 9990: Loss 12.942209243774414\n",
      "Batch num 10000: Loss 10.226921081542969\n",
      "Batch num 10010: Loss 11.638154983520508\n",
      "Batch num 10020: Loss 10.99144172668457\n",
      "Batch num 10030: Loss 11.090584754943848\n",
      "Batch num 10040: Loss 11.272512435913086\n",
      "Batch num 10050: Loss 11.152210235595703\n",
      "Batch num 10060: Loss 9.882286071777344\n",
      "Batch num 10070: Loss 12.753324508666992\n",
      "Batch num 10080: Loss 10.467098236083984\n",
      "Batch num 10090: Loss 10.360530853271484\n",
      "Batch num 10100: Loss 11.156152725219727\n",
      "Batch num 10110: Loss 10.747713088989258\n",
      "Batch num 10120: Loss 11.563678741455078\n",
      "Batch num 10130: Loss 11.480022430419922\n",
      "Batch num 10140: Loss 11.503353118896484\n",
      "Batch num 10150: Loss 12.09740924835205\n",
      "Batch num 10160: Loss 11.256216049194336\n",
      "Batch num 10170: Loss 11.969255447387695\n",
      "Batch num 10180: Loss 10.868205070495605\n",
      "Batch num 10190: Loss 11.722524642944336\n",
      "Batch num 10200: Loss 12.758779525756836\n",
      "Batch num 10210: Loss 9.423036575317383\n",
      "Batch num 10220: Loss 12.063005447387695\n",
      "Batch num 10230: Loss 11.11550521850586\n",
      "Batch num 10240: Loss 10.576297760009766\n",
      "Batch num 10250: Loss 11.522425651550293\n",
      "Batch num 10260: Loss 11.367802619934082\n",
      "Batch num 10270: Loss 9.16573715209961\n",
      "Batch num 10280: Loss 10.43338394165039\n",
      "Batch num 10290: Loss 13.132631301879883\n",
      "Batch num 10300: Loss 12.518617630004883\n",
      "Batch num 10310: Loss 11.62657642364502\n",
      "Batch num 10320: Loss 10.558794021606445\n",
      "Batch num 10330: Loss 9.634687423706055\n",
      "Batch num 10340: Loss 11.46081256866455\n",
      "Batch num 10350: Loss 16.137062072753906\n",
      "Batch num 10360: Loss 9.402168273925781\n",
      "Batch num 10370: Loss 11.09128189086914\n",
      "Batch num 10380: Loss 10.467826843261719\n",
      "Batch num 10390: Loss 8.879117965698242\n",
      "Batch num 10400: Loss 11.94913387298584\n",
      "Batch num 10410: Loss 12.832915306091309\n",
      "Batch num 10420: Loss 12.322063446044922\n",
      "Batch num 10430: Loss 13.651050567626953\n",
      "Batch num 10440: Loss 10.39880657196045\n",
      "Batch num 10450: Loss 12.398077964782715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 10460: Loss 10.858391761779785\n",
      "Batch num 10470: Loss 12.892328262329102\n",
      "Batch num 10480: Loss 8.484601020812988\n",
      "Batch num 10490: Loss 11.072135925292969\n",
      "Batch num 10500: Loss 11.677855491638184\n",
      "Batch num 10510: Loss 10.643876075744629\n",
      "Batch num 10520: Loss 11.120824813842773\n",
      "Batch num 10530: Loss 11.391757011413574\n",
      "Batch num 10540: Loss 11.082942962646484\n",
      "Batch num 10550: Loss 11.673659324645996\n",
      "Batch num 10560: Loss 10.787030220031738\n",
      "Batch num 10570: Loss 12.285197257995605\n",
      "Batch num 10580: Loss 10.833815574645996\n",
      "Batch num 10590: Loss 12.058860778808594\n",
      "Batch num 10600: Loss 11.389311790466309\n",
      "Batch num 10610: Loss 12.09605884552002\n",
      "Batch num 10620: Loss 11.568236351013184\n",
      "Batch num 10630: Loss 9.907034873962402\n",
      "Batch num 10640: Loss 11.140267372131348\n",
      "Batch num 10650: Loss 11.82919979095459\n",
      "Batch num 10660: Loss 9.665026664733887\n",
      "Batch num 10670: Loss 11.914475440979004\n",
      "Batch num 10680: Loss 13.762864112854004\n",
      "Batch num 10690: Loss 10.621342658996582\n",
      "Batch num 10700: Loss 11.579960823059082\n",
      "Batch num 10710: Loss 9.773618698120117\n",
      "Batch num 10720: Loss 12.145703315734863\n",
      "Batch num 10730: Loss 11.230620384216309\n",
      "Batch num 10740: Loss 12.81177806854248\n",
      "Batch num 10750: Loss 9.416059494018555\n",
      "Batch num 10760: Loss 12.14777946472168\n",
      "Batch num 10770: Loss 12.193195343017578\n",
      "Batch num 10780: Loss 12.838823318481445\n",
      "Batch num 10790: Loss 11.258551597595215\n",
      "Batch num 10800: Loss 10.690263748168945\n",
      "Batch num 10810: Loss 10.282234191894531\n",
      "Batch num 10820: Loss 10.723371505737305\n",
      "Batch num 10830: Loss 11.558778762817383\n",
      "Batch num 10840: Loss 12.15447998046875\n",
      "Batch num 10850: Loss 9.958508491516113\n",
      "Batch num 10860: Loss 9.787821769714355\n",
      "Batch num 10870: Loss 10.729166984558105\n",
      "Batch num 10880: Loss 14.883176803588867\n",
      "Batch num 10890: Loss 241.54132080078125\n",
      "Batch num 10900: Loss 12.316041946411133\n",
      "Batch num 10910: Loss 10.769603729248047\n",
      "Batch num 10920: Loss 11.755081176757812\n",
      "Batch num 10930: Loss 11.208447456359863\n",
      "Batch num 10940: Loss 12.297101974487305\n",
      "Batch num 10950: Loss 10.952414512634277\n",
      "Batch num 10960: Loss 12.553499221801758\n",
      "Batch num 10970: Loss 9.753900527954102\n",
      "Batch num 10980: Loss 8.5300874710083\n",
      "Batch num 10990: Loss 10.427413940429688\n",
      "Batch num 11000: Loss 11.723553657531738\n",
      "Batch num 11010: Loss 13.986804008483887\n",
      "Batch num 11020: Loss 11.619437217712402\n",
      "Batch num 11030: Loss 13.067642211914062\n",
      "Batch num 11040: Loss 12.741009712219238\n",
      "Batch num 11050: Loss 10.842754364013672\n",
      "Batch num 11060: Loss 11.306876182556152\n",
      "Batch num 11070: Loss 11.506753921508789\n",
      "Batch num 11080: Loss 11.245884895324707\n",
      "Batch num 11090: Loss 11.027734756469727\n",
      "Batch num 11100: Loss 11.128671646118164\n",
      "Batch num 11110: Loss 9.352666854858398\n",
      "Batch num 11120: Loss 11.864595413208008\n",
      "Batch num 11130: Loss 12.56765079498291\n",
      "Batch num 11140: Loss 11.538043022155762\n",
      "Batch num 11150: Loss 13.012188911437988\n",
      "Batch num 11160: Loss 9.956564903259277\n",
      "Batch num 11170: Loss 11.219573974609375\n",
      "Batch num 11180: Loss 10.763557434082031\n",
      "Batch num 11190: Loss 11.766363143920898\n",
      "Batch num 11200: Loss 10.79720401763916\n",
      "Batch num 11210: Loss 12.296131134033203\n",
      "Batch num 11220: Loss 9.793514251708984\n",
      "Batch num 11230: Loss 12.927796363830566\n",
      "Batch num 11240: Loss 10.796531677246094\n",
      "Batch num 11250: Loss 10.991667747497559\n",
      "Batch num 11260: Loss 11.99223804473877\n",
      "Batch num 11270: Loss 12.73646354675293\n",
      "Batch num 11280: Loss 12.108402252197266\n",
      "Batch num 11290: Loss 11.471321105957031\n",
      "Batch num 11300: Loss 9.60496711730957\n",
      "Batch num 11310: Loss 10.936271667480469\n",
      "Batch num 11320: Loss 10.427267074584961\n",
      "Batch num 11330: Loss 13.237320899963379\n",
      "Batch num 11340: Loss 11.500449180603027\n",
      "Batch num 11350: Loss 8.85899829864502\n",
      "Batch num 11360: Loss 11.285714149475098\n",
      "Batch num 11370: Loss 12.672872543334961\n",
      "Batch num 11380: Loss 12.424205780029297\n",
      "Batch num 11390: Loss 11.356657981872559\n",
      "Batch num 11400: Loss 11.294303894042969\n",
      "Batch num 11410: Loss 11.720281600952148\n",
      "Batch num 11420: Loss 11.057451248168945\n",
      "Batch num 11430: Loss 9.618918418884277\n",
      "Batch num 11440: Loss 10.580259323120117\n",
      "Batch num 11450: Loss 11.93703842163086\n",
      "Batch num 11460: Loss 13.478924751281738\n",
      "Batch num 11470: Loss 12.897287368774414\n",
      "Batch num 11480: Loss 10.512913703918457\n",
      "Batch num 11490: Loss 12.753358840942383\n",
      "Batch num 11500: Loss 11.363664627075195\n",
      "Batch num 11510: Loss 11.60444450378418\n",
      "Batch num 11520: Loss 9.455540657043457\n",
      "Batch num 11530: Loss 12.25223159790039\n",
      "Batch num 11540: Loss 11.29539680480957\n",
      "Batch num 11550: Loss 11.00459098815918\n",
      "Batch num 11560: Loss 12.285998344421387\n",
      "Batch num 11570: Loss 10.225454330444336\n",
      "Batch num 11580: Loss 11.231209754943848\n",
      "Batch num 11590: Loss 10.926470756530762\n",
      "Batch num 11600: Loss 14.989712715148926\n",
      "Batch num 11610: Loss 11.585650444030762\n",
      "Batch num 11620: Loss 11.881858825683594\n",
      "Batch num 11630: Loss 10.142380714416504\n",
      "Batch num 11640: Loss 12.301316261291504\n",
      "Batch num 11650: Loss 9.667013168334961\n",
      "Batch num 11660: Loss 11.085447311401367\n",
      "Batch num 11670: Loss 9.286747932434082\n",
      "Batch num 11680: Loss 11.547466278076172\n",
      "Batch num 11690: Loss 11.693917274475098\n",
      "Batch num 11700: Loss 11.266380310058594\n",
      "Batch num 11710: Loss 12.50036334991455\n",
      "Batch num 11720: Loss 67.68367004394531\n",
      "Batch num 11730: Loss 11.544329643249512\n",
      "Batch num 11740: Loss 22.96666717529297\n",
      "Batch num 11750: Loss 10.547513008117676\n",
      "Batch num 11760: Loss 28.03826904296875\n",
      "Batch num 11770: Loss 9.040410995483398\n",
      "Batch num 11780: Loss 10.267071723937988\n",
      "Batch num 11790: Loss 11.406304359436035\n",
      "Batch num 11800: Loss 10.708762168884277\n",
      "Batch num 11810: Loss 9.818948745727539\n",
      "Batch num 11820: Loss 11.815316200256348\n",
      "Batch num 11830: Loss 10.434871673583984\n",
      "Batch num 11840: Loss 12.809375762939453\n",
      "Batch num 11850: Loss 11.531886100769043\n",
      "Batch num 11860: Loss 54.01914978027344\n",
      "Batch num 11870: Loss 96.74918365478516\n",
      "Batch num 11880: Loss 20.645029067993164\n",
      "Batch num 11890: Loss 32.983970642089844\n",
      "Batch num 11900: Loss 82.40209197998047\n",
      "Batch num 11910: Loss 71.86420440673828\n",
      "Batch num 11920: Loss 11.057561874389648\n",
      "Batch num 11930: Loss 11.329748153686523\n",
      "Batch num 11940: Loss 8.220306396484375\n",
      "Batch num 11950: Loss 8.643868446350098\n",
      "Batch num 11960: Loss 8.568068504333496\n",
      "Batch num 11970: Loss 12.348402976989746\n",
      "Batch num 11980: Loss 28.28351593017578\n",
      "Batch num 11990: Loss 28.897109985351562\n",
      "Batch num 12000: Loss 12.21174144744873\n",
      "Batch num 12010: Loss 12.195286750793457\n",
      "Batch num 12020: Loss 12.372647285461426\n",
      "Batch num 12030: Loss 12.5551118850708\n",
      "Batch num 12040: Loss 10.529657363891602\n",
      "Batch num 12050: Loss 11.8263521194458\n",
      "Batch num 12060: Loss 11.873573303222656\n",
      "Batch num 12070: Loss 10.301777839660645\n",
      "Batch num 12080: Loss 10.266700744628906\n",
      "Batch num 12090: Loss 11.697983741760254\n",
      "Batch num 12100: Loss 9.783008575439453\n",
      "Batch num 12110: Loss 12.308444023132324\n",
      "Batch num 12120: Loss 10.649869918823242\n",
      "Batch num 12130: Loss 10.8816556930542\n",
      "Batch num 12140: Loss 10.278324127197266\n",
      "Batch num 12150: Loss 11.893502235412598\n",
      "Batch num 12160: Loss 11.079707145690918\n",
      "Batch num 12170: Loss 13.025030136108398\n",
      "Batch num 12180: Loss 11.06408977508545\n",
      "Batch num 12190: Loss 11.201258659362793\n",
      "Batch num 12200: Loss 10.97785758972168\n",
      "Batch num 12210: Loss 10.325986862182617\n",
      "Batch num 12220: Loss 10.69540023803711\n",
      "Batch num 12230: Loss 10.73043155670166\n",
      "Batch num 12240: Loss 11.133519172668457\n",
      "Batch num 12250: Loss 10.369550704956055\n",
      "Batch num 12260: Loss 12.403753280639648\n",
      "Batch num 12270: Loss 12.194113731384277\n",
      "Batch num 12280: Loss 11.054754257202148\n",
      "Batch num 12290: Loss 11.672220230102539\n",
      "Batch num 12300: Loss 10.858602523803711\n",
      "Batch num 12310: Loss 13.436344146728516\n",
      "Batch num 12320: Loss 11.56551456451416\n",
      "Batch num 12330: Loss 12.287177085876465\n",
      "Batch num 12340: Loss 10.619454383850098\n",
      "Batch num 12350: Loss 10.630352973937988\n",
      "Batch num 12360: Loss 9.847017288208008\n",
      "Batch num 12370: Loss 11.106568336486816\n",
      "Batch num 12380: Loss 10.55273151397705\n",
      "Batch num 12390: Loss 11.514892578125\n",
      "Batch num 12400: Loss 11.593297958374023\n",
      "Batch num 12410: Loss 11.01686954498291\n",
      "Batch num 12420: Loss 12.077188491821289\n",
      "Batch num 12430: Loss 12.015376091003418\n",
      "Batch num 12440: Loss 12.418347358703613\n",
      "Batch num 12450: Loss 9.294358253479004\n",
      "Batch num 12460: Loss 11.936178207397461\n",
      "Batch num 12470: Loss 10.619620323181152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 12480: Loss 10.077719688415527\n",
      "Batch num 12490: Loss 11.784712791442871\n",
      "Batch num 12500: Loss 12.58205509185791\n",
      "Batch num 12510: Loss 10.40953254699707\n",
      "Batch num 12520: Loss 10.646117210388184\n",
      "Batch num 12530: Loss 11.8521089553833\n",
      "Batch num 12540: Loss 11.339330673217773\n",
      "Batch num 12550: Loss 12.517513275146484\n",
      "Batch num 12560: Loss 14.443086624145508\n",
      "Batch num 12570: Loss 11.316356658935547\n",
      "Batch num 12580: Loss 9.814388275146484\n",
      "Batch num 12590: Loss 11.813817024230957\n",
      "Batch num 12600: Loss 18.65672492980957\n",
      "Batch num 12610: Loss 15.470144271850586\n",
      "Batch num 12620: Loss 12.837380409240723\n",
      "Batch num 12630: Loss 17.190393447875977\n",
      "Batch num 12640: Loss 97.02857208251953\n",
      "Batch num 12650: Loss 8.846025466918945\n",
      "Batch num 12660: Loss 11.241003036499023\n",
      "Batch num 12670: Loss 10.981300354003906\n",
      "Batch num 12680: Loss 9.915390014648438\n",
      "Batch num 12690: Loss 12.138344764709473\n",
      "Batch num 12700: Loss 11.785970687866211\n",
      "Batch num 12710: Loss 15.774094581604004\n",
      "Batch num 12720: Loss 10.965097427368164\n",
      "Batch num 12730: Loss 11.51269817352295\n",
      "Batch num 12740: Loss 7.940396785736084\n",
      "Batch num 12750: Loss 9.67608642578125\n",
      "Batch num 12760: Loss 12.383517265319824\n",
      "Batch num 12770: Loss 11.323776245117188\n",
      "Batch num 12780: Loss 12.9968843460083\n",
      "Batch num 12790: Loss 9.001091003417969\n",
      "Batch num 12800: Loss 8.583218574523926\n",
      "Batch num 12810: Loss 13.303922653198242\n",
      "Batch num 12820: Loss 11.732633590698242\n",
      "Batch num 12830: Loss 10.1514892578125\n",
      "Batch num 12840: Loss 11.26773738861084\n",
      "Batch num 12850: Loss 9.564325332641602\n",
      "Batch num 12860: Loss 10.524079322814941\n",
      "Batch num 12870: Loss 11.310006141662598\n",
      "Batch num 12880: Loss 10.495911598205566\n",
      "Batch num 12890: Loss 10.613472938537598\n",
      "Batch num 12900: Loss 10.997136116027832\n",
      "Batch num 12910: Loss 9.940441131591797\n",
      "Batch num 12920: Loss 9.592581748962402\n",
      "Batch num 12930: Loss 10.983194351196289\n",
      "Batch num 12940: Loss 29.44963836669922\n",
      "Batch num 12950: Loss 10.472647666931152\n",
      "Batch num 12960: Loss 12.225481033325195\n",
      "Batch num 12970: Loss 34.58032989501953\n",
      "Batch num 12980: Loss 47.624122619628906\n",
      "Batch num 12990: Loss 104.44599914550781\n",
      "Batch num 13000: Loss 50.904457092285156\n",
      "Batch num 13010: Loss 11.148019790649414\n",
      "Batch num 13020: Loss 11.660688400268555\n",
      "Batch num 13030: Loss 10.50964069366455\n",
      "Batch num 13040: Loss 8.369362831115723\n",
      "Batch num 13050: Loss 10.06126880645752\n",
      "Batch num 13060: Loss 9.519752502441406\n",
      "Batch num 13070: Loss 8.60665512084961\n",
      "Batch num 13080: Loss 11.435596466064453\n",
      "Batch num 13090: Loss 12.720144271850586\n",
      "Batch num 13100: Loss 13.240424156188965\n",
      "Batch num 13110: Loss 10.229674339294434\n",
      "Batch num 13120: Loss 10.305980682373047\n",
      "Batch num 13130: Loss 10.408804893493652\n",
      "Batch num 13140: Loss 11.15052318572998\n",
      "Batch num 13150: Loss 10.808755874633789\n",
      "Batch num 13160: Loss 11.253496170043945\n",
      "Batch num 13170: Loss 10.418194770812988\n",
      "Batch num 13180: Loss 9.664069175720215\n",
      "Batch num 13190: Loss 13.070082664489746\n",
      "Batch num 13200: Loss 11.273238182067871\n",
      "Batch num 13210: Loss 11.393770217895508\n",
      "Batch num 13220: Loss 9.997814178466797\n",
      "Batch num 13230: Loss 10.299931526184082\n",
      "Batch num 13240: Loss 10.866313934326172\n",
      "Batch num 13250: Loss 11.051959037780762\n",
      "Batch num 13260: Loss 10.485634803771973\n",
      "Batch num 13270: Loss 13.36167049407959\n",
      "Batch num 13280: Loss 8.946083068847656\n",
      "Batch num 13290: Loss 12.107226371765137\n",
      "Batch num 13300: Loss 12.34994125366211\n",
      "Batch num 13310: Loss 11.829412460327148\n",
      "Batch num 13320: Loss 11.074505805969238\n",
      "Batch num 13330: Loss 12.629107475280762\n",
      "Batch num 13340: Loss 10.503231048583984\n",
      "Batch num 13350: Loss 10.525742530822754\n",
      "Batch num 13360: Loss 10.852645874023438\n",
      "Batch num 13370: Loss 11.625288009643555\n",
      "Batch num 13380: Loss 10.756735801696777\n",
      "Batch num 13390: Loss 28.547048568725586\n",
      "Batch num 13400: Loss 11.90455436706543\n",
      "Batch num 13410: Loss 46.24474334716797\n",
      "Batch num 13420: Loss 33.241615295410156\n",
      "Batch num 13430: Loss 10.01550579071045\n",
      "Batch num 13440: Loss 16.3050537109375\n",
      "Batch num 13450: Loss 10.829620361328125\n",
      "Batch num 13460: Loss 11.623626708984375\n",
      "Batch num 13470: Loss 10.591967582702637\n",
      "Batch num 13480: Loss 13.636933326721191\n",
      "Batch num 13490: Loss 11.696742057800293\n",
      "Batch num 13500: Loss 9.383209228515625\n",
      "Batch num 13510: Loss 9.958148002624512\n",
      "Batch num 13520: Loss 10.993557929992676\n",
      "Batch num 13530: Loss 12.430938720703125\n",
      "Batch num 13540: Loss 11.836320877075195\n",
      "Batch num 13550: Loss 10.830206871032715\n",
      "Batch num 13560: Loss 16.987781524658203\n",
      "Batch num 13570: Loss 17.723339080810547\n",
      "Batch num 13580: Loss 10.654251098632812\n",
      "Batch num 13590: Loss 10.550505638122559\n",
      "Batch num 13600: Loss 11.395110130310059\n",
      "Batch num 13610: Loss 10.540197372436523\n",
      "Batch num 13620: Loss 9.473408699035645\n",
      "Batch num 13630: Loss 10.888139724731445\n",
      "Batch num 13640: Loss 11.510737419128418\n",
      "Batch num 13650: Loss 17.183374404907227\n",
      "Batch num 13660: Loss 9.608175277709961\n",
      "Batch num 13670: Loss 13.067028999328613\n",
      "Batch num 13680: Loss 11.518716812133789\n",
      "Batch num 13690: Loss 11.459016799926758\n",
      "Batch num 13700: Loss 58.5936393737793\n",
      "Batch num 13710: Loss 11.14391803741455\n",
      "Batch num 13720: Loss 10.981313705444336\n",
      "Batch num 13730: Loss 12.860902786254883\n",
      "Batch num 13740: Loss 10.724048614501953\n",
      "Batch num 13750: Loss 10.844367027282715\n",
      "Batch num 13760: Loss 11.649773597717285\n",
      "Batch num 13770: Loss 10.717731475830078\n",
      "Batch num 13780: Loss 10.907182693481445\n",
      "Batch num 13790: Loss 10.598357200622559\n",
      "Batch num 13800: Loss 11.084692001342773\n",
      "Batch num 13810: Loss 11.179539680480957\n",
      "Batch num 13820: Loss 11.375208854675293\n",
      "Batch num 13830: Loss 12.945124626159668\n",
      "Batch num 13840: Loss 11.038274765014648\n",
      "Batch num 13850: Loss 10.044909477233887\n",
      "Batch num 13860: Loss 6.805497169494629\n",
      "Batch num 13870: Loss 10.844063758850098\n",
      "Batch num 13880: Loss 10.132094383239746\n",
      "Batch num 13890: Loss 10.872235298156738\n",
      "Batch num 13900: Loss 11.851984977722168\n",
      "Batch num 13910: Loss 10.524513244628906\n",
      "Batch num 13920: Loss 12.91878890991211\n",
      "Batch num 13930: Loss 10.98926830291748\n",
      "Batch num 13940: Loss 9.017502784729004\n",
      "Batch num 13950: Loss 13.645015716552734\n",
      "Batch num 13960: Loss 11.402952194213867\n",
      "Batch num 13970: Loss 9.3337984085083\n",
      "Batch num 13980: Loss 10.450126647949219\n",
      "Batch num 13990: Loss 10.00487232208252\n",
      "Batch num 14000: Loss 10.115032196044922\n",
      "Batch num 14010: Loss 10.865042686462402\n",
      "Batch num 14020: Loss 11.805744171142578\n",
      "Batch num 14030: Loss 10.42248249053955\n",
      "Batch num 14040: Loss 10.857793807983398\n",
      "Batch num 14050: Loss 11.430206298828125\n",
      "Batch num 14060: Loss 11.415300369262695\n",
      "Batch num 14070: Loss 10.959887504577637\n",
      "Batch num 14080: Loss 10.17018985748291\n",
      "Batch num 14090: Loss 10.57496166229248\n",
      "Batch num 14100: Loss 9.611222267150879\n",
      "Batch num 14110: Loss 11.099499702453613\n",
      "Batch num 14120: Loss 11.454206466674805\n",
      "Batch num 14130: Loss 10.556203842163086\n",
      "Batch num 14140: Loss 9.337847709655762\n",
      "Batch num 14150: Loss 14.503819465637207\n",
      "Batch num 14160: Loss 13.559433937072754\n",
      "Batch num 14170: Loss 11.2783842086792\n",
      "Batch num 14180: Loss 11.53612232208252\n",
      "Batch num 14190: Loss 9.97979736328125\n",
      "Batch num 14200: Loss 10.206920623779297\n",
      "Batch num 14210: Loss 10.75629711151123\n",
      "Batch num 14220: Loss 14.66231632232666\n",
      "Batch num 14230: Loss 9.643402099609375\n",
      "Batch num 14240: Loss 10.00234317779541\n",
      "Batch num 14250: Loss 12.142992973327637\n",
      "Batch num 14260: Loss 13.520203590393066\n",
      "Batch num 14270: Loss 11.410195350646973\n",
      "Batch num 14280: Loss 9.048048973083496\n",
      "Batch num 14290: Loss 11.938969612121582\n",
      "Batch num 14300: Loss 12.8031644821167\n",
      "Batch num 14310: Loss 10.497184753417969\n",
      "Batch num 14320: Loss 10.260285377502441\n",
      "Batch num 14330: Loss 10.696184158325195\n",
      "Batch num 14340: Loss 10.12483024597168\n",
      "Batch num 14350: Loss 11.221319198608398\n",
      "Batch num 14360: Loss 10.266375541687012\n",
      "Batch num 14370: Loss 12.688051223754883\n",
      "Batch num 14380: Loss 11.387072563171387\n",
      "Batch num 14390: Loss 10.145052909851074\n",
      "Batch num 14400: Loss 11.027387619018555\n",
      "Batch num 14410: Loss 11.331554412841797\n",
      "Batch num 14420: Loss 10.009605407714844\n",
      "Batch num 14430: Loss 14.248007774353027\n",
      "Batch num 14440: Loss 10.7914400100708\n",
      "Batch num 14450: Loss 12.379571914672852\n",
      "Batch num 14460: Loss 10.493721961975098\n",
      "Batch num 14470: Loss 12.33455753326416\n",
      "Batch num 14480: Loss 10.069561958312988\n",
      "Batch num 14490: Loss 11.09544563293457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 14500: Loss 12.376367568969727\n",
      "Batch num 14510: Loss 10.430978775024414\n",
      "Batch num 14520: Loss 9.739832878112793\n",
      "Batch num 14530: Loss 10.955766677856445\n",
      "Batch num 14540: Loss 13.158984184265137\n",
      "Batch num 14550: Loss 8.585968017578125\n",
      "Batch num 14560: Loss 10.772320747375488\n",
      "Batch num 14570: Loss 11.195357322692871\n",
      "Batch num 14580: Loss 10.202375411987305\n",
      "Batch num 14590: Loss 13.0786714553833\n",
      "Batch num 14600: Loss 10.733305931091309\n",
      "Batch num 14610: Loss 10.88233757019043\n",
      "Batch num 14620: Loss 10.749368667602539\n",
      "Batch num 14630: Loss 10.338922500610352\n",
      "Batch num 14640: Loss 11.345375061035156\n",
      "Batch num 14650: Loss 10.935029983520508\n",
      "Batch num 14660: Loss 11.676665306091309\n",
      "Batch num 14670: Loss 11.173802375793457\n",
      "Batch num 14680: Loss 9.076919555664062\n",
      "Batch num 14690: Loss 10.405904769897461\n",
      "Batch num 14700: Loss 10.774101257324219\n",
      "Batch num 14710: Loss 9.752720832824707\n",
      "Batch num 14720: Loss 10.99216079711914\n",
      "Batch num 14730: Loss 12.298288345336914\n",
      "Batch num 14740: Loss 10.788494110107422\n",
      "Batch num 14750: Loss 11.698891639709473\n",
      "Batch num 14760: Loss 12.76928424835205\n",
      "Batch num 14770: Loss 11.290501594543457\n",
      "Batch num 14780: Loss 12.724143028259277\n",
      "Batch num 14790: Loss 10.703160285949707\n",
      "Batch num 14800: Loss 10.309146881103516\n",
      "Batch num 14810: Loss 11.564323425292969\n",
      "Batch num 14820: Loss 9.84931468963623\n",
      "Batch num 14830: Loss 12.247292518615723\n",
      "Batch num 14840: Loss 11.567782402038574\n",
      "Batch num 14850: Loss 11.322540283203125\n",
      "Batch num 14860: Loss 9.682750701904297\n",
      "Batch num 14870: Loss 13.588372230529785\n",
      "Batch num 14880: Loss 10.801608085632324\n",
      "Batch num 14890: Loss 9.37488079071045\n",
      "Batch num 14900: Loss 11.125886917114258\n",
      "Batch num 14910: Loss 10.955419540405273\n",
      "Batch num 14920: Loss 10.131885528564453\n",
      "Batch num 14930: Loss 11.039230346679688\n",
      "Batch num 14940: Loss 12.284924507141113\n",
      "Batch num 14950: Loss 11.898981094360352\n",
      "Batch num 14960: Loss 12.866475105285645\n",
      "Batch num 14970: Loss 10.155731201171875\n",
      "Batch num 14980: Loss 10.94214153289795\n",
      "Batch num 14990: Loss 10.965680122375488\n",
      "Batch num 15000: Loss 10.215566635131836\n",
      "Batch num 15010: Loss 11.0927152633667\n",
      "Batch num 15020: Loss 12.053550720214844\n",
      "Batch num 15030: Loss 11.665764808654785\n",
      "Batch num 15040: Loss 11.542745590209961\n",
      "Batch num 15050: Loss 11.0598726272583\n",
      "Batch num 15060: Loss 10.301141738891602\n",
      "Batch num 15070: Loss 10.408801078796387\n",
      "Batch num 15080: Loss 12.3821439743042\n",
      "Batch num 15090: Loss 11.499802589416504\n",
      "Batch num 15100: Loss 10.794205665588379\n",
      "Batch num 15110: Loss 11.984752655029297\n",
      "Batch num 15120: Loss 12.153788566589355\n",
      "Batch num 15130: Loss 9.617609024047852\n",
      "Batch num 15140: Loss 9.00991153717041\n",
      "Batch num 15150: Loss 8.588075637817383\n",
      "Batch num 15160: Loss 12.377302169799805\n",
      "Batch num 15170: Loss 10.161720275878906\n",
      "Batch num 15180: Loss 11.654644012451172\n",
      "Batch num 15190: Loss 13.26054859161377\n",
      "Batch num 15200: Loss 11.161227226257324\n",
      "Batch num 15210: Loss 10.339156150817871\n",
      "Batch num 15220: Loss 11.314270973205566\n",
      "Batch num 15230: Loss 10.114012718200684\n",
      "Batch num 15240: Loss 11.316828727722168\n",
      "Batch num 15250: Loss 10.325907707214355\n",
      "Batch num 15260: Loss 11.63282299041748\n",
      "Batch num 15270: Loss 11.284271240234375\n",
      "Batch num 15280: Loss 12.275498390197754\n",
      "Batch num 15290: Loss 11.572402000427246\n",
      "Batch num 15300: Loss 11.556856155395508\n",
      "Batch num 15310: Loss 11.02674674987793\n",
      "Batch num 15320: Loss 11.201347351074219\n",
      "Batch num 15330: Loss 8.586186408996582\n",
      "Batch num 15340: Loss 12.048320770263672\n",
      "Batch num 15350: Loss 10.798250198364258\n",
      "Batch num 15360: Loss 12.430514335632324\n",
      "Batch num 15370: Loss 10.96471118927002\n",
      "Batch num 15380: Loss 10.67221736907959\n",
      "Batch num 15390: Loss 11.131377220153809\n",
      "Batch num 15400: Loss 10.770611763000488\n",
      "Batch num 15410: Loss 9.871171951293945\n",
      "Batch num 15420: Loss 9.883923530578613\n",
      "Batch num 15430: Loss 11.405797004699707\n",
      "Batch num 15440: Loss 10.092509269714355\n",
      "Batch num 15450: Loss 11.030997276306152\n",
      "Batch num 15460: Loss 9.70536994934082\n",
      "Batch num 15470: Loss 11.307840347290039\n",
      "Batch num 15480: Loss 11.220544815063477\n",
      "Batch num 15490: Loss 11.036839485168457\n",
      "Batch num 15500: Loss 11.588723182678223\n",
      "Batch num 15510: Loss 12.087597846984863\n",
      "Batch num 15520: Loss 11.633359909057617\n",
      "Batch num 15530: Loss 11.461343765258789\n",
      "Batch num 15540: Loss 11.3599214553833\n",
      "Batch num 15550: Loss 11.820819854736328\n",
      "Batch num 15560: Loss 11.394353866577148\n",
      "Batch num 15570: Loss 9.608538627624512\n",
      "Batch num 15580: Loss 8.50409984588623\n",
      "Batch num 15590: Loss 8.036505699157715\n",
      "Batch num 15600: Loss 12.288281440734863\n",
      "Batch num 15610: Loss 12.347498893737793\n",
      "Batch num 15620: Loss 11.491119384765625\n",
      "Batch num 15630: Loss 11.245582580566406\n",
      "Batch num 15640: Loss 9.97080135345459\n",
      "Batch num 15650: Loss 11.335392951965332\n",
      "Batch num 15660: Loss 12.070453643798828\n",
      "Batch num 15670: Loss 10.744811058044434\n",
      "Batch num 15680: Loss 11.543951988220215\n",
      "Batch num 15690: Loss 10.162371635437012\n",
      "Batch num 15700: Loss 11.123503684997559\n",
      "Batch num 15710: Loss 12.706331253051758\n",
      "Batch num 15720: Loss 12.510993957519531\n",
      "Batch num 15730: Loss 11.56191635131836\n",
      "Batch num 15740: Loss 10.765185356140137\n",
      "Batch num 15750: Loss 11.04161262512207\n",
      "Batch num 15760: Loss 12.600529670715332\n",
      "Batch num 15770: Loss 11.793800354003906\n",
      "Batch num 15780: Loss 12.21273136138916\n",
      "Batch num 15790: Loss 9.406848907470703\n",
      "Batch num 15800: Loss 10.623176574707031\n",
      "Batch num 15810: Loss 11.38832950592041\n",
      "Batch num 15820: Loss 10.139707565307617\n",
      "Batch num 15830: Loss 10.841196060180664\n",
      "Batch num 15840: Loss 11.331707000732422\n",
      "Batch num 15850: Loss 9.068899154663086\n",
      "Batch num 15860: Loss 10.122553825378418\n",
      "Batch num 15870: Loss 10.721230506896973\n",
      "Batch num 15880: Loss 11.564229011535645\n",
      "Batch num 15890: Loss 10.389147758483887\n",
      "Batch num 15900: Loss 11.136962890625\n",
      "Batch num 15910: Loss 10.621129035949707\n",
      "Batch num 15920: Loss 10.528626441955566\n",
      "Batch num 15930: Loss 11.38273811340332\n",
      "Batch num 15940: Loss 11.616859436035156\n",
      "Batch num 15950: Loss 10.500462532043457\n",
      "Batch num 15960: Loss 9.79054069519043\n",
      "Batch num 15970: Loss 9.209722518920898\n",
      "Batch num 15980: Loss 10.266449928283691\n",
      "Batch num 15990: Loss 11.00373363494873\n",
      "Batch num 16000: Loss 10.844026565551758\n",
      "Batch num 16010: Loss 12.753165245056152\n",
      "Batch num 16020: Loss 12.35509204864502\n",
      "Batch num 16030: Loss 11.803217887878418\n",
      "Batch num 16040: Loss 11.655747413635254\n",
      "Batch num 16050: Loss 9.700459480285645\n",
      "Batch num 16060: Loss 10.468494415283203\n",
      "Batch num 16070: Loss 10.273706436157227\n",
      "Batch num 16080: Loss 10.704166412353516\n",
      "Batch num 16090: Loss 9.767179489135742\n",
      "Batch num 16100: Loss 10.08846378326416\n",
      "Batch num 16110: Loss 11.384096145629883\n",
      "Batch num 16120: Loss 9.942756652832031\n",
      "Batch num 16130: Loss 9.875414848327637\n",
      "Batch num 16140: Loss 9.703993797302246\n",
      "Batch num 16150: Loss 9.685192108154297\n",
      "Batch num 16160: Loss 10.620492935180664\n",
      "Batch num 16170: Loss 10.506332397460938\n",
      "Batch num 16180: Loss 10.702767372131348\n",
      "Batch num 16190: Loss 10.627856254577637\n",
      "Batch num 16200: Loss 11.387182235717773\n",
      "Batch num 16210: Loss 9.689090728759766\n",
      "Batch num 16220: Loss 11.620864868164062\n",
      "Batch num 16230: Loss 10.715804100036621\n",
      "Batch num 16240: Loss 13.417153358459473\n",
      "Batch num 16250: Loss 10.30116081237793\n",
      "Batch num 16260: Loss 9.98672103881836\n",
      "Batch num 16270: Loss 11.033712387084961\n",
      "Batch num 16280: Loss 10.897855758666992\n",
      "Batch num 16290: Loss 12.213593482971191\n",
      "Batch num 16300: Loss 11.279248237609863\n",
      "Batch num 16310: Loss 9.618642807006836\n",
      "Batch num 16320: Loss 11.991706848144531\n",
      "Batch num 16330: Loss 11.13166618347168\n",
      "Batch num 16340: Loss 10.339590072631836\n",
      "Batch num 16350: Loss 12.17542839050293\n",
      "Batch num 16360: Loss 9.957046508789062\n",
      "Batch num 16370: Loss 11.903813362121582\n",
      "Batch num 16380: Loss 10.136113166809082\n",
      "Batch num 16390: Loss 11.082616806030273\n",
      "Batch num 16400: Loss 11.592190742492676\n",
      "Batch num 16410: Loss 9.37682056427002\n",
      "Batch num 16420: Loss 13.959312438964844\n",
      "Batch num 16430: Loss 9.494500160217285\n",
      "Batch num 16440: Loss 12.442028045654297\n",
      "Batch num 16450: Loss 11.975486755371094\n",
      "Batch num 16460: Loss 12.661112785339355\n",
      "Batch num 16470: Loss 11.496828079223633\n",
      "Batch num 16480: Loss 11.45889663696289\n",
      "Batch num 16490: Loss 10.732852935791016\n",
      "Batch num 16500: Loss 9.020771026611328\n",
      "Batch num 16510: Loss 11.350393295288086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 16520: Loss 12.065289497375488\n",
      "Batch num 16530: Loss 11.177978515625\n",
      "Batch num 16540: Loss 10.480552673339844\n",
      "Batch num 16550: Loss 10.095067977905273\n",
      "Batch num 16560: Loss 12.663717269897461\n",
      "Batch num 16570: Loss 11.056676864624023\n",
      "Batch num 16580: Loss 9.329103469848633\n",
      "Batch num 16590: Loss 12.764383316040039\n",
      "Batch num 16600: Loss 9.365935325622559\n",
      "Batch num 16610: Loss 9.27983283996582\n",
      "Batch num 16620: Loss 10.032051086425781\n",
      "Batch num 16630: Loss 10.627384185791016\n",
      "Batch num 16640: Loss 9.567373275756836\n",
      "Batch num 16650: Loss 11.837615013122559\n",
      "Batch num 16660: Loss 13.02377700805664\n",
      "Batch num 16670: Loss 13.262388229370117\n",
      "Batch num 16680: Loss 10.321815490722656\n",
      "Batch num 16690: Loss 10.02649211883545\n",
      "Batch num 16700: Loss 10.78988265991211\n",
      "Batch num 16710: Loss 10.469772338867188\n",
      "Batch num 16720: Loss 11.659978866577148\n",
      "Batch num 16730: Loss 10.459527969360352\n",
      "Batch num 16740: Loss 10.90418815612793\n",
      "Batch num 16750: Loss 11.280086517333984\n",
      "Batch num 16760: Loss 11.238247871398926\n",
      "Batch num 16770: Loss 9.579549789428711\n",
      "Batch num 16780: Loss 10.092724800109863\n",
      "Batch num 16790: Loss 10.465611457824707\n",
      "Batch num 16800: Loss 12.86524486541748\n",
      "Batch num 16810: Loss 11.103466033935547\n",
      "Batch num 16820: Loss 11.820106506347656\n",
      "Batch num 16830: Loss 8.636034965515137\n",
      "Batch num 16840: Loss 11.705103874206543\n",
      "Batch num 16850: Loss 11.415515899658203\n",
      "Batch num 16860: Loss 11.226966857910156\n",
      "Batch num 16870: Loss 11.419647216796875\n",
      "Batch num 16880: Loss 10.768768310546875\n",
      "Batch num 16890: Loss 11.474372863769531\n",
      "Batch num 16900: Loss 11.706222534179688\n",
      "Batch num 16910: Loss 11.57044506072998\n",
      "Batch num 16920: Loss 12.001338958740234\n",
      "Batch num 16930: Loss 10.971638679504395\n",
      "Batch num 16940: Loss 10.389968872070312\n",
      "Batch num 16950: Loss 9.843709945678711\n",
      "Batch num 16960: Loss 13.41064453125\n",
      "Batch num 16970: Loss 9.998393058776855\n",
      "Batch num 16980: Loss 12.0404052734375\n",
      "Batch num 16990: Loss 10.716571807861328\n",
      "Batch num 17000: Loss 10.80785083770752\n",
      "Batch num 17010: Loss 9.996414184570312\n",
      "Batch num 17020: Loss 11.087435722351074\n",
      "Batch num 17030: Loss 9.896860122680664\n",
      "Batch num 17040: Loss 10.977846145629883\n",
      "Batch num 17050: Loss 10.792339324951172\n",
      "Batch num 17060: Loss 12.1069917678833\n",
      "Batch num 17070: Loss 10.45736026763916\n",
      "Batch num 17080: Loss 12.37569522857666\n",
      "Batch num 17090: Loss 10.035335540771484\n",
      "Batch num 17100: Loss 8.780410766601562\n",
      "Batch num 17110: Loss 10.809770584106445\n",
      "Batch num 17120: Loss 11.193552017211914\n",
      "Batch num 17130: Loss 10.531688690185547\n",
      "Batch num 17140: Loss 10.040243148803711\n",
      "Batch num 17150: Loss 9.087603569030762\n",
      "Batch num 17160: Loss 10.384483337402344\n",
      "Batch num 17170: Loss 13.561178207397461\n",
      "Batch num 17180: Loss 10.413622856140137\n",
      "Batch num 17190: Loss 8.767669677734375\n",
      "Batch num 17200: Loss 11.9117431640625\n",
      "Batch num 17210: Loss 10.994767189025879\n",
      "Batch num 17220: Loss 12.73527717590332\n",
      "Batch num 17230: Loss 11.288973808288574\n",
      "Batch num 17240: Loss 11.217456817626953\n",
      "Batch num 17250: Loss 11.016179084777832\n",
      "Batch num 17260: Loss 11.204541206359863\n",
      "Batch num 17270: Loss 9.658185958862305\n",
      "Batch num 17280: Loss 8.954578399658203\n",
      "Batch num 17290: Loss 10.243310928344727\n",
      "Batch num 17300: Loss 10.856271743774414\n",
      "Batch num 17310: Loss 10.186201095581055\n",
      "Batch num 17320: Loss 9.783147811889648\n",
      "Batch num 17330: Loss 10.050714492797852\n",
      "Batch num 17340: Loss 9.672449111938477\n",
      "Batch num 17350: Loss 8.683502197265625\n",
      "Batch num 17360: Loss 10.942140579223633\n",
      "Batch num 17370: Loss 10.229042053222656\n",
      "Batch num 17380: Loss 8.364583969116211\n",
      "Batch num 17390: Loss 10.837369918823242\n",
      "Batch num 17400: Loss 10.655662536621094\n",
      "Batch num 17410: Loss 8.357041358947754\n",
      "Batch num 17420: Loss 10.376838684082031\n",
      "Batch num 17430: Loss 11.096696853637695\n",
      "Batch num 17440: Loss 11.34975528717041\n",
      "Batch num 17450: Loss 11.901466369628906\n",
      "Batch num 17460: Loss 11.703437805175781\n",
      "Batch num 17470: Loss 10.98653507232666\n",
      "Batch num 17480: Loss 10.943193435668945\n",
      "Batch num 17490: Loss 8.488813400268555\n",
      "Batch num 17500: Loss 12.003814697265625\n",
      "Batch num 17510: Loss 10.169288635253906\n",
      "Batch num 17520: Loss 11.093996047973633\n",
      "Batch num 17530: Loss 11.084404945373535\n",
      "Batch num 17540: Loss 9.86176586151123\n",
      "Batch num 17550: Loss 10.394660949707031\n",
      "Batch num 17560: Loss 10.268037796020508\n",
      "Batch num 17570: Loss 9.18228816986084\n",
      "Batch num 17580: Loss 11.819212913513184\n",
      "Batch num 17590: Loss 9.786392211914062\n",
      "Batch num 17600: Loss 12.07733154296875\n",
      "Batch num 17610: Loss 9.90211296081543\n",
      "Batch num 17620: Loss 10.77505874633789\n",
      "Batch num 17630: Loss 12.995697975158691\n",
      "Batch num 17640: Loss 10.413488388061523\n",
      "Batch num 17650: Loss 10.678560256958008\n",
      "Batch num 17660: Loss 10.814619064331055\n",
      "Batch num 17670: Loss 11.110340118408203\n",
      "Batch num 17680: Loss 10.453478813171387\n",
      "Batch num 17690: Loss 11.11038589477539\n",
      "Batch num 17700: Loss 10.943836212158203\n",
      "Batch num 17710: Loss 10.882123947143555\n",
      "Batch num 17720: Loss 9.750204086303711\n",
      "Batch num 17730: Loss 10.607596397399902\n",
      "Batch num 17740: Loss 10.965847969055176\n",
      "Batch num 17750: Loss 11.251588821411133\n",
      "Batch num 17760: Loss 9.427469253540039\n",
      "Batch num 17770: Loss 9.839950561523438\n",
      "Batch num 17780: Loss 11.574756622314453\n",
      "Batch num 17790: Loss 11.462303161621094\n",
      "Batch num 17800: Loss 9.755239486694336\n",
      "Batch num 17810: Loss 10.40518569946289\n",
      "Batch num 17820: Loss 11.215255737304688\n",
      "Batch num 17830: Loss 10.57962703704834\n",
      "Batch num 17840: Loss 8.918615341186523\n",
      "Batch num 17850: Loss 12.323192596435547\n",
      "Batch num 17860: Loss 10.75973129272461\n",
      "Batch num 17870: Loss 11.21383285522461\n",
      "Batch num 17880: Loss 10.988163948059082\n",
      "Batch num 17890: Loss 10.46811294555664\n",
      "Batch num 17900: Loss 16.372900009155273\n",
      "Batch num 17910: Loss 20.175888061523438\n",
      "Batch num 17920: Loss 10.182830810546875\n",
      "Batch num 17930: Loss 17.306148529052734\n",
      "Batch num 17940: Loss 9.486471176147461\n",
      "Batch num 17950: Loss 12.450679779052734\n",
      "Batch num 17960: Loss 10.852206230163574\n",
      "Batch num 17970: Loss 10.96007251739502\n",
      "Batch num 17980: Loss 9.20187759399414\n",
      "Batch num 17990: Loss 9.920852661132812\n",
      "Batch num 18000: Loss 11.001083374023438\n",
      "Batch num 18010: Loss 12.447214126586914\n",
      "Batch num 18020: Loss 11.9764986038208\n",
      "Batch num 18030: Loss 10.16791820526123\n",
      "Batch num 18040: Loss 11.621101379394531\n",
      "Batch num 18050: Loss 11.346059799194336\n",
      "Batch num 18060: Loss 12.035064697265625\n",
      "Batch num 18070: Loss 10.818550109863281\n",
      "Batch num 18080: Loss 10.612854957580566\n",
      "Batch num 18090: Loss 11.310412406921387\n",
      "Batch num 18100: Loss 11.113922119140625\n",
      "Batch num 18110: Loss 9.405529022216797\n",
      "Batch num 18120: Loss 11.692606925964355\n",
      "Batch num 18130: Loss 12.34055233001709\n",
      "Batch num 18140: Loss 12.582987785339355\n",
      "Batch num 18150: Loss 8.966634750366211\n",
      "Batch num 18160: Loss 12.043211936950684\n",
      "Batch num 18170: Loss 11.502409934997559\n",
      "Batch num 18180: Loss 11.07144832611084\n",
      "Batch num 18190: Loss 11.3370943069458\n",
      "Batch num 18200: Loss 11.192447662353516\n",
      "Batch num 18210: Loss 10.708637237548828\n",
      "Batch num 18220: Loss 9.546504974365234\n",
      "Batch num 18230: Loss 9.194711685180664\n",
      "Batch num 18240: Loss 12.331684112548828\n",
      "Batch num 18250: Loss 11.370918273925781\n",
      "Batch num 18260: Loss 10.620157241821289\n",
      "Batch num 18270: Loss 9.57541561126709\n",
      "Batch num 18280: Loss 12.227920532226562\n",
      "Batch num 18290: Loss 10.08756160736084\n",
      "Batch num 18300: Loss 10.465505599975586\n",
      "Batch num 18310: Loss 10.472280502319336\n",
      "Batch num 18320: Loss 9.892802238464355\n",
      "Batch num 18330: Loss 11.20129108428955\n",
      "Batch num 18340: Loss 11.524469375610352\n",
      "Batch num 18350: Loss 10.354790687561035\n",
      "Batch num 18360: Loss 10.260214805603027\n",
      "Batch num 18370: Loss 10.852734565734863\n",
      "Batch num 18380: Loss 9.55958366394043\n",
      "Batch num 18390: Loss 10.156965255737305\n",
      "Batch num 18400: Loss 10.313993453979492\n",
      "Batch num 18410: Loss 10.17177963256836\n",
      "Batch num 18420: Loss 9.40947437286377\n",
      "Batch num 18430: Loss 10.70628833770752\n",
      "Batch num 18440: Loss 10.169601440429688\n",
      "Batch num 18450: Loss 10.476419448852539\n",
      "Batch num 18460: Loss 8.781709671020508\n",
      "Batch num 18470: Loss 9.581668853759766\n",
      "Batch num 18480: Loss 10.634207725524902\n",
      "Batch num 18490: Loss 9.949109077453613\n",
      "Batch num 18500: Loss 10.970240592956543\n",
      "Batch num 18510: Loss 11.18000316619873\n",
      "Batch num 18520: Loss 10.312638282775879\n",
      "Batch num 18530: Loss 12.651140213012695\n",
      "Batch num 18540: Loss 9.270286560058594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 18550: Loss 11.646556854248047\n",
      "Batch num 18560: Loss 11.047597885131836\n",
      "Batch num 18570: Loss 9.894384384155273\n",
      "Batch num 18580: Loss 11.46529769897461\n",
      "Batch num 18590: Loss 10.82190227508545\n",
      "Batch num 18600: Loss 11.083831787109375\n",
      "Batch num 18610: Loss 12.366808891296387\n",
      "Batch num 18620: Loss 11.067879676818848\n",
      "Batch num 18630: Loss 9.013800621032715\n",
      "Batch num 18640: Loss 10.119670867919922\n",
      "Batch num 18650: Loss 10.83991813659668\n",
      "Batch num 18660: Loss 10.47279167175293\n",
      "Batch num 18670: Loss 11.414790153503418\n",
      "Batch num 18680: Loss 12.132226943969727\n",
      "Batch num 18690: Loss 12.242347717285156\n",
      "Batch num 18700: Loss 10.281196594238281\n",
      "Batch num 18710: Loss 11.191089630126953\n",
      "Batch num 18720: Loss 11.255404472351074\n",
      "Batch num 18730: Loss 10.345362663269043\n",
      "Batch num 18740: Loss 11.25413703918457\n",
      "Batch num 18750: Loss 10.909553527832031\n",
      "Batch num 18760: Loss 10.350628852844238\n",
      "Batch num 18770: Loss 10.78247356414795\n",
      "Batch num 18780: Loss 10.935955047607422\n",
      "Batch num 18790: Loss 10.559554100036621\n",
      "Batch num 18800: Loss 11.030355453491211\n",
      "Batch num 18810: Loss 11.01696491241455\n",
      "Batch num 18820: Loss 10.112629890441895\n",
      "Batch num 18830: Loss 10.766260147094727\n",
      "Batch num 18840: Loss 12.712919235229492\n",
      "Batch num 18850: Loss 10.197985649108887\n",
      "Batch num 18860: Loss 11.649067878723145\n",
      "Batch num 18870: Loss 11.08090877532959\n",
      "Batch num 18880: Loss 10.50711441040039\n",
      "Batch num 18890: Loss 8.956281661987305\n",
      "Batch num 18900: Loss 13.032132148742676\n",
      "Batch num 18910: Loss 9.608525276184082\n",
      "Batch num 18920: Loss 11.451679229736328\n",
      "Batch num 18930: Loss 10.885436058044434\n",
      "Batch num 18940: Loss 8.850241661071777\n",
      "Batch num 18950: Loss 10.373846054077148\n",
      "Batch num 18960: Loss 10.657549858093262\n",
      "Batch num 18970: Loss 10.594958305358887\n",
      "Batch num 18980: Loss 11.419530868530273\n",
      "Batch num 18990: Loss 9.575542449951172\n",
      "Batch num 19000: Loss 9.14753246307373\n",
      "Batch num 19010: Loss 12.508861541748047\n",
      "Batch num 19020: Loss 10.119207382202148\n",
      "Batch num 19030: Loss 11.671987533569336\n",
      "Batch num 19040: Loss 11.48763370513916\n",
      "Batch num 19050: Loss 11.323578834533691\n",
      "Batch num 19060: Loss 13.516636848449707\n",
      "Batch num 19070: Loss 11.312726974487305\n",
      "Batch num 19080: Loss 9.758179664611816\n",
      "Batch num 19090: Loss 10.332198143005371\n",
      "Batch num 19100: Loss 12.661559104919434\n",
      "Batch num 19110: Loss 11.066097259521484\n",
      "Batch num 19120: Loss 8.097829818725586\n",
      "Batch num 19130: Loss 11.765692710876465\n",
      "Batch num 19140: Loss 11.791946411132812\n",
      "Batch num 19150: Loss 9.89848518371582\n",
      "Batch num 19160: Loss 10.213459014892578\n",
      "Batch num 19170: Loss 10.231389999389648\n",
      "Batch num 19180: Loss 11.35348892211914\n",
      "Batch num 19190: Loss 10.940130233764648\n",
      "Batch num 19200: Loss 10.773435592651367\n",
      "Batch num 19210: Loss 9.179546356201172\n",
      "Batch num 19220: Loss 10.05454158782959\n",
      "Batch num 19230: Loss 9.82290267944336\n",
      "Batch num 19240: Loss 10.03065013885498\n",
      "Batch num 19250: Loss 7.979605197906494\n",
      "Batch num 19260: Loss 12.714815139770508\n",
      "Batch num 19270: Loss 9.396021842956543\n",
      "Batch num 19280: Loss 12.401843070983887\n",
      "Batch num 19290: Loss 10.330981254577637\n",
      "Batch num 19300: Loss 10.615022659301758\n",
      "Batch num 19310: Loss 11.334563255310059\n",
      "Batch num 19320: Loss 9.993468284606934\n",
      "Batch num 19330: Loss 10.630013465881348\n",
      "Batch num 19340: Loss 11.161507606506348\n",
      "Batch num 19350: Loss 9.636151313781738\n",
      "Batch num 19360: Loss 9.38595962524414\n",
      "Batch num 19370: Loss 10.611857414245605\n",
      "Batch num 19380: Loss 9.775350570678711\n",
      "Batch num 19390: Loss 12.38708209991455\n",
      "Batch num 19400: Loss 9.839008331298828\n",
      "Batch num 19410: Loss 11.007847785949707\n",
      "Batch num 19420: Loss 8.768572807312012\n",
      "Batch num 19430: Loss 9.851366996765137\n",
      "Batch num 19440: Loss 10.826211929321289\n",
      "Batch num 19450: Loss 10.789170265197754\n",
      "Batch num 19460: Loss 11.406676292419434\n",
      "Batch num 19470: Loss 10.796606063842773\n",
      "Batch num 19480: Loss 9.635509490966797\n",
      "Batch num 19490: Loss 10.845470428466797\n",
      "Batch num 19500: Loss 10.959437370300293\n",
      "Batch num 19510: Loss 11.62948989868164\n",
      "Batch num 19520: Loss 10.596060752868652\n",
      "Batch num 19530: Loss 13.038091659545898\n",
      "Batch num 19540: Loss 11.42384147644043\n",
      "Batch num 19550: Loss 10.89870834350586\n",
      "Batch num 19560: Loss 10.245552062988281\n",
      "Batch num 19570: Loss 10.280035018920898\n",
      "Batch num 19580: Loss 11.426207542419434\n",
      "Batch num 19590: Loss 10.650854110717773\n",
      "Batch num 19600: Loss 9.974815368652344\n",
      "Batch num 19610: Loss 10.042747497558594\n",
      "Batch num 19620: Loss 9.507726669311523\n",
      "Batch num 19630: Loss 10.484249114990234\n",
      "Batch num 19640: Loss 8.554141998291016\n",
      "Batch num 19650: Loss 10.464526176452637\n",
      "Batch num 19660: Loss 10.519344329833984\n",
      "Batch num 19670: Loss 10.478303909301758\n",
      "Batch num 19680: Loss 11.607763290405273\n",
      "Batch num 19690: Loss 9.505270004272461\n",
      "Batch num 19700: Loss 10.15337085723877\n",
      "Batch num 19710: Loss 11.111048698425293\n",
      "Batch num 19720: Loss 11.660921096801758\n",
      "Batch num 19730: Loss 8.734485626220703\n",
      "Batch num 19740: Loss 10.640021324157715\n",
      "Batch num 19750: Loss 11.845861434936523\n",
      "Batch num 19760: Loss 11.568910598754883\n",
      "Batch num 19770: Loss 9.586305618286133\n",
      "Batch num 19780: Loss 9.591325759887695\n",
      "Batch num 19790: Loss 10.030997276306152\n",
      "Batch num 19800: Loss 11.902539253234863\n",
      "Batch num 19810: Loss 12.331908226013184\n",
      "Batch num 19820: Loss 10.072054862976074\n",
      "Batch num 19830: Loss 11.087806701660156\n",
      "Batch num 19840: Loss 11.37668228149414\n",
      "Batch num 19850: Loss 9.46872329711914\n",
      "Batch num 19860: Loss 10.572128295898438\n",
      "Batch num 19870: Loss 12.217052459716797\n",
      "Batch num 19880: Loss 11.424551010131836\n",
      "Batch num 19890: Loss 10.293166160583496\n",
      "Batch num 19900: Loss 10.462442398071289\n",
      "Batch num 19910: Loss 10.232762336730957\n",
      "Batch num 19920: Loss 11.327143669128418\n",
      "Batch num 19930: Loss 8.878165245056152\n",
      "Batch num 19940: Loss 9.61514949798584\n",
      "Batch num 19950: Loss 11.913220405578613\n",
      "Batch num 19960: Loss 9.656561851501465\n",
      "Batch num 19970: Loss 10.488032341003418\n",
      "Batch num 19980: Loss 11.48762321472168\n",
      "Batch num 19990: Loss 10.864652633666992\n",
      "Batch num 20000: Loss 11.107285499572754\n",
      "Batch num 20010: Loss 9.968018531799316\n",
      "Batch num 20020: Loss 10.113361358642578\n",
      "Batch num 20030: Loss 10.324617385864258\n",
      "Batch num 20040: Loss 16.867664337158203\n",
      "Batch num 20050: Loss 11.926385879516602\n",
      "Batch num 20060: Loss 9.707996368408203\n",
      "Batch num 20070: Loss 11.190069198608398\n",
      "Batch num 20080: Loss 9.80971908569336\n",
      "Batch num 20090: Loss 10.517740249633789\n",
      "Batch num 20100: Loss 8.991674423217773\n",
      "Batch num 20110: Loss 13.820287704467773\n",
      "Batch num 20120: Loss 11.133737564086914\n",
      "Batch num 20130: Loss 12.296445846557617\n",
      "Batch num 20140: Loss 11.594898223876953\n",
      "Batch num 20150: Loss 8.946260452270508\n",
      "Batch num 20160: Loss 11.099111557006836\n",
      "Batch num 20170: Loss 11.014532089233398\n",
      "Batch num 20180: Loss 9.849692344665527\n",
      "Batch num 20190: Loss 12.195991516113281\n",
      "Batch num 20200: Loss 10.536003112792969\n",
      "Batch num 20210: Loss 11.277935028076172\n",
      "Batch num 20220: Loss 10.344395637512207\n",
      "Batch num 20230: Loss 11.61123275756836\n",
      "Batch num 20240: Loss 9.087198257446289\n",
      "Batch num 20250: Loss 8.029122352600098\n",
      "Batch num 20260: Loss 13.095189094543457\n",
      "Batch num 20270: Loss 10.452335357666016\n",
      "Batch num 20280: Loss 10.153097152709961\n",
      "Batch num 20290: Loss 11.759029388427734\n",
      "Batch num 20300: Loss 11.948991775512695\n",
      "Batch num 20310: Loss 10.260501861572266\n",
      "Batch num 20320: Loss 9.348054885864258\n",
      "Batch num 20330: Loss 11.01891040802002\n",
      "Batch num 20340: Loss 10.602901458740234\n",
      "Batch num 20350: Loss 9.514581680297852\n",
      "Batch num 20360: Loss 11.174113273620605\n",
      "Batch num 20370: Loss 10.756143569946289\n",
      "Batch num 20380: Loss 12.213436126708984\n",
      "Batch num 20390: Loss 11.531204223632812\n",
      "Batch num 20400: Loss 6.47672176361084\n",
      "Batch num 20410: Loss 10.971368789672852\n",
      "Batch num 20420: Loss 10.274845123291016\n",
      "Batch num 20430: Loss 10.90772819519043\n",
      "Batch num 20440: Loss 11.319289207458496\n",
      "Batch num 20450: Loss 10.957945823669434\n",
      "Batch num 20460: Loss 12.862524032592773\n",
      "Batch num 20470: Loss 10.031776428222656\n",
      "Batch num 20480: Loss 10.427452087402344\n",
      "Batch num 20490: Loss 9.746030807495117\n",
      "Batch num 20500: Loss 11.31998348236084\n",
      "Batch num 20510: Loss 10.470905303955078\n",
      "Batch num 20520: Loss 10.725566864013672\n",
      "Batch num 20530: Loss 9.711706161499023\n",
      "Batch num 20540: Loss 10.303117752075195\n",
      "Batch num 20550: Loss 10.881227493286133\n",
      "Batch num 20560: Loss 12.064081192016602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 20570: Loss 10.235538482666016\n",
      "Batch num 20580: Loss 9.939751625061035\n",
      "Batch num 20590: Loss 11.473068237304688\n",
      "Batch num 20600: Loss 9.83657169342041\n",
      "Batch num 20610: Loss 11.21064567565918\n",
      "Batch num 20620: Loss 9.827032089233398\n",
      "Batch num 20630: Loss 8.866369247436523\n",
      "Batch num 20640: Loss 11.359190940856934\n",
      "Batch num 20650: Loss 11.109756469726562\n",
      "Batch num 20660: Loss 7.051154613494873\n",
      "Batch num 20670: Loss 9.132936477661133\n",
      "Batch num 20680: Loss 10.76158618927002\n",
      "Batch num 20690: Loss 10.382133483886719\n",
      "Batch num 20700: Loss 8.862678527832031\n",
      "Batch num 20710: Loss 10.427435874938965\n",
      "Batch num 20720: Loss 11.401834487915039\n",
      "Batch num 20730: Loss 105.22845458984375\n",
      "Batch num 20740: Loss 25.29604721069336\n",
      "Batch num 20750: Loss 18.915874481201172\n",
      "Batch num 20760: Loss 11.599516868591309\n",
      "Batch num 20770: Loss 9.918065071105957\n",
      "Batch num 20780: Loss 9.893542289733887\n",
      "Batch num 20790: Loss 11.208086967468262\n",
      "Batch num 20800: Loss 10.008210182189941\n",
      "Batch num 20810: Loss 11.331613540649414\n",
      "Batch num 20820: Loss 12.370213508605957\n",
      "Batch num 20830: Loss 11.218173027038574\n",
      "Batch num 20840: Loss 12.155659675598145\n",
      "Batch num 20850: Loss 10.6923189163208\n",
      "Batch num 20860: Loss 9.199782371520996\n",
      "Batch num 20870: Loss 8.527071952819824\n",
      "Batch num 20880: Loss 12.469799995422363\n",
      "Batch num 20890: Loss 10.244122505187988\n",
      "Batch num 20900: Loss 11.039620399475098\n",
      "Batch num 20910: Loss 20.236270904541016\n",
      "Batch num 20920: Loss 10.865031242370605\n",
      "Batch num 20930: Loss 15.412297248840332\n",
      "Batch num 20940: Loss 12.922956466674805\n",
      "Batch num 20950: Loss 29.028995513916016\n",
      "Batch num 20960: Loss 34.18059158325195\n",
      "Batch num 20970: Loss 142.22906494140625\n",
      "Batch num 20980: Loss 54.71706771850586\n",
      "Batch num 20990: Loss 46.259429931640625\n",
      "Batch num 21000: Loss 11.033611297607422\n",
      "Batch num 21010: Loss 38.68061065673828\n",
      "Batch num 21020: Loss 10.471158981323242\n",
      "Batch num 21030: Loss 60.66777038574219\n",
      "Batch num 21040: Loss 34.14478302001953\n",
      "Batch num 21050: Loss 13.89188003540039\n",
      "Batch num 21060: Loss 10.899356842041016\n",
      "Batch num 21070: Loss 13.200784683227539\n",
      "Batch num 21080: Loss 10.470060348510742\n",
      "Batch num 21090: Loss 10.795808792114258\n",
      "Batch num 21100: Loss 12.420858383178711\n",
      "Batch num 21110: Loss 11.049294471740723\n",
      "Batch num 21120: Loss 9.59935188293457\n",
      "Batch num 21130: Loss 11.796290397644043\n",
      "Batch num 21140: Loss 10.797707557678223\n",
      "Batch num 21150: Loss 8.286026000976562\n",
      "Batch num 21160: Loss 11.627006530761719\n",
      "Batch num 21170: Loss 11.766858100891113\n",
      "Batch num 21180: Loss 10.547276496887207\n",
      "Batch num 21190: Loss 8.6220703125\n",
      "Batch num 21200: Loss 11.17061996459961\n",
      "Batch num 21210: Loss 11.206286430358887\n",
      "Batch num 21220: Loss 11.7573881149292\n",
      "Batch num 21230: Loss 9.112215042114258\n",
      "Batch num 21240: Loss 11.434102058410645\n",
      "Batch num 21250: Loss 10.005941390991211\n",
      "Batch num 21260: Loss 11.735519409179688\n",
      "Batch num 21270: Loss 9.584493637084961\n",
      "Batch num 21280: Loss 11.342496871948242\n",
      "Batch num 21290: Loss 11.358967781066895\n",
      "Batch num 21300: Loss 11.737476348876953\n",
      "Batch num 21310: Loss 10.41437816619873\n",
      "Batch num 21320: Loss 10.58547306060791\n",
      "Batch num 21330: Loss 10.401481628417969\n",
      "Batch num 21340: Loss 8.188539505004883\n",
      "Batch num 21350: Loss 10.886730194091797\n",
      "Batch num 21360: Loss 9.967208862304688\n",
      "Batch num 21370: Loss 9.955355644226074\n",
      "Batch num 21380: Loss 10.003662109375\n",
      "Batch num 21390: Loss 12.3803071975708\n",
      "Batch num 21400: Loss 10.337756156921387\n",
      "Batch num 21410: Loss 11.014857292175293\n",
      "Batch num 21420: Loss 10.683035850524902\n",
      "Batch num 21430: Loss 10.919921875\n",
      "Batch num 21440: Loss 11.729345321655273\n",
      "Batch num 21450: Loss 9.782295227050781\n",
      "Batch num 21460: Loss 8.718206405639648\n",
      "Batch num 21470: Loss 10.894086837768555\n",
      "Batch num 21480: Loss 9.23774242401123\n",
      "Batch num 21490: Loss 23.25715446472168\n",
      "Batch num 21500: Loss 11.38412857055664\n",
      "Batch num 21510: Loss 11.702295303344727\n",
      "Batch num 21520: Loss 10.154609680175781\n",
      "Batch num 21530: Loss 118.21532440185547\n",
      "Batch num 21540: Loss 76.48920440673828\n",
      "Batch num 21550: Loss 238.73287963867188\n",
      "Batch num 21560: Loss 196.77597045898438\n",
      "Batch num 21570: Loss 84.60124206542969\n",
      "Batch num 21580: Loss 17.389690399169922\n",
      "Batch num 21590: Loss 33.191734313964844\n",
      "Batch num 21600: Loss 217.04849243164062\n",
      "Batch num 21610: Loss 144.34735107421875\n",
      "Batch num 21620: Loss 53.4637451171875\n",
      "Batch num 21630: Loss 47.294288635253906\n",
      "Batch num 21640: Loss 72.73416900634766\n",
      "Batch num 21650: Loss 15.57005500793457\n",
      "Batch num 21660: Loss 10.958555221557617\n",
      "Batch num 21670: Loss 40.098838806152344\n",
      "Batch num 21680: Loss 10.421489715576172\n",
      "Batch num 21690: Loss 11.112887382507324\n",
      "Batch num 21700: Loss 12.107159614562988\n",
      "Batch num 21710: Loss 9.217421531677246\n",
      "Batch num 21720: Loss 10.679556846618652\n",
      "Batch num 21730: Loss 10.428321838378906\n",
      "Batch num 21740: Loss 11.342000961303711\n",
      "Batch num 21750: Loss 10.583853721618652\n",
      "Batch num 21760: Loss 10.452751159667969\n",
      "Batch num 21770: Loss 13.756223678588867\n",
      "Batch num 21780: Loss 8.806714057922363\n",
      "Batch num 21790: Loss 10.130374908447266\n",
      "Batch num 21800: Loss 9.029582023620605\n",
      "Batch num 21810: Loss 12.301593780517578\n",
      "Batch num 21820: Loss 10.068257331848145\n",
      "Batch num 21830: Loss 11.80122184753418\n",
      "Batch num 21840: Loss 12.104697227478027\n",
      "Batch num 21850: Loss 13.830653190612793\n",
      "Batch num 21860: Loss 15.962751388549805\n",
      "Batch num 21870: Loss 9.256816864013672\n",
      "Batch num 21880: Loss 11.550052642822266\n",
      "Batch num 21890: Loss 10.538578033447266\n",
      "Batch num 21900: Loss 44.420265197753906\n",
      "Batch num 21910: Loss 11.337742805480957\n",
      "Batch num 21920: Loss 10.787052154541016\n",
      "Batch num 21930: Loss 9.731280326843262\n",
      "Batch num 21940: Loss 10.831640243530273\n",
      "Batch num 21950: Loss 12.167708396911621\n",
      "Batch num 21960: Loss 10.009038925170898\n",
      "Batch num 21970: Loss 10.921541213989258\n",
      "Batch num 21980: Loss 9.763473510742188\n",
      "Batch num 21990: Loss 9.701188087463379\n",
      "Batch num 22000: Loss 10.550971031188965\n",
      "Batch num 22010: Loss 18.043148040771484\n",
      "Batch num 22020: Loss 15.09538459777832\n",
      "Batch num 22030: Loss 7.7883453369140625\n",
      "Batch num 22040: Loss 9.273246765136719\n",
      "Batch num 22050: Loss 10.442967414855957\n",
      "Batch num 22060: Loss 11.74959659576416\n",
      "Batch num 22070: Loss 24.47800064086914\n",
      "Batch num 22080: Loss 11.55189323425293\n",
      "Batch num 22090: Loss 9.99067211151123\n",
      "Batch num 22100: Loss 13.050454139709473\n",
      "Batch num 22110: Loss 15.225227355957031\n",
      "Batch num 22120: Loss 11.480326652526855\n",
      "Batch num 22130: Loss 10.618508338928223\n",
      "Batch num 22140: Loss 11.419489860534668\n",
      "Batch num 22150: Loss 10.617987632751465\n",
      "Batch num 22160: Loss 11.312268257141113\n",
      "Batch num 22170: Loss 10.8479642868042\n",
      "Batch num 22180: Loss 11.674506187438965\n",
      "Batch num 22190: Loss 11.926066398620605\n",
      "Batch num 22200: Loss 10.725899696350098\n",
      "Batch num 22210: Loss 8.235255241394043\n",
      "Batch num 22220: Loss 10.577775955200195\n",
      "Batch num 22230: Loss 10.244643211364746\n",
      "Batch num 22240: Loss 10.036015510559082\n",
      "Batch num 22250: Loss 10.473913192749023\n",
      "Batch num 22260: Loss 11.361821174621582\n",
      "Batch num 22270: Loss 10.195502281188965\n",
      "Batch num 22280: Loss 11.904733657836914\n",
      "Batch num 22290: Loss 12.104513168334961\n",
      "Batch num 22300: Loss 10.030434608459473\n",
      "Batch num 22310: Loss 12.271196365356445\n",
      "Batch num 22320: Loss 11.160741806030273\n",
      "Batch num 22330: Loss 10.382695198059082\n",
      "Batch num 22340: Loss 11.463356018066406\n",
      "Batch num 22350: Loss 10.015410423278809\n",
      "Batch num 22360: Loss 9.085247993469238\n",
      "Batch num 22370: Loss 10.534594535827637\n",
      "Batch num 22380: Loss 11.349125862121582\n",
      "Batch num 22390: Loss 11.333027839660645\n",
      "Batch num 22400: Loss 9.776976585388184\n",
      "Batch num 22410: Loss 10.806313514709473\n",
      "Batch num 22420: Loss 10.403002738952637\n",
      "Batch num 22430: Loss 9.338021278381348\n",
      "Batch num 22440: Loss 11.307382583618164\n",
      "Batch num 22450: Loss 12.191872596740723\n",
      "Batch num 22460: Loss 10.844303131103516\n",
      "Batch num 22470: Loss 11.114924430847168\n",
      "Batch num 22480: Loss 9.856614112854004\n",
      "Batch num 22490: Loss 11.288875579833984\n",
      "Batch num 22500: Loss 10.08675765991211\n",
      "Batch num 22510: Loss 10.787019729614258\n",
      "Batch num 22520: Loss 12.233116149902344\n",
      "Batch num 22530: Loss 7.99430513381958\n",
      "Batch num 22540: Loss 10.151219367980957\n",
      "Batch num 22550: Loss 11.273995399475098\n",
      "Batch num 22560: Loss 11.757088661193848\n",
      "Batch num 22570: Loss 12.82652473449707\n",
      "Batch num 22580: Loss 10.76456356048584\n",
      "Batch num 22590: Loss 10.671589851379395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 22600: Loss 9.754932403564453\n",
      "Batch num 22610: Loss 10.733416557312012\n",
      "Batch num 22620: Loss 10.102594375610352\n",
      "Batch num 22630: Loss 10.877947807312012\n",
      "Batch num 22640: Loss 11.368189811706543\n",
      "Batch num 22650: Loss 11.345340728759766\n",
      "Batch num 22660: Loss 12.42745304107666\n",
      "Batch num 22670: Loss 9.934901237487793\n",
      "Batch num 22680: Loss 9.50755500793457\n",
      "Batch num 22690: Loss 9.56198501586914\n",
      "Batch num 22700: Loss 8.518766403198242\n",
      "Batch num 22710: Loss 11.853572845458984\n",
      "Batch num 22720: Loss 11.648247718811035\n",
      "Batch num 22730: Loss 9.831513404846191\n",
      "Batch num 22740: Loss 9.957441329956055\n",
      "Batch num 22750: Loss 11.908453941345215\n",
      "Batch num 22760: Loss 10.223410606384277\n",
      "Batch num 22770: Loss 9.64030933380127\n",
      "Batch num 22780: Loss 9.595108985900879\n",
      "Batch num 22790: Loss 11.547274589538574\n",
      "Batch num 22800: Loss 12.46478271484375\n",
      "Batch num 22810: Loss 11.403555870056152\n",
      "Batch num 22820: Loss 9.883271217346191\n",
      "Batch num 22830: Loss 11.409259796142578\n",
      "Batch num 22840: Loss 8.441596984863281\n",
      "Batch num 22850: Loss 9.913041114807129\n",
      "Batch num 22860: Loss 10.713678359985352\n",
      "Batch num 22870: Loss 8.50468635559082\n",
      "Batch num 22880: Loss 9.180574417114258\n",
      "Batch num 22890: Loss 9.48225212097168\n",
      "Batch num 22900: Loss 11.911531448364258\n",
      "Batch num 22910: Loss 11.279057502746582\n",
      "Batch num 22920: Loss 10.691288948059082\n",
      "Batch num 22930: Loss 12.72869873046875\n",
      "Batch num 22940: Loss 10.250593185424805\n",
      "Batch num 22950: Loss 9.234942436218262\n",
      "Batch num 22960: Loss 9.838431358337402\n",
      "Batch num 22970: Loss 10.02672290802002\n",
      "Batch num 22980: Loss 10.515442848205566\n",
      "Batch num 22990: Loss 10.125876426696777\n",
      "Batch num 23000: Loss 9.188882827758789\n",
      "Batch num 23010: Loss 10.351006507873535\n",
      "Batch num 23020: Loss 10.950815200805664\n",
      "Batch num 23030: Loss 8.672636985778809\n",
      "Batch num 23040: Loss 9.791510581970215\n",
      "Batch num 23050: Loss 9.015142440795898\n",
      "Batch num 23060: Loss 8.981772422790527\n",
      "Batch num 23070: Loss 9.316344261169434\n",
      "Batch num 23080: Loss 10.047043800354004\n",
      "Batch num 23090: Loss 9.413243293762207\n",
      "Batch num 23100: Loss 9.45683765411377\n",
      "Batch num 23110: Loss 9.39154052734375\n",
      "Batch num 23120: Loss 10.182147979736328\n",
      "Batch num 23130: Loss 9.993104934692383\n",
      "Batch num 23140: Loss 10.643348693847656\n",
      "Batch num 23150: Loss 9.225608825683594\n",
      "Batch num 23160: Loss 8.885098457336426\n",
      "Batch num 23170: Loss 10.14667797088623\n",
      "Batch num 23180: Loss 9.637328147888184\n",
      "Batch num 23190: Loss 11.082402229309082\n",
      "Batch num 23200: Loss 9.138418197631836\n",
      "Batch num 23210: Loss 10.696006774902344\n",
      "Batch num 23220: Loss 12.084278106689453\n",
      "Batch num 23230: Loss 10.38619327545166\n",
      "Batch num 23240: Loss 11.073111534118652\n",
      "Batch num 23250: Loss 9.551316261291504\n",
      "Batch num 23260: Loss 9.005118370056152\n",
      "Batch num 23270: Loss 9.420207023620605\n",
      "Batch num 23280: Loss 11.23459243774414\n",
      "Batch num 23290: Loss 8.895804405212402\n",
      "Batch num 23300: Loss 10.64335823059082\n",
      "Batch num 23310: Loss 10.601483345031738\n",
      "Batch num 23320: Loss 9.843289375305176\n",
      "Batch num 23330: Loss 10.7376127243042\n",
      "Batch num 23340: Loss 11.336289405822754\n",
      "Batch num 23350: Loss 10.310198783874512\n",
      "Batch num 23360: Loss 9.525630950927734\n",
      "Batch num 23370: Loss 9.676190376281738\n",
      "Batch num 23380: Loss 10.54493522644043\n",
      "Batch num 23390: Loss 10.312820434570312\n",
      "Batch num 23400: Loss 8.802117347717285\n",
      "Batch num 23410: Loss 9.943435668945312\n",
      "Batch num 23420: Loss 12.141220092773438\n",
      "Batch num 23430: Loss 11.173582077026367\n",
      "Batch num 23440: Loss 9.569721221923828\n",
      "Batch num 23450: Loss 11.375000953674316\n",
      "Batch num 23460: Loss 9.98776912689209\n",
      "Batch num 23470: Loss 10.682732582092285\n",
      "Batch num 23480: Loss 9.792537689208984\n",
      "Batch num 23490: Loss 10.041374206542969\n",
      "Batch num 23500: Loss 11.465750694274902\n",
      "Batch num 23510: Loss 9.45725154876709\n",
      "Batch num 23520: Loss 8.94273567199707\n",
      "Batch num 23530: Loss 10.411093711853027\n",
      "Batch num 23540: Loss 9.384111404418945\n",
      "Batch num 23550: Loss 9.000309944152832\n",
      "Batch num 23560: Loss 11.608358383178711\n",
      "Batch num 23570: Loss 11.322639465332031\n",
      "Batch num 23580: Loss 10.053953170776367\n",
      "Batch num 23590: Loss 10.18980598449707\n",
      "Batch num 23600: Loss 10.218435287475586\n",
      "Batch num 23610: Loss 10.77108383178711\n",
      "Batch num 23620: Loss 10.264748573303223\n",
      "Batch num 23630: Loss 9.68016242980957\n",
      "Batch num 23640: Loss 7.995810508728027\n",
      "Batch num 23650: Loss 8.179293632507324\n",
      "Batch num 23660: Loss 9.48426628112793\n",
      "Batch num 23670: Loss 12.01887035369873\n",
      "Batch num 23680: Loss 9.2535400390625\n",
      "Batch num 23690: Loss 9.382333755493164\n",
      "Batch num 23700: Loss 11.542712211608887\n",
      "Batch num 23710: Loss 8.933136940002441\n",
      "Batch num 23720: Loss 9.587762832641602\n",
      "Batch num 23730: Loss 11.524541854858398\n",
      "Batch num 23740: Loss 10.365452766418457\n",
      "Batch num 23750: Loss 10.45683479309082\n",
      "Batch num 23760: Loss 10.560185432434082\n",
      "Batch num 23770: Loss 9.720192909240723\n",
      "Batch num 23780: Loss 11.282368659973145\n",
      "Batch num 23790: Loss 9.89132022857666\n",
      "Batch num 23800: Loss 11.347655296325684\n",
      "Batch num 23810: Loss 11.89235782623291\n",
      "Batch num 23820: Loss 10.387152671813965\n",
      "Batch num 23830: Loss 11.515549659729004\n",
      "Batch num 23840: Loss 9.140480995178223\n",
      "Batch num 23850: Loss 9.798660278320312\n",
      "Batch num 23860: Loss 10.24260139465332\n",
      "Batch num 23870: Loss 10.309797286987305\n",
      "Batch num 23880: Loss 10.809948921203613\n",
      "Batch num 23890: Loss 10.09054183959961\n",
      "Batch num 23900: Loss 9.484664916992188\n",
      "Batch num 23910: Loss 10.606050491333008\n",
      "Batch num 23920: Loss 9.093012809753418\n",
      "Batch num 23930: Loss 10.740519523620605\n",
      "Batch num 23940: Loss 11.475333213806152\n",
      "Batch num 23950: Loss 8.785628318786621\n",
      "Batch num 23960: Loss 10.378586769104004\n",
      "Batch num 23970: Loss 9.038705825805664\n",
      "Batch num 23980: Loss 9.037168502807617\n",
      "Batch num 23990: Loss 8.411503791809082\n",
      "Batch num 24000: Loss 9.629253387451172\n",
      "Batch num 24010: Loss 8.187159538269043\n",
      "Batch num 24020: Loss 10.375293731689453\n",
      "Batch num 24030: Loss 10.166790962219238\n",
      "Batch num 24040: Loss 10.64967155456543\n",
      "Batch num 24050: Loss 10.829642295837402\n",
      "Batch num 24060: Loss 10.982000350952148\n",
      "Batch num 24070: Loss 10.96161937713623\n",
      "Batch num 24080: Loss 12.69849967956543\n",
      "Batch num 24090: Loss 12.266328811645508\n",
      "Batch num 24100: Loss 10.142642974853516\n",
      "Batch num 24110: Loss 11.805469512939453\n",
      "Batch num 24120: Loss 11.247356414794922\n",
      "Batch num 24130: Loss 10.23949909210205\n",
      "Batch num 24140: Loss 11.38096809387207\n",
      "Batch num 24150: Loss 12.073455810546875\n",
      "Batch num 24160: Loss 9.628535270690918\n",
      "Batch num 24170: Loss 9.986593246459961\n",
      "Batch num 24180: Loss 10.576871871948242\n",
      "Batch num 24190: Loss 12.033713340759277\n",
      "Batch num 24200: Loss 9.984282493591309\n",
      "Batch num 24210: Loss 10.608333587646484\n",
      "Batch num 24220: Loss 12.425223350524902\n",
      "Batch num 24230: Loss 10.039552688598633\n",
      "Batch num 24240: Loss 11.376311302185059\n",
      "Batch num 24250: Loss 12.548047065734863\n",
      "Batch num 24260: Loss 10.860207557678223\n",
      "Batch num 24270: Loss 10.027693748474121\n",
      "Batch num 24280: Loss 10.058172225952148\n",
      "Batch num 24290: Loss 10.35503101348877\n",
      "Batch num 24300: Loss 8.819967269897461\n",
      "Batch num 24310: Loss 9.801183700561523\n",
      "Batch num 24320: Loss 9.54920482635498\n",
      "Batch num 24330: Loss 11.149039268493652\n",
      "Batch num 24340: Loss 9.184636116027832\n",
      "Batch num 24350: Loss 12.699775695800781\n",
      "Batch num 24360: Loss 9.987860679626465\n",
      "Batch num 24370: Loss 9.690016746520996\n",
      "Batch num 24380: Loss 11.897045135498047\n",
      "Batch num 24390: Loss 10.36181640625\n",
      "Batch num 24400: Loss 10.852312088012695\n",
      "Batch num 24410: Loss 9.934664726257324\n",
      "Batch num 24420: Loss 9.668344497680664\n",
      "Batch num 24430: Loss 9.735148429870605\n",
      "Batch num 24440: Loss 10.329487800598145\n",
      "Batch num 24450: Loss 10.86054801940918\n",
      "Batch num 24460: Loss 12.476810455322266\n",
      "Batch num 24470: Loss 9.272353172302246\n",
      "Batch num 24480: Loss 9.499813079833984\n",
      "Batch num 24490: Loss 8.175070762634277\n",
      "Batch num 24500: Loss 8.931497573852539\n",
      "Batch num 24510: Loss 18.703433990478516\n",
      "Batch num 24520: Loss 14.479915618896484\n",
      "Batch num 24530: Loss 11.586944580078125\n",
      "Batch num 24540: Loss 10.864890098571777\n",
      "Batch num 24550: Loss 10.617609024047852\n",
      "Batch num 24560: Loss 8.909902572631836\n",
      "Batch num 24570: Loss 10.899051666259766\n",
      "Batch num 24580: Loss 19.193845748901367\n",
      "Batch num 24590: Loss 14.442909240722656\n",
      "Batch num 24600: Loss 9.553970336914062\n",
      "Batch num 24610: Loss 11.625128746032715\n",
      "Batch num 24620: Loss 14.629379272460938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 24630: Loss 14.763765335083008\n",
      "Batch num 24640: Loss 12.52927017211914\n",
      "Batch num 24650: Loss 9.438425064086914\n",
      "Batch num 24660: Loss 10.161961555480957\n",
      "Batch num 24670: Loss 10.369175910949707\n",
      "Batch num 24680: Loss 11.321742057800293\n",
      "Batch num 24690: Loss 11.657962799072266\n",
      "Batch num 24700: Loss 9.06144905090332\n",
      "Batch num 24710: Loss 11.022412300109863\n",
      "Batch num 24720: Loss 10.698310852050781\n",
      "Batch num 24730: Loss 10.294012069702148\n",
      "Batch num 24740: Loss 42.95354080200195\n",
      "Batch num 24750: Loss 21.0216007232666\n",
      "Batch num 24760: Loss 10.347230911254883\n",
      "Batch num 24770: Loss 10.744314193725586\n",
      "Batch num 24780: Loss 10.012750625610352\n",
      "Batch num 24790: Loss 9.177306175231934\n",
      "Batch num 24800: Loss 14.965353012084961\n",
      "Batch num 24810: Loss 10.588312149047852\n",
      "Batch num 24820: Loss 12.3358736038208\n",
      "Batch num 24830: Loss 8.116700172424316\n",
      "Batch num 24840: Loss 9.065238952636719\n",
      "Batch num 24850: Loss 45.48961639404297\n",
      "Batch num 24860: Loss 15.35739517211914\n",
      "Batch num 24870: Loss 42.08970260620117\n",
      "Batch num 24880: Loss 14.65192699432373\n",
      "Batch num 24890: Loss 14.344849586486816\n",
      "Batch num 24900: Loss 224.36366271972656\n",
      "Batch num 24910: Loss 131.78822326660156\n",
      "Batch num 24920: Loss 68.79145050048828\n",
      "Batch num 24930: Loss 13.26053237915039\n",
      "Batch num 24940: Loss 29.751522064208984\n",
      "Batch num 24950: Loss 10.280784606933594\n",
      "Batch num 24960: Loss 21.31369972229004\n",
      "Batch num 24970: Loss 9.545873641967773\n",
      "Batch num 24980: Loss 10.534427642822266\n",
      "Batch num 24990: Loss 11.325958251953125\n",
      "Batch num 25000: Loss 10.77352523803711\n",
      "Batch num 25010: Loss 10.443252563476562\n",
      "Batch num 25020: Loss 11.49310302734375\n",
      "Batch num 25030: Loss 8.924864768981934\n",
      "Batch num 25040: Loss 10.969908714294434\n",
      "Batch num 25050: Loss 14.008391380310059\n",
      "Batch num 25060: Loss 165.07733154296875\n",
      "Batch num 25070: Loss 22.373821258544922\n",
      "Batch num 25080: Loss 9.760560035705566\n",
      "Batch num 25090: Loss 10.945243835449219\n",
      "Batch num 25100: Loss 12.552223205566406\n",
      "Batch num 25110: Loss 24.500410079956055\n",
      "Batch num 25120: Loss 31.378578186035156\n",
      "Batch num 25130: Loss 20.493810653686523\n",
      "Batch num 25140: Loss 107.27786254882812\n",
      "Batch num 25150: Loss 12.227832794189453\n",
      "Batch num 25160: Loss 10.815120697021484\n",
      "Batch num 25170: Loss 11.356548309326172\n",
      "Batch num 25180: Loss 10.207825660705566\n",
      "Batch num 25190: Loss 11.631598472595215\n",
      "Batch num 25200: Loss 9.14487361907959\n",
      "Batch num 25210: Loss 18.937896728515625\n",
      "Batch num 25220: Loss 27.59136199951172\n",
      "Batch num 25230: Loss 10.06730842590332\n",
      "Batch num 25240: Loss 11.980030059814453\n",
      "Batch num 25250: Loss 10.712379455566406\n",
      "Batch num 25260: Loss 8.926446914672852\n",
      "Batch num 25270: Loss 9.324893951416016\n",
      "Batch num 25280: Loss 9.56706714630127\n",
      "Batch num 25290: Loss 10.450455665588379\n",
      "Batch num 25300: Loss 10.280449867248535\n",
      "Batch num 25310: Loss 12.018292427062988\n",
      "Batch num 25320: Loss 10.229853630065918\n",
      "Batch num 25330: Loss 9.389077186584473\n",
      "Batch num 25340: Loss 9.820100784301758\n",
      "Batch num 25350: Loss 10.075408935546875\n",
      "Batch num 25360: Loss 11.115715026855469\n",
      "Batch num 25370: Loss 8.42067813873291\n",
      "Batch num 25380: Loss 9.822847366333008\n",
      "Batch num 25390: Loss 9.065241813659668\n",
      "Batch num 25400: Loss 11.433707237243652\n",
      "Batch num 25410: Loss 12.056548118591309\n",
      "Batch num 25420: Loss 11.52022933959961\n",
      "Batch num 25430: Loss 8.863870620727539\n",
      "Batch num 25440: Loss 10.643208503723145\n",
      "Batch num 25450: Loss 8.912282943725586\n",
      "Batch num 25460: Loss 12.599626541137695\n",
      "Batch num 25470: Loss 8.783120155334473\n",
      "Batch num 25480: Loss 9.912596702575684\n",
      "Batch num 25490: Loss 10.384305953979492\n",
      "Batch num 25500: Loss 10.105643272399902\n",
      "Batch num 25510: Loss 12.206611633300781\n",
      "Batch num 25520: Loss 10.381878852844238\n",
      "Batch num 25530: Loss 9.785327911376953\n",
      "Batch num 25540: Loss 10.962782859802246\n",
      "Batch num 25550: Loss 10.626455307006836\n",
      "Batch num 25560: Loss 11.2156982421875\n",
      "Batch num 25570: Loss 11.335909843444824\n",
      "Batch num 25580: Loss 10.727567672729492\n",
      "Batch num 25590: Loss 8.8281888961792\n",
      "Batch num 25600: Loss 12.07872200012207\n",
      "Batch num 25610: Loss 10.678681373596191\n",
      "Batch num 25620: Loss 9.560476303100586\n",
      "Batch num 25630: Loss 11.0646390914917\n",
      "Batch num 25640: Loss 10.067142486572266\n",
      "Batch num 25650: Loss 9.945555686950684\n",
      "Batch num 25660: Loss 10.385836601257324\n",
      "Batch num 25670: Loss 9.907466888427734\n",
      "Batch num 25680: Loss 10.84853744506836\n",
      "Batch num 25690: Loss 11.64574146270752\n",
      "Batch num 25700: Loss 10.854532241821289\n",
      "Batch num 25710: Loss 10.036341667175293\n",
      "Batch num 25720: Loss 10.36483383178711\n",
      "Batch num 25730: Loss 11.746312141418457\n",
      "Batch num 25740: Loss 9.813570976257324\n",
      "Batch num 25750: Loss 10.469223976135254\n",
      "Batch num 25760: Loss 9.368659019470215\n",
      "Batch num 25770: Loss 10.589676856994629\n",
      "Batch num 25780: Loss 5.54462194442749\n",
      "Batch num 25790: Loss 10.003314018249512\n",
      "Batch num 25800: Loss 9.341523170471191\n",
      "Batch num 25810: Loss 11.043502807617188\n",
      "Batch num 25820: Loss 10.630821228027344\n",
      "Batch num 25830: Loss 12.5551176071167\n",
      "Batch num 25840: Loss 12.250658988952637\n",
      "Batch num 25850: Loss 11.499917030334473\n",
      "Batch num 25860: Loss 10.554939270019531\n",
      "Batch num 25870: Loss 11.729459762573242\n",
      "Batch num 25880: Loss 11.314659118652344\n",
      "Batch num 25890: Loss 10.9594087600708\n",
      "Batch num 25900: Loss 10.927541732788086\n",
      "Batch num 25910: Loss 9.240007400512695\n",
      "Batch num 25920: Loss 11.155058860778809\n",
      "Batch num 25930: Loss 10.116978645324707\n",
      "Batch num 25940: Loss 10.4481782913208\n",
      "Batch num 25950: Loss 11.71214485168457\n",
      "Batch num 25960: Loss 9.439906120300293\n",
      "Batch num 25970: Loss 8.267505645751953\n",
      "Batch num 25980: Loss 11.172889709472656\n",
      "Batch num 25990: Loss 8.856486320495605\n",
      "Batch num 26000: Loss 10.641551971435547\n",
      "Batch num 26010: Loss 11.208059310913086\n",
      "Batch num 26020: Loss 7.295819282531738\n",
      "Batch num 26030: Loss 10.604325294494629\n",
      "Batch num 26040: Loss 10.305437088012695\n",
      "Batch num 26050: Loss 10.744418144226074\n",
      "Batch num 26060: Loss 10.283713340759277\n",
      "Batch num 26070: Loss 9.285152435302734\n",
      "Batch num 26080: Loss 9.52140998840332\n",
      "Batch num 26090: Loss 11.303671836853027\n",
      "Batch num 26100: Loss 9.174873352050781\n",
      "Batch num 26110: Loss 10.175651550292969\n",
      "Batch num 26120: Loss 10.200037002563477\n",
      "Batch num 26130: Loss 9.424275398254395\n",
      "Batch num 26140: Loss 9.500127792358398\n",
      "Batch num 26150: Loss 11.025721549987793\n",
      "Batch num 26160: Loss 9.527412414550781\n",
      "Batch num 26170: Loss 9.91679859161377\n",
      "Batch num 26180: Loss 10.63524055480957\n",
      "Batch num 26190: Loss 10.798782348632812\n",
      "Batch num 26200: Loss 9.997511863708496\n",
      "Batch num 26210: Loss 11.270734786987305\n",
      "Batch num 26220: Loss 8.903721809387207\n",
      "Batch num 26230: Loss 10.915452003479004\n",
      "Batch num 26240: Loss 10.842244148254395\n",
      "Batch num 26250: Loss 11.214747428894043\n",
      "Batch num 26260: Loss 9.700756072998047\n",
      "Batch num 26270: Loss 10.965333938598633\n",
      "Batch num 26280: Loss 10.856042861938477\n",
      "Batch num 26290: Loss 11.430021286010742\n",
      "Batch num 26300: Loss 11.491737365722656\n",
      "Batch num 26310: Loss 10.37331771850586\n",
      "Batch num 26320: Loss 10.758011817932129\n",
      "Batch num 26330: Loss 11.529726028442383\n",
      "Batch num 26340: Loss 9.77926254272461\n",
      "Batch num 26350: Loss 9.755792617797852\n",
      "Batch num 26360: Loss 8.187357902526855\n",
      "Batch num 26370: Loss 11.742266654968262\n",
      "Batch num 26380: Loss 10.558730125427246\n",
      "Batch num 26390: Loss 8.626556396484375\n",
      "Batch num 26400: Loss 11.459468841552734\n",
      "Batch num 26410: Loss 9.819506645202637\n",
      "Batch num 26420: Loss 10.074808120727539\n",
      "Batch num 26430: Loss 12.88792896270752\n",
      "Batch num 26440: Loss 10.154685020446777\n",
      "Batch num 26450: Loss 9.38085651397705\n",
      "Batch num 26460: Loss 9.811455726623535\n",
      "Batch num 26470: Loss 9.690387725830078\n",
      "Batch num 26480: Loss 9.48890495300293\n",
      "Batch num 26490: Loss 10.301374435424805\n",
      "Batch num 26500: Loss 10.066967964172363\n",
      "Batch num 26510: Loss 9.819087028503418\n",
      "Batch num 26520: Loss 9.484429359436035\n",
      "Batch num 26530: Loss 11.777377128601074\n",
      "Batch num 26540: Loss 9.294289588928223\n",
      "Batch num 26550: Loss 10.684591293334961\n",
      "Batch num 26560: Loss 11.478029251098633\n",
      "Batch num 26570: Loss 11.131715774536133\n",
      "Batch num 26580: Loss 9.697152137756348\n",
      "Batch num 26590: Loss 9.566217422485352\n",
      "Batch num 26600: Loss 11.177886962890625\n",
      "Batch num 26610: Loss 10.969229698181152\n",
      "Batch num 26620: Loss 10.338849067687988\n",
      "Batch num 26630: Loss 9.76778507232666\n",
      "Batch num 26640: Loss 12.863430976867676\n",
      "Batch num 26650: Loss 9.884770393371582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 26660: Loss 10.004644393920898\n",
      "Batch num 26670: Loss 11.190723419189453\n",
      "Batch num 26680: Loss 7.2605299949646\n",
      "Batch num 26690: Loss 9.84959602355957\n",
      "Batch num 26700: Loss 10.935554504394531\n",
      "Batch num 26710: Loss 9.514371871948242\n",
      "Batch num 26720: Loss 8.619270324707031\n",
      "Batch num 26730: Loss 10.982379913330078\n",
      "Batch num 26740: Loss 11.472953796386719\n",
      "Batch num 26750: Loss 10.428322792053223\n",
      "Batch num 26760: Loss 11.553048133850098\n",
      "Batch num 26770: Loss 10.759095191955566\n",
      "Batch num 26780: Loss 7.299527168273926\n",
      "Batch num 26790: Loss 10.75650405883789\n",
      "Batch num 26800: Loss 10.261479377746582\n",
      "Batch num 26810: Loss 10.253299713134766\n",
      "Batch num 26820: Loss 10.338923454284668\n",
      "Batch num 26830: Loss 9.867844581604004\n",
      "Batch num 26840: Loss 10.8430814743042\n",
      "Batch num 26850: Loss 11.335005760192871\n",
      "Batch num 26860: Loss 9.695938110351562\n",
      "Batch num 26870: Loss 12.805227279663086\n",
      "Batch num 26880: Loss 10.942564010620117\n",
      "Batch num 26890: Loss 9.597543716430664\n",
      "Batch num 26900: Loss 11.384653091430664\n",
      "Batch num 26910: Loss 11.837408065795898\n",
      "Batch num 26920: Loss 9.142268180847168\n",
      "Batch num 26930: Loss 10.32625675201416\n",
      "Batch num 26940: Loss 12.138481140136719\n",
      "Batch num 26950: Loss 11.286628723144531\n",
      "Batch num 26960: Loss 10.467179298400879\n",
      "Batch num 26970: Loss 10.118486404418945\n",
      "Batch num 26980: Loss 11.446377754211426\n",
      "Batch num 26990: Loss 9.843762397766113\n",
      "Batch num 27000: Loss 11.15638256072998\n",
      "Batch num 27010: Loss 11.066380500793457\n",
      "Batch num 27020: Loss 10.745890617370605\n",
      "Batch num 27030: Loss 12.745233535766602\n",
      "Batch num 27040: Loss 8.49256420135498\n",
      "Batch num 27050: Loss 9.172945022583008\n",
      "Batch num 27060: Loss 10.440793991088867\n",
      "Batch num 27070: Loss 10.251260757446289\n",
      "Batch num 27080: Loss 10.950676918029785\n",
      "Batch num 27090: Loss 9.92618179321289\n",
      "Batch num 27100: Loss 8.66564655303955\n",
      "Batch num 27110: Loss 9.605461120605469\n",
      "Batch num 27120: Loss 10.789475440979004\n",
      "Batch num 27130: Loss 10.955931663513184\n",
      "Batch num 27140: Loss 9.111196517944336\n",
      "Batch num 27150: Loss 12.321561813354492\n",
      "Batch num 27160: Loss 9.44929313659668\n",
      "Batch num 27170: Loss 9.734735488891602\n",
      "Batch num 27180: Loss 9.496691703796387\n",
      "Batch num 27190: Loss 10.116447448730469\n",
      "Batch num 27200: Loss 11.099968910217285\n",
      "Batch num 27210: Loss 9.201605796813965\n",
      "Batch num 27220: Loss 11.818470001220703\n",
      "Batch num 27230: Loss 12.089683532714844\n",
      "Batch num 27240: Loss 11.273334503173828\n",
      "Batch num 27250: Loss 10.520696640014648\n",
      "Batch num 27260: Loss 9.762948989868164\n",
      "Batch num 27270: Loss 9.641159057617188\n",
      "Batch num 27280: Loss 10.027094841003418\n",
      "Batch num 27290: Loss 11.525969505310059\n",
      "Batch num 27300: Loss 12.60318374633789\n",
      "Batch num 27310: Loss 10.62458324432373\n",
      "Batch num 27320: Loss 10.356395721435547\n",
      "Batch num 27330: Loss 10.017196655273438\n",
      "Batch num 27340: Loss 11.02155590057373\n",
      "Batch num 27350: Loss 11.579560279846191\n",
      "Batch num 27360: Loss 12.230196952819824\n",
      "Batch num 27370: Loss 9.881973266601562\n",
      "Batch num 27380: Loss 10.400542259216309\n",
      "Batch num 27390: Loss 9.867055892944336\n",
      "Batch num 27400: Loss 11.213191986083984\n",
      "Batch num 27410: Loss 13.086830139160156\n",
      "Batch num 27420: Loss 9.654731750488281\n",
      "Batch num 27430: Loss 9.061955451965332\n",
      "Batch num 27440: Loss 9.985518455505371\n",
      "Batch num 27450: Loss 9.243276596069336\n",
      "Batch num 27460: Loss 11.034514427185059\n",
      "Batch num 27470: Loss 11.34519100189209\n",
      "Batch num 27480: Loss 11.800597190856934\n",
      "Batch num 27490: Loss 10.631105422973633\n",
      "Batch num 27500: Loss 11.069075584411621\n",
      "Batch num 27510: Loss 9.578108787536621\n",
      "Batch num 27520: Loss 10.835121154785156\n",
      "Batch num 27530: Loss 9.20854377746582\n",
      "Batch num 27540: Loss 8.672691345214844\n",
      "Batch num 27550: Loss 10.714607238769531\n",
      "Batch num 27560: Loss 5.810161590576172\n",
      "Batch num 27570: Loss 9.776372909545898\n",
      "Batch num 27580: Loss 12.25084114074707\n",
      "Batch num 27590: Loss 9.669992446899414\n",
      "Batch num 27600: Loss 12.506736755371094\n",
      "Batch num 27610: Loss 12.210901260375977\n",
      "Batch num 27620: Loss 11.50749397277832\n",
      "Batch num 27630: Loss 11.918697357177734\n",
      "Batch num 27640: Loss 10.085175514221191\n",
      "Batch num 27650: Loss 11.020574569702148\n",
      "Batch num 27660: Loss 9.685428619384766\n",
      "Batch num 27670: Loss 10.166308403015137\n",
      "Batch num 27680: Loss 9.900264739990234\n",
      "Batch num 27690: Loss 10.52298355102539\n",
      "Batch num 27700: Loss 11.692975997924805\n",
      "Batch num 27710: Loss 10.43897533416748\n",
      "Batch num 27720: Loss 12.182183265686035\n",
      "Batch num 27730: Loss 11.213512420654297\n",
      "Batch num 27740: Loss 6.721314907073975\n",
      "Batch num 27750: Loss 11.959823608398438\n",
      "Batch num 27760: Loss 11.076335906982422\n",
      "Batch num 27770: Loss 8.52731704711914\n",
      "Batch num 27780: Loss 11.870996475219727\n",
      "Batch num 27790: Loss 10.095658302307129\n",
      "Batch num 27800: Loss 11.057714462280273\n",
      "Batch num 27810: Loss 8.853922843933105\n",
      "Batch num 27820: Loss 10.16346549987793\n",
      "Batch num 27830: Loss 11.882564544677734\n",
      "Batch num 27840: Loss 11.334538459777832\n",
      "Batch num 27850: Loss 11.8412504196167\n",
      "Batch num 27860: Loss 10.775948524475098\n",
      "Batch num 27870: Loss 11.567119598388672\n",
      "Batch num 27880: Loss 9.990904808044434\n",
      "Batch num 27890: Loss 11.600473403930664\n",
      "Batch num 27900: Loss 9.938901901245117\n",
      "Batch num 27910: Loss 10.24413013458252\n",
      "Batch num 27920: Loss 10.126730918884277\n",
      "Batch num 27930: Loss 10.57735538482666\n",
      "Batch num 27940: Loss 10.674527168273926\n",
      "Batch num 27950: Loss 10.866280555725098\n",
      "Batch num 27960: Loss 11.08765983581543\n",
      "Batch num 27970: Loss 13.15300464630127\n",
      "Batch num 27980: Loss 11.799968719482422\n",
      "Batch num 27990: Loss 10.236577987670898\n",
      "Batch num 28000: Loss 9.409916877746582\n",
      "Batch num 28010: Loss 9.728218078613281\n",
      "Batch num 28020: Loss 10.129311561584473\n",
      "Batch num 28030: Loss 9.962053298950195\n",
      "Batch num 28040: Loss 9.45244026184082\n",
      "Batch num 28050: Loss 11.421548843383789\n",
      "Batch num 28060: Loss 10.93143367767334\n",
      "Batch num 28070: Loss 10.862457275390625\n",
      "Batch num 28080: Loss 9.122411727905273\n",
      "Batch num 28090: Loss 10.14299201965332\n",
      "Batch num 28100: Loss 11.967106819152832\n",
      "Batch num 28110: Loss 11.286674499511719\n",
      "Batch num 28120: Loss 11.368118286132812\n",
      "Batch num 28130: Loss 9.549736022949219\n",
      "Batch num 28140: Loss 11.580665588378906\n",
      "Batch num 28150: Loss 10.780694961547852\n",
      "Batch num 28160: Loss 11.186274528503418\n",
      "Batch num 28170: Loss 9.778460502624512\n",
      "Batch num 28180: Loss 11.100218772888184\n",
      "Batch num 28190: Loss 10.569726943969727\n",
      "Batch num 28200: Loss 9.887665748596191\n",
      "Batch num 28210: Loss 9.121814727783203\n",
      "Batch num 28220: Loss 12.141264915466309\n",
      "Batch num 28230: Loss 9.728829383850098\n",
      "Batch num 28240: Loss 9.5452241897583\n",
      "Batch num 28250: Loss 10.174002647399902\n",
      "Batch num 28260: Loss 10.917884826660156\n",
      "Batch num 28270: Loss 11.610733985900879\n",
      "Batch num 28280: Loss 8.929018020629883\n",
      "Batch num 28290: Loss 9.933619499206543\n",
      "Batch num 28300: Loss 9.37195873260498\n",
      "Batch num 28310: Loss 9.793217658996582\n",
      "Batch num 28320: Loss 9.022459983825684\n",
      "Batch num 28330: Loss 9.248897552490234\n",
      "Batch num 28340: Loss 10.73963451385498\n",
      "Batch num 28350: Loss 9.807695388793945\n",
      "Batch num 28360: Loss 10.722375869750977\n",
      "Batch num 28370: Loss 10.680587768554688\n",
      "Batch num 28380: Loss 9.652303695678711\n",
      "Batch num 28390: Loss 8.998566627502441\n",
      "Batch num 28400: Loss 10.243958473205566\n",
      "Batch num 28410: Loss 14.381062507629395\n",
      "Batch num 28420: Loss 9.78791618347168\n",
      "Batch num 28430: Loss 9.760590553283691\n",
      "Batch num 28440: Loss 10.273995399475098\n",
      "Batch num 28450: Loss 10.292518615722656\n",
      "Batch num 28460: Loss 10.816657066345215\n",
      "Batch num 28470: Loss 12.232468605041504\n",
      "Batch num 28480: Loss 11.035845756530762\n",
      "Batch num 28490: Loss 9.792794227600098\n",
      "Batch num 28500: Loss 8.760231018066406\n",
      "Batch num 28510: Loss 10.087182998657227\n",
      "Batch num 28520: Loss 11.455480575561523\n",
      "Batch num 28530: Loss 10.783808708190918\n",
      "Batch num 28540: Loss 12.533591270446777\n",
      "Batch num 28550: Loss 8.911628723144531\n",
      "Batch num 28560: Loss 12.164246559143066\n",
      "Batch num 28570: Loss 10.877676963806152\n",
      "Batch num 28580: Loss 10.570778846740723\n",
      "Batch num 28590: Loss 10.222391128540039\n",
      "Batch num 28600: Loss 10.684919357299805\n",
      "Batch num 28610: Loss 13.015615463256836\n",
      "Batch num 28620: Loss 11.15325927734375\n",
      "Batch num 28630: Loss 10.766748428344727\n",
      "Batch num 28640: Loss 11.596940994262695\n",
      "Batch num 28650: Loss 10.863269805908203\n",
      "Batch num 28660: Loss 10.545233726501465\n",
      "Batch num 28670: Loss 8.858589172363281\n",
      "Batch num 28680: Loss 9.910650253295898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch num 28690: Loss 10.975594520568848\n",
      "Batch num 28700: Loss 10.613696098327637\n",
      "Batch num 28710: Loss 10.698972702026367\n",
      "Batch num 28720: Loss 10.099038124084473\n",
      "Batch num 28730: Loss 10.048246383666992\n",
      "Batch num 28740: Loss 9.158878326416016\n",
      "Batch num 28750: Loss 9.357900619506836\n",
      "Batch num 28760: Loss 11.324456214904785\n",
      "Batch num 28770: Loss 10.153430938720703\n",
      "Batch num 28780: Loss 9.665699005126953\n",
      "Batch num 28790: Loss 11.559667587280273\n",
      "Batch num 28800: Loss 9.621159553527832\n",
      "Batch num 28810: Loss 11.059752464294434\n",
      "Batch num 28820: Loss 9.363100051879883\n",
      "Batch num 28830: Loss 9.526044845581055\n",
      "Batch num 28840: Loss 9.330915451049805\n",
      "Batch num 28850: Loss 8.345442771911621\n",
      "Batch num 28860: Loss 9.3341646194458\n",
      "Batch num 28870: Loss 10.684869766235352\n",
      "Batch num 28880: Loss 9.202886581420898\n",
      "Batch num 28890: Loss 8.351156234741211\n",
      "Batch num 28900: Loss 10.578193664550781\n",
      "Batch num 28910: Loss 12.204509735107422\n",
      "Batch num 28920: Loss 10.78753662109375\n",
      "Batch num 28930: Loss 10.685811996459961\n",
      "Batch num 28940: Loss 10.805929183959961\n",
      "Batch num 28950: Loss 9.800167083740234\n",
      "Batch num 28960: Loss 10.482427597045898\n",
      "Batch num 28970: Loss 10.755782127380371\n",
      "Batch num 28980: Loss 8.798858642578125\n",
      "Batch num 28990: Loss 9.750826835632324\n",
      "Batch num 29000: Loss 10.63124942779541\n",
      "Batch num 29010: Loss 9.50223159790039\n",
      "Batch num 29020: Loss 10.49321174621582\n",
      "Batch num 29030: Loss 10.118663787841797\n",
      "Batch num 29040: Loss 10.416601181030273\n",
      "Batch num 29050: Loss 12.123146057128906\n",
      "Batch num 29060: Loss 9.860763549804688\n",
      "Batch num 29070: Loss 11.640549659729004\n",
      "Batch num 29080: Loss 13.552355766296387\n",
      "Batch num 29090: Loss 11.978886604309082\n",
      "Batch num 29100: Loss 9.6202392578125\n",
      "Batch num 29110: Loss 8.848673820495605\n",
      "Batch num 29120: Loss 9.90880298614502\n",
      "Batch num 29130: Loss 11.064911842346191\n",
      "Batch num 29140: Loss 10.964818000793457\n",
      "Batch num 29150: Loss 11.08413314819336\n",
      "Batch num 29160: Loss 11.916669845581055\n",
      "Batch num 29170: Loss 10.237689018249512\n",
      "Batch num 29180: Loss 10.036341667175293\n",
      "Batch num 29190: Loss 10.098369598388672\n",
      "Batch num 29200: Loss 10.196640014648438\n",
      "Batch num 29210: Loss 8.887717247009277\n",
      "Batch num 29220: Loss 9.358661651611328\n",
      "Batch num 29230: Loss 10.14971923828125\n",
      "Batch num 29240: Loss 8.952457427978516\n",
      "Batch num 29250: Loss 10.661507606506348\n",
      "Batch num 29260: Loss 10.67087173461914\n",
      "Batch num 29270: Loss 10.847013473510742\n",
      "Batch num 29280: Loss 10.447874069213867\n",
      "Batch num 29290: Loss 8.218286514282227\n",
      "Batch num 29300: Loss 9.027203559875488\n",
      "Batch num 29310: Loss 10.118731498718262\n",
      "Batch num 29320: Loss 8.488348007202148\n",
      "Batch num 29330: Loss 9.8400297164917\n",
      "Batch num 29340: Loss 9.337106704711914\n",
      "Batch num 29350: Loss 10.221870422363281\n",
      "Batch num 29360: Loss 9.92422866821289\n",
      "Batch num 29370: Loss 12.19403076171875\n",
      "Batch num 29380: Loss 9.259560585021973\n",
      "Batch num 29390: Loss 9.264328002929688\n",
      "Batch num 29400: Loss 11.16304874420166\n",
      "Batch num 29410: Loss 7.692366123199463\n",
      "Batch num 29420: Loss 11.822991371154785\n",
      "Batch num 29430: Loss 11.22559642791748\n",
      "Batch num 29440: Loss 8.842330932617188\n",
      "Batch num 29450: Loss 11.16086196899414\n",
      "Batch num 29460: Loss 10.423629760742188\n",
      "Batch num 29470: Loss 7.526691436767578\n",
      "Batch num 29480: Loss 9.840184211730957\n",
      "Batch num 29490: Loss 9.348974227905273\n",
      "Batch num 29500: Loss 8.391741752624512\n",
      "Batch num 29510: Loss 10.348097801208496\n",
      "Batch num 29520: Loss 10.545172691345215\n",
      "Batch num 29530: Loss 9.535385131835938\n",
      "Batch num 29540: Loss 10.379499435424805\n",
      "Batch num 29550: Loss 10.796396255493164\n",
      "Batch num 29560: Loss 9.378097534179688\n",
      "Batch num 29570: Loss 9.514947891235352\n",
      "Batch num 29580: Loss 11.965944290161133\n",
      "Batch num 29590: Loss 11.765722274780273\n",
      "Batch num 29600: Loss 10.738039016723633\n",
      "Batch num 29610: Loss 10.575102806091309\n",
      "Batch num 29620: Loss 10.361471176147461\n",
      "Batch num 29630: Loss 10.005642890930176\n",
      "Batch num 29640: Loss 8.398177146911621\n",
      "Batch num 29650: Loss 10.818035125732422\n",
      "Batch num 29660: Loss 10.027738571166992\n",
      "Batch num 29670: Loss 7.880996227264404\n",
      "Batch num 29680: Loss 11.45193099975586\n",
      "Batch num 29690: Loss 9.287622451782227\n",
      "Batch num 29700: Loss 9.702648162841797\n",
      "Batch num 29710: Loss 9.577709197998047\n",
      "Batch num 29720: Loss 11.623333930969238\n",
      "Batch num 29730: Loss 11.289441108703613\n",
      "Batch num 29740: Loss 9.91090202331543\n",
      "Batch num 29750: Loss 8.572437286376953\n",
      "Batch num 29760: Loss 9.649970054626465\n",
      "Batch num 29770: Loss 10.241636276245117\n",
      "Batch num 29780: Loss 9.859275817871094\n",
      "Batch num 29790: Loss 10.456053733825684\n",
      "Batch num 29800: Loss 8.797235488891602\n",
      "Batch num 29810: Loss 11.603191375732422\n",
      "Batch num 29820: Loss 10.40817642211914\n",
      "Batch num 29830: Loss 10.647671699523926\n",
      "Batch num 29840: Loss 10.77475357055664\n",
      "Batch num 29850: Loss 11.102072715759277\n",
      "Batch num 29860: Loss 8.802706718444824\n",
      "Batch num 29870: Loss 10.28437328338623\n",
      "Batch num 29880: Loss 9.722332954406738\n",
      "Batch num 29890: Loss 11.792448043823242\n",
      "Batch num 29900: Loss 9.880213737487793\n",
      "Batch num 29910: Loss 8.22912883758545\n",
      "Batch num 29920: Loss 10.185843467712402\n",
      "Batch num 29930: Loss 12.78062629699707\n",
      "Batch num 29940: Loss 10.360679626464844\n",
      "Batch num 29950: Loss 9.472947120666504\n",
      "Batch num 29960: Loss 10.597686767578125\n",
      "Batch num 29970: Loss 9.810670852661133\n",
      "Batch num 29980: Loss 10.838547706604004\n",
      "Batch num 29990: Loss 10.793800354003906\n",
      "Batch num 30000: Loss 11.173799514770508\n",
      "Batch num 30010: Loss 9.706099510192871\n",
      "Batch num 30020: Loss 10.327439308166504\n",
      "Batch num 30030: Loss 11.575173377990723\n",
      "Batch num 30040: Loss 9.283926963806152\n",
      "Batch num 30050: Loss 9.970918655395508\n",
      "Batch num 30060: Loss 10.87037467956543\n",
      "Batch num 30070: Loss 11.39035415649414\n",
      "Batch num 30080: Loss 11.10929012298584\n",
      "Batch num 30090: Loss 9.860133171081543\n",
      "Batch num 30100: Loss 9.84573745727539\n",
      "Batch num 30110: Loss 10.586525917053223\n",
      "Batch num 30120: Loss 10.270456314086914\n",
      "Batch num 30130: Loss 10.533750534057617\n",
      "Batch num 30140: Loss 10.786140441894531\n",
      "Batch num 30150: Loss 10.122675895690918\n",
      "Batch num 30160: Loss 11.751786231994629\n",
      "Batch num 30170: Loss 9.99176025390625\n",
      "Batch num 30180: Loss 10.039974212646484\n",
      "Batch num 30190: Loss 10.123246192932129\n",
      "Batch num 30200: Loss 10.358059883117676\n",
      "Batch num 30210: Loss 9.665878295898438\n",
      "Batch num 30220: Loss 10.265758514404297\n",
      "Batch num 30230: Loss 10.28492259979248\n",
      "Batch num 30240: Loss 10.420045852661133\n",
      "Batch num 30250: Loss 9.966882705688477\n",
      "Batch num 30260: Loss 10.575043678283691\n",
      "Batch num 30270: Loss 10.238112449645996\n",
      "Batch num 30280: Loss 12.022294998168945\n",
      "Batch num 30290: Loss 9.331707000732422\n",
      "Batch num 30300: Loss 10.889695167541504\n",
      "Batch num 30310: Loss 11.400981903076172\n",
      "Batch num 30320: Loss 10.828995704650879\n",
      "Batch num 30330: Loss 9.309144973754883\n",
      "Batch num 30340: Loss 10.426033020019531\n",
      "Batch num 30350: Loss 9.326172828674316\n",
      "Batch num 30360: Loss 9.84560489654541\n",
      "Batch num 30370: Loss 11.844237327575684\n",
      "Batch num 30380: Loss 10.576086044311523\n",
      "Batch num 30390: Loss 9.627327919006348\n",
      "Batch num 30400: Loss 10.425460815429688\n",
      "Batch num 30410: Loss 10.122457504272461\n",
      "Batch num 30420: Loss 10.136775016784668\n",
      "Batch num 30430: Loss 10.511214256286621\n",
      "Batch num 30440: Loss 9.417811393737793\n",
      "Batch num 30450: Loss 9.70873737335205\n",
      "Batch num 30460: Loss 9.877601623535156\n",
      "Batch num 30470: Loss 9.847257614135742\n",
      "Batch num 30480: Loss 9.976813316345215\n",
      "Batch num 30490: Loss 10.297220230102539\n",
      "Batch num 30500: Loss 9.617533683776855\n",
      "Batch num 30510: Loss 9.21158218383789\n",
      "Batch num 30520: Loss 10.869607925415039\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, val_dataloader, regularize=\"hidden_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do for final paper: \n",
    "* Add Tensorboard stuff \n",
    "* Print out accuracy for the encoder\n",
    "* Create Perplexity evaluation metric\n",
    "* Run example with discriminator over both the attention and hidden\n",
    "* Factorize Code into util and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
